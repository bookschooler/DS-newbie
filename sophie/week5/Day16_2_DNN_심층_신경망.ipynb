{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_2: DNN (Deep Neural Network) - 심층 신경망\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 깊은 신경망의 장점과 문제점 이해하기\n",
    "2. 가중치 초기화 (Xavier, He) 이해하기\n",
    "3. Batch Normalization 이해하기\n",
    "4. Dropout으로 과적합 방지하기\n",
    "5. 조기 종료(Early Stopping) 구현하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. Learning Rate Scheduler 활용하기\n",
    "2. 모델 저장과 로드 (torch.save/load)\n",
    "3. 체크포인트 관리하기\n",
    "4. Fashion-MNIST로 DNN 실습하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| 깊은 신경망 | 복잡한 패턴 학습 | 이미지 인식, 자연어 처리 |\n",
    "| 가중치 초기화 | 학습 안정성 확보 | 기울기 소실/폭발 방지 |\n",
    "| BatchNorm | 빠른 학습, 일반화 | 층이 깊어도 안정적 학습 |\n",
    "| Dropout | 과적합 방지 | 더 강건한 모델 |\n",
    "| 체크포인트 | 모델 관리, 재개 | 긴 학습 과정 관리 |\n",
    "\n",
    "**분석가 관점**: DNN은 더 깊은 층을 쌓아 복잡한 패턴을 학습합니다. 하지만 깊어질수록 학습이 어려워지는 문제가 있어, 이를 해결하는 다양한 기법을 배웁니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.13.9)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. listen EFAULT: bad address in system call argument 0.0.0.0:9000"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# device 설정\n",
    "device = torch.device('mps')\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"사용 device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 깊은 신경망의 장점과 문제점\n",
    "\n",
    "### 층이 많으면 좋은가?\n",
    "\n",
    "**장점: 더 복잡한 패턴 학습 가능**\n",
    "\n",
    "```\n",
    "얕은 네트워크 (2층):    입력 -> 은닉 -> 출력\n",
    "깊은 네트워크 (5층):    입력 -> 은닉1 -> 은닉2 -> 은닉3 -> 은닉4 -> 출력\n",
    "```\n",
    "\n",
    "- 각 층이 점점 더 추상적인 특성을 학습\n",
    "- 예: 이미지에서 Edge -> Texture -> Pattern -> Object\n",
    "\n",
    "**문제점: 기울기 소실/폭발 (Gradient Vanishing/Exploding)**\n",
    "\n",
    "- **기울기 소실**: 역전파 시 기울기가 점점 작아져 앞쪽 층이 학습되지 않음\n",
    "- **기울기 폭발**: 기울기가 너무 커져서 학습이 불안정해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 소실 문제 시뮬레이션\n",
    "# Sigmoid 활성화 함수를 여러 층 통과하면 기울기가 어떻게 변하는지 확인\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)  # 최대값 0.25\n",
    "\n",
    "# 10개 층을 통과할 때 기울기 변화\n",
    "num_layers = 10\n",
    "gradients = [1.0]  # 초기 기울기\n",
    "\n",
    "for i in range(num_layers):\n",
    "    # Sigmoid 도함수의 최대값(0.25)으로 계산\n",
    "    gradients.append(gradients[-1] * 0.25)\n",
    "\n",
    "print(\"층별 기울기 크기 (Sigmoid):\")\n",
    "for i, g in enumerate(gradients):\n",
    "    print(f\"  층 {i}: {g:.10f}\")\n",
    "\n",
    "print(f\"\\n10층 후 기울기: {gradients[-1]:.2e} (거의 0!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 소실 시각화\n",
    "layers = list(range(len(gradients)))\n",
    "\n",
    "fig = px.line(x=layers, y=gradients, markers=True,\n",
    "              title='기울기 소실 문제 (Sigmoid 활성화)',\n",
    "              labels={'x': '층 번호', 'y': '기울기 크기'})\n",
    "fig.update_layout(yaxis_type='log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU로 기울기 소실 완화\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`\n",
    "- 양수 영역에서 도함수가 1 -> 기울기 유지\n",
    "- 계산 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU vs Sigmoid 비교\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['활성화 함수', '도함수'])\n",
    "\n",
    "# 활성화 함수\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid(x), name='Sigmoid'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=np.maximum(0, x), name='ReLU'), row=1, col=1)\n",
    "\n",
    "# 도함수\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid_derivative(x), name='Sigmoid\\''), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=(x > 0).astype(float), name='ReLU\\''), row=1, col=2)\n",
    "\n",
    "fig.update_layout(title='Sigmoid vs ReLU', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 가중치 초기화 (Weight Initialization)\n",
    "\n",
    "### 왜 중요한가?\n",
    "\n",
    "**비유: 물이 흐르는 파이프**\n",
    "- **너무 작은 초기화**: 파이프가 좁아서 물이 점점 줄어듦 → 신호 소실 (Vanishing)\n",
    "- **너무 큰 초기화**: 파이프가 넓어서 물이 점점 불어남 → 신호 폭발 (Exploding)\n",
    "- **적절한 초기화**: 각 층에서 신호가 적절히 유지되도록 가중치 범위 조절\n",
    "\n",
    "**핵심 아이디어**: 각 층의 입력과 출력 분산을 비슷하게 유지하여 신호가 안정적으로 전달되도록 함\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier (Glorot) 초기화\n",
    "\n",
    "**발명자**: Xavier Glorot (2010년)\n",
    "\n",
    "**핵심 아이디어**: \n",
    "- 입력과 출력의 연결 개수를 모두 고려하여 가중치 범위를 설정\n",
    "- 각 층을 통과할 때 신호의 분산이 유지되도록 설계\n",
    "\n",
    "**적용 대상**: \n",
    "- Sigmoid, Tanh 같은 대칭적 활성화 함수\n",
    "- 입력과 출력 모두 고려하는 방식\n",
    "\n",
    "**공식**: \n",
    "```\n",
    "범위 = [-√(6/(입력개수 + 출력개수)), √(6/(입력개수 + 출력개수))]\n",
    "```\n",
    "\n",
    "**직관적 이해**:\n",
    "- 입력이 100개, 출력이 50개인 층이라면\n",
    "- 범위 = [-√(6/150), √(6/150)] ≈ [-0.2, 0.2]\n",
    "- 이 범위에서 가중치를 랜덤하게 초기화하면 신호가 안정적으로 전달됨\n",
    "\n",
    "**왜 이렇게?**\n",
    "- 입력과 출력의 연결 개수를 모두 고려하여 \"양쪽 방향\" 모두에서 신호가 적절히 유지되도록 함\n",
    "\n",
    "---\n",
    "\n",
    "### He (Kaiming) 초기화\n",
    "\n",
    "**발명자**: Kaiming He (2015년)\n",
    "\n",
    "**핵심 아이디어**:\n",
    "- ReLU는 음수 부분을 0으로 만드는 비대칭 함수\n",
    "- 입력 쪽만 고려하여 가중치 범위를 설정\n",
    "- Xavier보다 약 2배 큰 범위 사용\n",
    "\n",
    "**적용 대상**:\n",
    "- ReLU, LeakyReLU 같은 비대칭 활성화 함수\n",
    "- ReLU가 절반의 뉴런을 0으로 만들기 때문에 더 큰 가중치가 필요\n",
    "\n",
    "**공식**:\n",
    "```\n",
    "표준편차 = √(2/입력개수)\n",
    "또는\n",
    "범위 = [-√(6/입력개수), √(6/입력개수)]  (uniform 버전)\n",
    "```\n",
    "\n",
    "**직관적 이해**:\n",
    "- 입력이 100개인 층이라면\n",
    "- 범위 = [-√(6/100), √(6/100)] ≈ [-0.24, 0.24]\n",
    "- Xavier보다 약간 더 큰 범위 (ReLU가 절반을 죽이기 때문)\n",
    "\n",
    "**왜 이렇게?**\n",
    "- ReLU는 음수를 0으로 만들기 때문에, 활성화되는 뉴런이 절반으로 줄어듦\n",
    "- 따라서 더 큰 가중치가 필요하여 입력 개수만 고려하고 2배를 곱함\n",
    "\n",
    "---\n",
    "\n",
    "### 비교 요약\n",
    "\n",
    "| 초기화 방법 | 대상 활성화 함수 | 고려 사항 | 범위 크기 |\n",
    "|------------|----------------|----------|----------|\n",
    "| **Xavier** | Sigmoid, Tanh | 입력 + 출력 개수 | 중간 |\n",
    "| **He (Kaiming)** | ReLU, LeakyReLU | 입력 개수만 | 더 큼 (약 2배) |\n",
    "\n",
    "**실무 팁**:\n",
    "- Sigmoid/Tanh 사용 시 → Xavier 초기화\n",
    "- ReLU 사용 시 → He (Kaiming) 초기화\n",
    "- PyTorch에서는 자동으로 적절한 초기화를 사용하지만, 수동으로 설정할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 기본 초기화 확인\n",
    "layer = nn.Linear(100, 50)\n",
    "\n",
    "print(\"PyTorch Linear 기본 초기화:\")\n",
    "print(f\"  weight 평균: {layer.weight.data.mean():.6f}\")\n",
    "print(f\"  weight 표준편차: {layer.weight.data.std():.6f}\")\n",
    "print(f\"  weight 범위: [{layer.weight.data.min():.4f}, {layer.weight.data.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier 초기화 적용\n",
    "layer_xavier = nn.Linear(100, 50)\n",
    "nn.init.xavier_uniform_(layer_xavier.weight)\n",
    "\n",
    "print(\"Xavier 초기화:\")\n",
    "print(f\"  weight 평균: {layer_xavier.weight.data.mean():.6f}\")\n",
    "print(f\"  weight 표준편차: {layer_xavier.weight.data.std():.6f}\")\n",
    "\n",
    "# He 초기화 적용\n",
    "layer_he = nn.Linear(100, 50)\n",
    "nn.init.kaiming_uniform_(layer_he.weight, nonlinearity='relu')\n",
    "\n",
    "print(\"\\nHe (Kaiming) 초기화:\")\n",
    "print(f\"  weight 평균: {layer_he.weight.data.mean():.6f}\")\n",
    "print(f\"  weight 표준편차: {layer_he.weight.data.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 방법에 따른 가중치 분포 비교\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=['기본 초기화', 'Xavier', 'He (Kaiming)'])\n",
    "\n",
    "fig.add_trace(go.Histogram(x=layer.weight.data.numpy().flatten(), nbinsx=50, name='기본'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=layer_xavier.weight.data.numpy().flatten(), nbinsx=50, name='Xavier'), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=layer_he.weight.data.numpy().flatten(), nbinsx=50, name='He'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(title='가중치 초기화 방법 비교', height=400, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 초기화가 적용된 신경망\n",
    "class DNNWithInitialization(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # He 초기화 적용\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 모델 생성\n",
    "model_init = DNNWithInitialization(784, [256, 128, 64], 10)\n",
    "print(model_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 언제 어떤 초기화를 사용하나요?\n",
    "\n",
    "| 활성화 함수 | 추천 초기화 | PyTorch 함수 |\n",
    "|------------|------------|-------------|\n",
    "| Sigmoid, Tanh | Xavier | `nn.init.xavier_uniform_()` |\n",
    "| ReLU, LeakyReLU | He (Kaiming) | `nn.init.kaiming_uniform_()` |\n",
    "| SELU | LeCun | `nn.init.lecun_normal_()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Batch Normalization\n",
    "\n",
    "### 내부 공변량 이동 (Internal Covariate Shift)\n",
    "\n",
    "학습 중 각 층의 입력 분포가 계속 변해서 학습이 어려워지는 현상입니다.\n",
    "\n",
    "### Batch Normalization의 효과\n",
    "\n",
    "1. **정규화**: 미니배치의 평균과 분산으로 정규화\n",
    "2. **스케일/시프트**: 학습 가능한 파라미터로 조정\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu_{batch}}{\\sqrt{\\sigma^2_{batch} + \\epsilon}}$$\n",
    "$$y = \\gamma \\hat{x} + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm1d 사용법\n",
    "bn = nn.BatchNorm1d(num_features=64)  # 특성 수\n",
    "\n",
    "# 배치 데이터 생성\n",
    "x = torch.randn(32, 64)  # batch_size=32, features=64\n",
    "\n",
    "# BatchNorm 적용 전\n",
    "print(\"BatchNorm 적용 전:\")\n",
    "print(f\"  평균: {x.mean():.4f}, 표준편차: {x.std():.4f}\")\n",
    "\n",
    "# BatchNorm 적용 후\n",
    "bn.train()  # 학습 모드\n",
    "x_bn = bn(x)\n",
    "print(\"\\nBatchNorm 적용 후:\")\n",
    "print(f\"  평균: {x_bn.mean():.4f}, 표준편차: {x_bn.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm 학습 파라미터 확인\n",
    "print(\"BatchNorm 학습 가능 파라미터:\")\n",
    "print(f\"  gamma (weight): shape={bn.weight.shape}\")\n",
    "print(f\"  beta (bias): shape={bn.bias.shape}\")\n",
    "\n",
    "print(\"\\nBatchNorm 통계량 (학습 중 업데이트):\")\n",
    "print(f\"  running_mean: shape={bn.running_mean.shape}\")\n",
    "print(f\"  running_var: shape={bn.running_var.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm이 포함된 DNN\n",
    "class DNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # BatchNorm 추가\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model_bn = DNNWithBatchNorm(784, [256, 128, 64], 10)\n",
    "print(model_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: BatchNorm 순서\n",
    "\n",
    "**권장 순서**: `Linear -> BatchNorm -> ReLU`\n",
    "\n",
    "- BatchNorm이 활성화 함수 전에 위치\n",
    "- 일부는 `Linear -> ReLU -> BatchNorm` 사용 (결과 비슷)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Dropout\n",
    "\n",
    "### 과적합 방지 원리\n",
    "\n",
    "**학습 시**: 무작위로 뉴런을 비활성화 (p 확률로 0으로 설정)\n",
    "**추론 시**: 모든 뉴런 사용 (출력을 1-p로 스케일링)\n",
    "\n",
    "```\n",
    "학습 시:    [1] [0] [1] [1] [0] [1]   (일부 비활성화)\n",
    "추론 시:    [1] [1] [1] [1] [1] [1]   (전체 사용)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout 사용법\n",
    "dropout = nn.Dropout(p=0.5)  # 50% 비활성화\n",
    "\n",
    "x = torch.ones(10)\n",
    "print(f\"원본: {x}\")\n",
    "\n",
    "# 학습 모드\n",
    "dropout.train()\n",
    "print(f\"학습 모드 (dropout): {dropout(x)}\")\n",
    "print(f\"학습 모드 (dropout): {dropout(x)}  # 매번 다름\")\n",
    "\n",
    "# 평가 모드\n",
    "dropout.eval()\n",
    "print(f\"평가 모드: {dropout(x)}  # 변화 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout이 포함된 DNN\n",
    "class DNNWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))  # Dropout 추가\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model_dropout = DNNWithDropout(784, [256, 128, 64], 10, dropout_rate=0.3)\n",
    "print(model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: Dropout 비율 선택\n",
    "\n",
    "| 레이어 위치 | 권장 비율 |\n",
    "|------------|----------|\n",
    "| 입력층 근처 | 0.2 |\n",
    "| 은닉층 | 0.3 ~ 0.5 |\n",
    "| 출력층 근처 | 0.5 이하 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 조기 종료 (Early Stopping)\n",
    "\n",
    "### 언제 학습을 멈출까?\n",
    "\n",
    "검증 손실이 더 이상 개선되지 않으면 학습을 중단합니다.\n",
    "\n",
    "```\n",
    "학습 손실:  \\___     (계속 감소)\n",
    "검증 손실:  \\__/---  (감소 후 증가 = 과적합)\n",
    "                ^\n",
    "             여기서 멈춤!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping 클래스 구현\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        patience: 개선 없이 기다릴 에포크 수\n",
    "        min_delta: 개선으로 인정할 최소 변화량\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"  EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "# 사용 예시\n",
    "early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "# 가상의 검증 손실 시퀀스\n",
    "val_losses = [0.5, 0.4, 0.35, 0.36, 0.37, 0.38, 0.39]\n",
    "\n",
    "for epoch, loss in enumerate(val_losses):\n",
    "    print(f\"Epoch {epoch}: val_loss = {loss}\")\n",
    "    if early_stopper(loss):\n",
    "        print(f\"Early stopping at epoch {epoch}!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Learning Rate Scheduler\n",
    "\n",
    "학습률을 동적으로 조정하여 학습을 최적화합니다.\n",
    "\n",
    "### 주요 스케줄러\n",
    "\n",
    "1. **StepLR**: 일정 간격마다 학습률 감소\n",
    "2. **ReduceLROnPlateau**: 손실이 정체되면 감소\n",
    "3. **CosineAnnealingLR**: 코사인 함수로 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 스케줄러 비교\n",
    "def simulate_scheduler(scheduler_class, scheduler_params, epochs=50):\n",
    "    \"\"\"스케줄러의 학습률 변화를 시뮬레이션\"\"\"\n",
    "    model = nn.Linear(10, 1)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    scheduler = scheduler_class(optimizer, **scheduler_params)\n",
    "    \n",
    "    lrs = []\n",
    "    for epoch in range(epochs):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "    \n",
    "    return lrs\n",
    "\n",
    "# 각 스케줄러 시뮬레이션\n",
    "epochs = 50\n",
    "\n",
    "lrs_step = simulate_scheduler(optim.lr_scheduler.StepLR, {'step_size': 10, 'gamma': 0.5}, epochs)\n",
    "lrs_exp = simulate_scheduler(optim.lr_scheduler.ExponentialLR, {'gamma': 0.95}, epochs)\n",
    "lrs_cosine = simulate_scheduler(optim.lr_scheduler.CosineAnnealingLR, {'T_max': epochs}, epochs)\n",
    "\n",
    "# 시각화\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=lrs_step, name='StepLR (step=10, gamma=0.5)'))\n",
    "fig.add_trace(go.Scatter(y=lrs_exp, name='ExponentialLR (gamma=0.95)'))\n",
    "fig.add_trace(go.Scatter(y=lrs_cosine, name='CosineAnnealingLR'))\n",
    "\n",
    "fig.update_layout(title='Learning Rate Scheduler 비교',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Learning Rate')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau 사용 예시\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 손실이 3 에포크 동안 개선 없으면 학습률 0.1배로 감소\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',      # 손실 최소화\n",
    "    factor=0.1,      # 학습률 * 0.1\n",
    "    patience=3,      # 3 에포크 기다림\n",
    ")\n",
    "\n",
    "# 사용법: scheduler.step(val_loss)\n",
    "print(f\"ReduceLROnPlateau 설정 완료\")\n",
    "print(f\"  mode: min (손실 최소화)\")\n",
    "print(f\"  factor: 0.1 (학습률 * 0.1)\")\n",
    "print(f\"  patience: 3 (3 에포크 기다림)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 모델 저장과 로드\n",
    "\n",
    "### state_dict 개념\n",
    "\n",
    "모델의 학습 가능한 파라미터(가중치, 편향)를 딕셔너리 형태로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = DNNWithBatchNorm(784, [256, 128], 10)\n",
    "\n",
    "# state_dict 확인\n",
    "print(\"Model state_dict 키:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 1: state_dict만 저장 (권장)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "print(\"state_dict 저장 완료: model_weights.pth\")\n",
    "\n",
    "# 로드\n",
    "model_loaded = DNNWithBatchNorm(784, [256, 128], 10)  # 같은 구조로 생성\n",
    "model_loaded.load_state_dict(torch.load('model_weights.pth'))\n",
    "model_loaded.eval()  # 평가 모드 설정\n",
    "print(\"state_dict 로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2: 전체 모델 저장\n",
    "torch.save(model, 'full_model.pth')\n",
    "print(\"전체 모델 저장 완료: full_model.pth\")\n",
    "\n",
    "# PyTorch 2.6 이후, torch.load의 기본값이 weights_only=True\n",
    "model_full = torch.load('full_model.pth', weights_only=False)\n",
    "model_full.eval()\n",
    "print(\"전체 모델 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: state_dict vs 전체 모델\n",
    "\n",
    "| 방법 | 장점 | 단점 |\n",
    "|------|------|------|\n",
    "| state_dict | 유연함, 호환성 좋음 | 모델 구조 별도 정의 필요 |\n",
    "| 전체 모델 | 간편함 | PyTorch 버전 의존성 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 체크포인트 관리\n",
    "\n",
    "학습 중 최고 성능 모델을 저장하고, 학습을 재개할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 저장 함수\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path='checkpoint.pth'):\n",
    "    \"\"\"학습 상태 저장\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} (loss: {loss:.4f})\")\n",
    "\n",
    "# 체크포인트 로드 함수\n",
    "def load_checkpoint(model, optimizer, path='checkpoint.pth'):\n",
    "    \"\"\"학습 상태 복원\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded from epoch {epoch} (loss: {loss:.4f})\")\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 성능 모델 저장 클래스\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath='best_model.pth', verbose=True):\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"  Validation loss improved ({self.best_loss:.4f} -> {val_loss:.4f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.filepath)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# 사용 예시\n",
    "model_ckpt = ModelCheckpoint('best_model.pth')\n",
    "print(f\"ModelCheckpoint 설정 완료: {model_ckpt.filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 Fashion-MNIST로 DNN 실습\n",
    "\n",
    "Fashion-MNIST는 10개 클래스의 패션 아이템 이미지 데이터셋입니다.\n",
    "\n",
    "| 레이블 | 클래스 |\n",
    "|--------|--------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST 데이터 로드\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 정규화\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"학습 데이터: {len(train_dataset)}개\")\n",
    "print(f\"테스트 데이터: {len(test_dataset)}개\")\n",
    "print(f\"이미지 shape: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 시각화\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=5, subplot_titles=[class_names[i] for i in range(10)])\n",
    "\n",
    "for i in range(10):\n",
    "    # 각 클래스의 첫 번째 이미지 찾기\n",
    "    for img, label in train_dataset:\n",
    "        if label == i:\n",
    "            row = i // 5 + 1\n",
    "            col = i % 5 + 1\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(z=img.squeeze().numpy(), showscale=False),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            break\n",
    "\n",
    "fig.update_layout(title='Fashion-MNIST 샘플', height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"학습 배치 수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전한 DNN 모델 정의\n",
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dims=[256, 64], output_dim=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (batch, 1, 28, 28) -> (batch, 784)\n",
    "        return self.network(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = FashionDNN()\n",
    "model = model.to('cpu')\n",
    "print(model)\n",
    "\n",
    "# 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n총 파라미터: {total_params:,}\")\n",
    "print(f\"학습 가능 파라미터: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Early Stopping & Checkpoint\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "model_ckpt = ModelCheckpoint('fashion_best_model.pth')\n",
    "\n",
    "print(\"학습 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(test_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "epochs = 30\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "print(\"학습 시작!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 학습\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # 검증\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # 학습률\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 기록\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # 스케줄러 업데이트\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # 체크포인트 저장\n",
    "    model_ckpt(val_loss, model)\n",
    "    \n",
    "    # Early Stopping 체크\n",
    "    if early_stopper(val_loss):\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "print(\"\\n학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=['Loss', 'Accuracy', 'Learning Rate'])\n",
    "\n",
    "# Loss\n",
    "fig.add_trace(go.Scatter(y=history['train_loss'], name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=history['val_loss'], name='Val Loss'), row=1, col=1)\n",
    "\n",
    "# Accuracy\n",
    "fig.add_trace(go.Scatter(y=history['train_acc'], name='Train Acc'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=history['val_acc'], name='Val Acc'), row=1, col=2)\n",
    "\n",
    "# Learning Rate\n",
    "fig.add_trace(go.Scatter(y=history['lr'], name='LR'), row=1, col=3)\n",
    "\n",
    "fig.update_layout(title='Fashion-MNIST DNN 학습 곡선', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 성능 모델 로드\n",
    "model.load_state_dict(torch.load('fashion_best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 최종 평가\n",
    "final_loss, final_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"최고 성능 모델 테스트 정확도: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동 행렬 계산\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "fig = px.imshow(cm, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                x=class_names, y=class_names,\n",
    "                title='Fashion-MNIST 혼동 행렬',\n",
    "                color_continuous_scale='Blues',\n",
    "                text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스별 정확도\n",
    "class_correct = np.diag(cm)\n",
    "class_total = cm.sum(axis=1)\n",
    "class_accuracy = class_correct / class_total * 100\n",
    "\n",
    "class_acc_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Accuracy': class_accuracy\n",
    "}).sort_values('Accuracy', ascending=True)\n",
    "\n",
    "fig = px.bar(class_acc_df, x='Accuracy', y='Class', orientation='h',\n",
    "             title='클래스별 정확도',\n",
    "             color='Accuracy', color_continuous_scale='RdYlGn')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 가중치 초기화 (기본)\n",
    "\n",
    "**문제**: `nn.Linear(100, 50)` 레이어를 생성하고 He 초기화를 적용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. BatchNorm 적용 (기본)\n",
    "\n",
    "**문제**: 32개의 특성을 가진 배치 데이터에 BatchNorm1d를 적용하고, 적용 전후의 평균과 표준편차를 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Dropout 동작 확인 (기본)\n",
    "\n",
    "**문제**: `p=0.5`인 Dropout 레이어를 만들고, 학습 모드와 평가 모드에서의 차이를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Early Stopping 구현 (응용)\n",
    "\n",
    "**문제**: patience=3인 Early Stopping 클래스를 사용하여, 아래 검증 손실 시퀀스에서 언제 학습이 중단되는지 확인하세요.\n",
    "\n",
    "```python\n",
    "val_losses = [0.8, 0.6, 0.5, 0.48, 0.49, 0.50, 0.51]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Learning Rate Scheduler (응용)\n",
    "\n",
    "**문제**: StepLR 스케줄러를 사용하여 10 에포크마다 학습률을 0.1배로 감소시키세요. 30 에포크 동안의 학습률 변화를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. 모델 저장/로드 (응용)\n",
    "\n",
    "**문제**: 간단한 MLP 모델을 생성하고, state_dict를 저장한 뒤 새로운 모델에 로드하세요. 두 모델의 첫 번째 레이어 가중치가 동일한지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. 체크포인트 저장 (복합)\n",
    "\n",
    "**문제**: 모델, 옵티마이저, 에포크, 손실을 포함한 체크포인트를 저장하고 로드하는 함수를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. 과적합 진단 (복합)\n",
    "\n",
    "**문제**: 학습 손실과 검증 손실 곡선을 보고 과적합 여부를 판단하세요. 아래 데이터를 Plotly로 시각화하고, 과적합이 시작되는 에포크를 찾으세요.\n",
    "\n",
    "```python\n",
    "train_losses = [2.0, 1.5, 1.0, 0.7, 0.5, 0.3, 0.2, 0.15, 0.1, 0.08]\n",
    "val_losses = [2.1, 1.6, 1.1, 0.8, 0.7, 0.75, 0.8, 0.9, 1.0, 1.1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Fashion-MNIST DNN (종합)\n",
    "\n",
    "**문제**: 다음 구조의 DNN을 정의하고 Fashion-MNIST에서 10 에포크 학습하세요.\n",
    "\n",
    "- 입력: 784\n",
    "- 은닉층: 512 -> 256 -> 128\n",
    "- 출력: 10\n",
    "- BatchNorm, Dropout(0.4), ReLU 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 종합 파이프라인 (종합)\n",
    "\n",
    "**문제**: Fashion-MNIST 학습 파이프라인을 완성하세요. 다음 요소를 모두 포함해야 합니다.\n",
    "\n",
    "1. DNN 모델 (He 초기화, BatchNorm, Dropout)\n",
    "2. Adam 옵티마이저\n",
    "3. ReduceLROnPlateau 스케줄러\n",
    "4. Early Stopping (patience=5)\n",
    "5. 최고 모델 체크포인트 저장\n",
    "6. 학습 곡선 시각화 (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 언제 사용? |\n",
    "|-----|----------|----------|\n",
    "| 가중치 초기화 | Xavier(Sigmoid/Tanh), He(ReLU) | 모든 신경망 |\n",
    "| BatchNorm | 미니배치 정규화 | 깊은 네트워크, 빠른 수렴 |\n",
    "| Dropout | 무작위 뉴런 비활성화 | 과적합 방지 |\n",
    "| Early Stopping | 검증 손실 모니터링 | 자동 학습 종료 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 메서드 | 언제 사용? |\n",
    "|-----|-----------|----------|\n",
    "| LR Scheduler | StepLR, ReduceLROnPlateau | 학습률 동적 조정 |\n",
    "| 모델 저장 | torch.save(state_dict) | 학습 결과 보존 |\n",
    "| 체크포인트 | 모델+옵티마이저+에포크 | 학습 재개, 최고 모델 저장 |\n",
    "\n",
    "### DNN 모델 구조 패턴\n",
    "\n",
    "```python\n",
    "# 권장 패턴\n",
    "Linear -> BatchNorm -> ReLU -> Dropout\n",
    "Linear -> BatchNorm -> ReLU -> Dropout\n",
    "...\n",
    "Linear (출력층)\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **He 초기화**: ReLU 계열 활성화 함수와 함께 사용\n",
    "2. **BatchNorm**: 학습 속도 향상, 더 높은 학습률 사용 가능\n",
    "3. **Dropout**: 0.3~0.5 범위로 시작, 필요에 따라 조정\n",
    "4. **Early Stopping**: patience=5~10 권장\n",
    "5. **체크포인트**: 긴 학습 시 반드시 사용\n",
    "6. **eval() 모드**: 평가 시 반드시 `model.eval()` 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 파일 정리\n",
    "import os\n",
    "\n",
    "temp_files = ['model_weights.pth', 'full_model.pth', 'checkpoint.pth', \n",
    "              'best_model.pth', 'fashion_best_model.pth']\n",
    "\n",
    "for f in temp_files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"삭제됨: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
