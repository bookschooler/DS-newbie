{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day18_0 ë³„ì²¨: GAN (Generative Adversarial Networks)\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "**ì´ ë³„ì²¨ ìë£Œì—ì„œëŠ”:**\n",
    "1. GAN (Generative Adversarial Network)ì˜ í•µì‹¬ ê°œë… ì´í•´í•˜ê¸°\n",
    "2. Generatorì™€ Discriminatorì˜ ì—­í• ê³¼ ê²½ìŸ êµ¬ì¡° íŒŒì•…í•˜ê¸°\n",
    "3. GAN ì†ì‹¤ í•¨ìˆ˜ì™€ í›ˆë ¨ ê³¼ì • ì´í•´í•˜ê¸°\n",
    "4. DCGAN (Deep Convolutional GAN) êµ¬ì¡° í•™ìŠµí•˜ê¸°\n",
    "5. MNIST ì†ê¸€ì”¨ ì´ë¯¸ì§€ ìƒì„± ì‹¤ìŠµí•˜ê¸°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì™œ ì´ê²ƒì„ ë°°ìš°ë‚˜ìš”?\n",
    "\n",
    "| ê°œë… | ì‹¤ë¬´ í™œìš© | ì˜ˆì‹œ |\n",
    "|------|----------|------|\n",
    "| GAN | ë°ì´í„° ìƒì„±/ì¦ê°• | í•™ìŠµ ë°ì´í„° ë¶€ì¡± ì‹œ í•©ì„± ë°ì´í„° ìƒì„± |\n",
    "| Generator | ìƒˆë¡œìš´ ì½˜í…ì¸  ìƒì„± | ì´ë¯¸ì§€, ìŒì•…, í…ìŠ¤íŠ¸ ì°½ì‘ |\n",
    "| Discriminator | í’ˆì§ˆ íŒë³„/í•„í„°ë§ | ê°€ì§œ ì´ë¯¸ì§€ íƒì§€, í’ˆì§ˆ í‰ê°€ |\n",
    "| DCGAN | ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± | ì–¼êµ´ ìƒì„±, ìŠ¤íƒ€ì¼ ë³€í™˜ |\n",
    "\n",
    "**ë¶„ì„ê°€ ê´€ì **: GANì€ 'ì—†ëŠ” ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ”' í˜ì‹ ì  ê¸°ìˆ ì…ë‹ˆë‹¤. ë°ì´í„° ì¦ê°•, ì´ìƒì¹˜ ìƒì„±, ì‹œë®¬ë ˆì´ì…˜ ë“± ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°ì— í™œìš©ë©ë‹ˆë‹¤. Stable Diffusion, DALL-E ë“± ìµœì‹  ì´ë¯¸ì§€ ìƒì„± AIì˜ ê¸°ì´ˆ ê°œë…ì´ê¸°ë„ í•©ë‹ˆë‹¤!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GAN ê°œë…\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 GANì´ë€?\n",
    "\n",
    "**GAN (Generative Adversarial Network)**: ë‘ ì‹ ê²½ë§ì´ ì„œë¡œ ê²½ìŸí•˜ë©° í•™ìŠµí•˜ëŠ” ìƒì„± ëª¨ë¸\n",
    "\n",
    "2014ë…„ Ian Goodfellowê°€ ì œì•ˆí•œ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¡œ, 'ì ëŒ€ì  í•™ìŠµ(Adversarial Training)'ì„ í†µí•´ ì‹¤ì œì™€ êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš´ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ë¹„ìœ : ìœ„ì¡°ì§€íë²” vs ê²½ì°°\n",
    "\n",
    "| ì—­í•  | GAN êµ¬ì„±ìš”ì†Œ | ëª©í‘œ |\n",
    "|------|-------------|------|\n",
    "| ìœ„ì¡°ì§€íë²” | Generator (ìƒì„±ì) | ê²½ì°°ë„ ì†ì¼ ìˆ˜ ìˆëŠ” ì§„ì§œ ê°™ì€ ê°€ì§œ ë§Œë“¤ê¸° |\n",
    "| ê²½ì°° | Discriminator (íŒë³„ì) | ì§„ì§œì™€ ê°€ì§œë¥¼ ì •í™•íˆ êµ¬ë¶„í•˜ê¸° |\n",
    "\n",
    "ë‘ ì—­í• ì´ ì„œë¡œ ê²½ìŸí•˜ë©° ë°œì „:\n",
    "- ìœ„ì¡°ì§€íë²”ì´ ë” ì •êµí•´ì§€ë©´ â†’ ê²½ì°°ë„ ë” ë‚ ì¹´ë¡œì›Œì§\n",
    "- ê²½ì°°ì´ ë” ì •í™•í•´ì§€ë©´ â†’ ìœ„ì¡°ì§€íë²”ë„ ë” ì •êµí•´ì§\n",
    "- ê²°êµ­ ì§„ì§œì™€ êµ¬ë¶„ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€ì— ë„ë‹¬!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ì¥ì¹˜ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 GAN êµ¬ì¡°\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   ëœë¤ ë…¸ì´ì¦ˆ z â”€â”€â”€â†’â”‚   Generator     â”‚â”€â”€â”€â†’ ê°€ì§œ ì´ë¯¸ì§€\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "                                                  â†“\n",
    "                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   ì§„ì§œ ì´ë¯¸ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Discriminator  â”‚â”€â”€â”€â†’ ì§„ì§œ/ê°€ì§œ íŒë³„\n",
    "                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Generator (ìƒì„±ì)\n",
    "- **ì…ë ¥**: ëœë¤ ë…¸ì´ì¦ˆ ë²¡í„° z (ì˜ˆ: 100ì°¨ì› ê°€ìš°ì‹œì•ˆ)\n",
    "- **ì¶œë ¥**: ê°€ì§œ ì´ë¯¸ì§€\n",
    "- **ëª©í‘œ**: Discriminatorë¥¼ ì†ì¼ ìˆ˜ ìˆëŠ” ì§„ì§œ ê°™ì€ ì´ë¯¸ì§€ ìƒì„±\n",
    "\n",
    "### Discriminator (íŒë³„ì)\n",
    "- **ì…ë ¥**: ì´ë¯¸ì§€ (ì§„ì§œ ë˜ëŠ” ê°€ì§œ)\n",
    "- **ì¶œë ¥**: ì§„ì§œì¼ í™•ë¥  (0~1)\n",
    "- **ëª©í‘œ**: ì§„ì§œì™€ ê°€ì§œë¥¼ ì •í™•íˆ êµ¬ë¶„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. GAN ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Minimax Game\n",
    "\n",
    "GANì˜ í•™ìŠµì€ **Minimax Game** (ì˜í•© ê²Œì„)ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "### ê° í•­ì˜ ì˜ë¯¸\n",
    "\n",
    "| í•­ | ì˜ë¯¸ | Discriminator ê´€ì  | Generator ê´€ì  |\n",
    "|---|------|-------------------|----------------|\n",
    "| $\\log D(x)$ | ì§„ì§œë¥¼ ì§„ì§œë¡œ íŒë³„ | ìµœëŒ€í™” (1ì— ê°€ê¹ê²Œ) | ê´€ê³„ì—†ìŒ |\n",
    "| $\\log(1 - D(G(z)))$ | ê°€ì§œë¥¼ ê°€ì§œë¡œ íŒë³„ | ìµœëŒ€í™” (0ì— ê°€ê¹ê²Œ) | ìµœì†Œí™” (1ì— ê°€ê¹ê²Œ) |\n",
    "\n",
    "### ì§ê´€ì  ì´í•´\n",
    "\n",
    "- **Discriminator**: ì§„ì§œëŠ” 1, ê°€ì§œëŠ” 0ìœ¼ë¡œ íŒë³„í•˜ê³  ì‹¶ìŒ â†’ ì†ì‹¤ í•¨ìˆ˜ ìµœëŒ€í™”\n",
    "- **Generator**: ê°€ì§œë„ 1ë¡œ íŒë³„ë˜ê²Œ í•˜ê³  ì‹¶ìŒ â†’ ì†ì‹¤ í•¨ìˆ˜ ìµœì†Œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN ì†ì‹¤ í•¨ìˆ˜ ì‹œê°í™”\n",
    "d_output = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# ì§„ì§œ ë°ì´í„°ì— ëŒ€í•œ Dì˜ ì†ì‹¤: -log(D(x))\n",
    "d_loss_real = -np.log(d_output)\n",
    "\n",
    "# ê°€ì§œ ë°ì´í„°ì— ëŒ€í•œ Dì˜ ì†ì‹¤: -log(1 - D(G(z)))\n",
    "d_loss_fake = -np.log(1 - d_output)\n",
    "\n",
    "# Gì˜ ì†ì‹¤: -log(D(G(z))) [ëŒ€ì•ˆ í˜•íƒœ]\n",
    "g_loss = -np.log(d_output)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Discriminator ì†ì‹¤', 'Generator ì†ì‹¤'])\n",
    "\n",
    "# Discriminator ì†ì‹¤\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=d_output, y=d_loss_real, name='ì§„ì§œ: -log(D(x))', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=d_output, y=d_loss_fake, name='ê°€ì§œ: -log(1-D(G(z)))', line=dict(color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Generator ì†ì‹¤\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=d_output, y=g_loss, name='G ì†ì‹¤: -log(D(G(z)))', line=dict(color='green')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='D(x) ë˜ëŠ” D(G(z))', row=1, col=1)\n",
    "fig.update_xaxes(title_text='D(G(z))', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text='GAN ì†ì‹¤ í•¨ìˆ˜ ì‹œê°í™”')\n",
    "fig.show()\n",
    "\n",
    "print(\"ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\n",
    "print(\"- Dê°€ ì§„ì§œë¥¼ 1ë¡œ íŒë³„í• ìˆ˜ë¡ â†’ D ì†ì‹¤ ê°ì†Œ (íŒŒë€ì„  ì™¼ìª½)\")\n",
    "print(\"- Dê°€ ê°€ì§œë¥¼ 0ìœ¼ë¡œ íŒë³„í• ìˆ˜ë¡ â†’ D ì†ì‹¤ ê°ì†Œ (ë¹¨ê°„ì„  ì™¼ìª½)\")\n",
    "print(\"- Gê°€ ê°€ì§œë¥¼ 1ë¡œ ì†ì¼ìˆ˜ë¡ â†’ G ì†ì‹¤ ê°ì†Œ (ë…¹ìƒ‰ì„  ì˜¤ë¥¸ìª½)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ì‹¤ì œ í•™ìŠµì—ì„œì˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Binary Cross Entropy (BCE)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "# Discriminator ì†ì‹¤\n",
    "d_loss_real = BCE(D(real_images), ones)   # ì§„ì§œëŠ” 1ë¡œ\n",
    "d_loss_fake = BCE(D(G(z)), zeros)         # ê°€ì§œëŠ” 0ìœ¼ë¡œ\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# Generator ì†ì‹¤ (Non-saturating ë²„ì „)\n",
    "g_loss = BCE(D(G(z)), ones)               # ê°€ì§œë¥¼ 1ë¡œ ì†ì´ê³  ì‹¶ìŒ\n",
    "```\n",
    "\n",
    "### Non-saturating Loss\n",
    "\n",
    "ì›ë˜ ê³µì‹ëŒ€ë¡œë¼ë©´ G ì†ì‹¤ì€ $\\log(1 - D(G(z)))$ë¥¼ ìµœì†Œí™”í•´ì•¼ í•˜ì§€ë§Œ,\n",
    "í•™ìŠµ ì´ˆê¸°ì— ê¸°ìš¸ê¸°ê°€ ê±°ì˜ 0ì´ ë˜ëŠ” ë¬¸ì œê°€ ìˆì–´ì„œ $-\\log(D(G(z)))$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. ê¸°ë³¸ GAN êµ¬í˜„\n",
    "\n",
    "---\n",
    "\n",
    "MNIST ì†ê¸€ì”¨ ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ GANì„ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "## 3.1 ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./datasets',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {len(train_dataset):,}\")\n",
    "print(f\"ë°°ì¹˜ ìˆ˜: {len(train_loader):,}\")\n",
    "print(f\"ì´ë¯¸ì§€ í¬ê¸°: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generator ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator: ëœë¤ ë…¸ì´ì¦ˆë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
    "    \n",
    "    - ì…ë ¥: ì ì¬ ë²¡í„° z (latent_dim ì°¨ì›)\n",
    "    - ì¶œë ¥: ìƒì„±ëœ ì´ë¯¸ì§€ (1, 28, 28)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "        img_size = int(np.prod(img_shape))  # 1 * 28 * 28 = 784\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # ì…ë ¥: (batch, latent_dim)\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, img_size),\n",
    "            nn.Tanh()  # ì¶œë ¥ ë²”ìœ„ [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim=latent_dim)\n",
    "\n",
    "# ëœë¤ ë…¸ì´ì¦ˆ ìƒì„±\n",
    "z = torch.randn(4, latent_dim)\n",
    "fake_images = generator(z)\n",
    "\n",
    "print(f\"ì…ë ¥ ë…¸ì´ì¦ˆ shape: {z.shape}\")\n",
    "print(f\"ìƒì„± ì´ë¯¸ì§€ shape: {fake_images.shape}\")\n",
    "print(f\"ì¶œë ¥ ë²”ìœ„: [{fake_images.min():.2f}, {fake_images.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Discriminator ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator: ì´ë¯¸ì§€ê°€ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ íŒë³„\n",
    "    \n",
    "    - ì…ë ¥: ì´ë¯¸ì§€ (1, 28, 28)\n",
    "    - ì¶œë ¥: ì§„ì§œì¼ í™•ë¥  (0~1)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        img_size = int(np.prod(img_shape))  # 784\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # ì…ë ¥: (batch, 784)\n",
    "            nn.Linear(img_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # ì¶œë ¥ ë²”ìœ„ [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# ì§„ì§œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ íŒë³„\n",
    "real_img, _ = train_dataset[0]\n",
    "real_img = real_img.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "\n",
    "real_pred = discriminator(real_img)\n",
    "fake_pred = discriminator(fake_images)\n",
    "\n",
    "print(f\"ì§„ì§œ ì´ë¯¸ì§€ íŒë³„: {real_pred.item():.4f}\")\n",
    "print(f\"ê°€ì§œ ì´ë¯¸ì§€ íŒë³„: {fake_pred.mean().item():.4f}\")\n",
    "print(\"(í•™ìŠµ ì „ì´ë¼ ëœë¤í•œ ê°’)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 í›ˆë ¨ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim=latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "lr = 0.0002\n",
    "betas = (0.5, 0.999)  # Adamì˜ ëª¨ë©˜í…€ íŒŒë¼ë¯¸í„° (GANì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"Generator íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, train_loader, n_epochs=50, sample_interval=10):\n",
    "    \"\"\"\n",
    "    GAN í›ˆë ¨ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    history = {'d_loss': [], 'g_loss': [], 'd_real': [], 'd_fake': []}\n",
    "    fixed_z = torch.randn(64, latent_dim, device=device)  # ê³ ì • ë…¸ì´ì¦ˆ (ì‹œê°í™”ìš©)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        d_losses, g_losses = [], []\n",
    "        d_reals, d_fakes = [], []\n",
    "        \n",
    "        for real_imgs, _ in train_loader:\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            # ë¼ë²¨ ìƒì„±\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "            \n",
    "            # ========================\n",
    "            # 1. Discriminator í›ˆë ¨\n",
    "            # ========================\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # ì§„ì§œ ì´ë¯¸ì§€ íŒë³„\n",
    "            real_pred = discriminator(real_imgs)\n",
    "            d_loss_real = criterion(real_pred, real_labels)\n",
    "            \n",
    "            # ê°€ì§œ ì´ë¯¸ì§€ ìƒì„± ë° íŒë³„\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            fake_imgs = generator(z)\n",
    "            fake_pred = discriminator(fake_imgs.detach())  # Generator ê·¸ë˜í”„ ë¶„ë¦¬\n",
    "            d_loss_fake = criterion(fake_pred, fake_labels)\n",
    "            \n",
    "            # ì´ Discriminator ì†ì‹¤\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # ========================\n",
    "            # 2. Generator í›ˆë ¨\n",
    "            # ========================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # GeneratorëŠ” Discriminatorë¥¼ ì†ì´ê³  ì‹¶ìŒ\n",
    "            fake_pred = discriminator(fake_imgs)\n",
    "            g_loss = criterion(fake_pred, real_labels)  # ê°€ì§œë¥¼ ì§„ì§œë¡œ íŒë³„í•˜ê²Œ\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # ê¸°ë¡\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_reals.append(real_pred.mean().item())\n",
    "            d_fakes.append(fake_pred.mean().item())\n",
    "        \n",
    "        # ì—í­ í‰ê· \n",
    "        history['d_loss'].append(np.mean(d_losses))\n",
    "        history['g_loss'].append(np.mean(g_losses))\n",
    "        history['d_real'].append(np.mean(d_reals))\n",
    "        history['d_fake'].append(np.mean(d_fakes))\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "        if epoch % sample_interval == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{n_epochs} | \"\n",
    "                  f\"D Loss: {history['d_loss'][-1]:.4f} | \"\n",
    "                  f\"G Loss: {history['g_loss'][-1]:.4f} | \"\n",
    "                  f\"D(real): {history['d_real'][-1]:.3f} | \"\n",
    "                  f\"D(fake): {history['d_fake'][-1]:.3f}\")\n",
    "            \n",
    "            # ìƒì„± ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_imgs = generator(fixed_z).cpu()\n",
    "            generator.train()\n",
    "            \n",
    "            # 8x8 ê·¸ë¦¬ë“œë¡œ í‘œì‹œ\n",
    "            grid = make_grid(sample_imgs, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "            plt.title(f'Epoch {epoch}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ ì‹¤í–‰ (ì§§ì€ ë²„ì „ - ì „ì²´ í›ˆë ¨ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "print(\"GAN í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì‹¤ì œ í•™ìŠµ ì‹œ n_epochs=100 ì´ìƒ ê¶Œì¥\n",
    "history = train_gan(generator, discriminator, train_loader, n_epochs=30, sample_interval=10)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í›ˆë ¨ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['ì†ì‹¤ í•¨ìˆ˜', 'íŒë³„ ê²°ê³¼'])\n",
    "\n",
    "# ì†ì‹¤\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=history['d_loss'], name='Discriminator Loss', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=history['g_loss'], name='Generator Loss', line=dict(color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# íŒë³„ ê²°ê³¼\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=history['d_real'], name='D(real)', line=dict(color='green')),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=history['d_fake'], name='D(fake)', line=dict(color='orange')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Epoch', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epoch', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Probability', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text='GAN í•™ìŠµ ê³¡ì„ ')\n",
    "fig.show()\n",
    "\n",
    "print(\"ğŸ’¡ í•´ì„:\")\n",
    "print(\"- D(real) â‰ˆ 0.5, D(fake) â‰ˆ 0.5: ì´ìƒì ì¸ ê· í˜• ìƒíƒœ (êµ¬ë¶„ ë¶ˆê°€)\")\n",
    "print(\"- G Lossê°€ ì•ˆì •ì ì´ë©´ì„œ D(fake)ê°€ ì˜¬ë¼ê°€ë©´ ì¢‹ì€ í•™ìŠµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. DCGAN (Deep Convolutional GAN)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 DCGANì´ë€?\n",
    "\n",
    "**DCGAN (Deep Convolutional GAN)**: CNNì„ í™œìš©í•œ GANìœ¼ë¡œ, ë” ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê¸°ë³¸ GAN vs DCGAN\n",
    "\n",
    "| êµ¬ë¶„ | ê¸°ë³¸ GAN | DCGAN |\n",
    "|------|----------|-------|\n",
    "| Generator | FC ë ˆì´ì–´ | ì „ì¹˜ í•©ì„±ê³± (ConvTranspose2d) |\n",
    "| Discriminator | FC ë ˆì´ì–´ | í•©ì„±ê³± (Conv2d) |\n",
    "| í™œì„±í™” í•¨ìˆ˜ | ReLU/LeakyReLU | Generator: ReLU, Discriminator: LeakyReLU |\n",
    "| ì •ê·œí™” | Dropout | BatchNorm |\n",
    "| ì´ë¯¸ì§€ í’ˆì§ˆ | íë¦¿í•¨ | ë” ì„ ëª…í•¨|\n",
    "\n",
    "### DCGANì˜ í•µì‹¬ ê°€ì´ë“œë¼ì¸ (ë…¼ë¬¸ ì œì‹œ)\n",
    "\n",
    "1. Pooling ë ˆì´ì–´ ëŒ€ì‹  Strided Convolution ì‚¬ìš©\n",
    "2. Generatorì™€ Discriminator ëª¨ë‘ BatchNorm ì‚¬ìš©\n",
    "3. FC ë ˆì´ì–´ ìµœì†Œí™”\n",
    "4. Generator: ReLU + ë§ˆì§€ë§‰ì€ Tanh\n",
    "5. Discriminator: LeakyReLU ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Generator\n",
    "    \n",
    "    - ConvTranspose2dë¡œ ì´ë¯¸ì§€ ì—…ìƒ˜í”Œë§\n",
    "    - BatchNorm + ReLU\n",
    "    - ë§ˆì§€ë§‰ì€ Tanh\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, channels=1, feature_maps=64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # ì…ë ¥: (batch, latent_dim, 1, 1)\n",
    "            # ì¶œë ¥: (batch, feature_maps*4, 4, 4)\n",
    "            nn.ConvTranspose2d(latent_dim, feature_maps * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (batch, feature_maps*4, 4, 4) -> (batch, feature_maps*2, 7, 7)\n",
    "            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (batch, feature_maps*2, 7, 7) -> (batch, feature_maps, 14, 14)\n",
    "            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # (batch, feature_maps, 14, 14) -> (batch, channels, 28, 28)\n",
    "            nn.ConvTranspose2d(feature_maps, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # z: (batch, latent_dim) -> (batch, latent_dim, 1, 1)\n",
    "        z = z.view(-1, self.latent_dim, 1, 1)\n",
    "        return self.model(z)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "dc_generator = DCGenerator(latent_dim=100)\n",
    "z = torch.randn(4, 100)\n",
    "fake_imgs = dc_generator(z)\n",
    "print(f\"DCGAN Generator ì¶œë ¥ shape: {fake_imgs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN Discriminator\n",
    "    \n",
    "    - Conv2dë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§\n",
    "    - BatchNorm + LeakyReLU\n",
    "    - ë§ˆì§€ë§‰ì€ Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=1, feature_maps=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # ì…ë ¥: (batch, channels, 28, 28)\n",
    "            # ì¶œë ¥: (batch, feature_maps, 14, 14)\n",
    "            nn.Conv2d(channels, feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # (batch, feature_maps, 14, 14) -> (batch, feature_maps*2, 7, 7)\n",
    "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # (batch, feature_maps*2, 7, 7) -> (batch, feature_maps*4, 4, 4)\n",
    "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # (batch, feature_maps*4, 4, 4) -> (batch, 1, 1, 1)\n",
    "            nn.Conv2d(feature_maps * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.model(img).view(-1, 1)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "dc_discriminator = DCDiscriminator()\n",
    "pred = dc_discriminator(fake_imgs)\n",
    "print(f\"DCGAN Discriminator ì¶œë ¥ shape: {pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "\n",
    "DCGAN ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    DCGAN ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "    - Conv/ConvTranspose: N(0, 0.02)\n",
    "    - BatchNorm: N(1, 0.02), bias=0\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# DCGAN ëª¨ë¸ ì´ˆê¸°í™”\n",
    "dc_generator = DCGenerator(latent_dim=100).to(device)\n",
    "dc_discriminator = DCDiscriminator().to(device)\n",
    "\n",
    "dc_generator.apply(weights_init)\n",
    "dc_discriminator.apply(weights_init)\n",
    "\n",
    "print(f\"DCGAN Generator íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in dc_generator.parameters()):,}\")\n",
    "print(f\"DCGAN Discriminator íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in dc_discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. GANì˜ ë¬¸ì œì ê³¼ í•´ê²°ì±…\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 ì£¼ìš” ë¬¸ì œì \n",
    "\n",
    "| ë¬¸ì œ | ì„¤ëª… | ì¦ìƒ |\n",
    "|------|------|------|\n",
    "| **Mode Collapse** | Generatorê°€ ë‹¤ì–‘ì„± ì—†ì´ ëª‡ ê°€ì§€ ìƒ˜í”Œë§Œ ìƒì„± | ëª¨ë“  ì¶œë ¥ì´ ë¹„ìŠ·í•¨ |\n",
    "| **í›ˆë ¨ ë¶ˆì•ˆì •** | Dì™€ Gì˜ ê· í˜•ì´ ë¬´ë„ˆì§ | ì†ì‹¤ì´ ì§„ë™í•˜ê±°ë‚˜ ë°œì‚° |\n",
    "| **Vanishing Gradient** | Dê°€ ë„ˆë¬´ ì˜ í•™ìŠµë˜ë©´ Gì˜ ê¸°ìš¸ê¸°ê°€ 0 | Gê°€ í•™ìŠµë˜ì§€ ì•ŠìŒ |\n",
    "| **í‰ê°€ì˜ ì–´ë ¤ì›€** | ìƒì„± í’ˆì§ˆì„ ìˆ˜ì¹˜í™”í•˜ê¸° ì–´ë ¤ì›€ | ì£¼ê´€ì  í‰ê°€ì— ì˜ì¡´ |\n",
    "\n",
    "## 5.2 í•´ê²°ì±… ë° ê°œì„  ê¸°ë²•\n",
    "\n",
    "| ë¬¸ì œ | í•´ê²°ì±… | ì„¤ëª… |\n",
    "|------|--------|------|\n",
    "| Mode Collapse | Minibatch Discrimination | ë°°ì¹˜ ë‚´ ë‹¤ì–‘ì„± ê³ ë ¤ |\n",
    "| í›ˆë ¨ ë¶ˆì•ˆì • | WGAN (Wasserstein GAN) | ë” ì•ˆì •ì ì¸ ì†ì‹¤ í•¨ìˆ˜ |\n",
    "| Vanishing Gradient | Non-saturating Loss | $-\\log(D(G(z)))$ ì‚¬ìš© |\n",
    "| í‰ê°€ ì–´ë ¤ì›€ | FID (FrÃ©chet Inception Distance) | ì •ëŸ‰ì  í‰ê°€ ì§€í‘œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode Collapse ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['ì •ìƒ í•™ìŠµ', 'Mode Collapse'])\n",
    "\n",
    "# ì •ìƒ í•™ìŠµ: ë‹¤ì–‘í•œ ë¶„í¬\n",
    "normal_x = np.random.randn(500)\n",
    "normal_y = np.random.randn(500)\n",
    "\n",
    "# Mode Collapse: ëª‡ ê°€ì§€ ëª¨ë“œì—ë§Œ ì§‘ì¤‘\n",
    "collapse_centers = [(-2, -2), (2, 2), (0, 0)]\n",
    "collapse_x, collapse_y = [], []\n",
    "for cx, cy in collapse_centers:\n",
    "    collapse_x.extend(np.random.randn(150) * 0.1 + cx)\n",
    "    collapse_y.extend(np.random.randn(150) * 0.1 + cy)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=normal_x, y=normal_y, mode='markers', marker=dict(size=5, color='blue', opacity=0.5)),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=collapse_x, y=collapse_y, mode='markers', marker=dict(size=5, color='red', opacity=0.5)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text='Mode Collapse ì‹œê°í™”', showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(\"ğŸ’¡ Mode Collapse:\")\n",
    "print(\"- ì™¼ìª½: ë‹¤ì–‘í•œ ìƒ˜í”Œ ìƒì„± (ì •ìƒ)\")\n",
    "print(\"- ì˜¤ë¥¸ìª½: ëª‡ ê°€ì§€ íŒ¨í„´ë§Œ ë°˜ë³µ ìƒì„± (Mode Collapse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. GANì˜ ì‘ìš© ë¶„ì•¼\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 ë‹¤ì–‘í•œ GAN ë³€í˜•\n",
    "\n",
    "| GAN ë³€í˜• | ìš©ë„ | íŠ¹ì§• |\n",
    "|----------|------|------|\n",
    "| **CGAN** (Conditional) | ì¡°ê±´ë¶€ ìƒì„± | ë¼ë²¨ ì¡°ê±´ìœ¼ë¡œ íŠ¹ì • í´ë˜ìŠ¤ ìƒì„± |\n",
    "| **Pix2Pix** | ì´ë¯¸ì§€-ì´ë¯¸ì§€ ë³€í™˜ | ìŠ¤ì¼€ì¹˜â†’ì‚¬ì§„, í‘ë°±â†’ì»¬ëŸ¬ |\n",
    "| **CycleGAN** | ë¹„ìŒ ì´ë¯¸ì§€ ë³€í™˜ | ë§â†’ì–¼ë£©ë§, ì—¬ë¦„â†’ê²¨ìš¸ |\n",
    "| **StyleGAN** | ê³ í•´ìƒë„ ì–¼êµ´ ìƒì„± | This Person Does Not Exist |\n",
    "| **SRGAN** | ì´ˆí•´ìƒë„ | ì €í•´ìƒë„â†’ê³ í•´ìƒë„ |\n",
    "\n",
    "## 6.2 ì‹¤ë¬´ í™œìš© ì‚¬ë¡€\n",
    "\n",
    "| ë¶„ì•¼ | í™œìš© | ì˜ˆì‹œ |\n",
    "|------|------|------|\n",
    "| ë°ì´í„° ì¦ê°• | í•™ìŠµ ë°ì´í„° ë¶€ì¡± í•´ê²° | ì˜ë£Œ ì˜ìƒ, í¬ê·€ ì¼€ì´ìŠ¤ |\n",
    "| ì´ìƒ íƒì§€ | ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ ì´ìƒ ê°ì§€ | ì œì¡° ë¶ˆëŸ‰ íƒì§€ |\n",
    "| ì˜ˆìˆ /ì°½ì‘ | AI ì•„íŠ¸ ìƒì„± | NFT, ê²Œì„ ì—ì…‹ |\n",
    "| ê°œì¸ì •ë³´ ë³´í˜¸ | í•©ì„± ë°ì´í„° ìƒì„± | ê°œì¸ì •ë³´ ë¹„ì‹ë³„í™” |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š í•™ìŠµ ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | í•µì‹¬ í¬ì¸íŠ¸ |\n",
    "|------|------|------------|\n",
    "| GAN | ìƒì„±ì vs íŒë³„ì ê²½ìŸ | Minimax Game |\n",
    "| Generator | ë…¸ì´ì¦ˆ â†’ ì´ë¯¸ì§€ | Discriminator ì†ì´ê¸° |\n",
    "| Discriminator | ì´ë¯¸ì§€ â†’ ì§„ìœ„ íŒë³„ | ì§„ì§œ/ê°€ì§œ êµ¬ë¶„ |\n",
    "| DCGAN | CNN ê¸°ë°˜ GAN | ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„± |\n",
    "| Mode Collapse | ë‹¤ì–‘ì„± ì†ì‹¤ | ì£¼ìš” ë¬¸ì œì  |\n",
    "\n",
    "### GAN vs ë‹¤ë¥¸ ìƒì„± ëª¨ë¸\n",
    "\n",
    "| íŠ¹ì„± | GAN | VAE | Diffusion |\n",
    "|------|-----|-----|-----------||\n",
    "| ì´ë¯¸ì§€ í’ˆì§ˆ | ë†’ìŒ | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |\n",
    "| í›ˆë ¨ ì•ˆì •ì„± | ë‚®ìŒ | ë†’ìŒ | ë†’ìŒ |\n",
    "| ë‹¤ì–‘ì„± | Mode Collapse ìœ„í—˜ | ì¢‹ìŒ | ë§¤ìš° ì¢‹ìŒ |\n",
    "| ì†ë„ | ë¹ ë¦„ | ë¹ ë¦„ | ëŠë¦¼ |\n",
    "\n",
    "### ğŸ’¡ ì‹¤ë¬´ íŒ\n",
    "\n",
    "1. **í›ˆë ¨ ì•ˆì •ì„±**: í•™ìŠµë¥ ì„ ë‚®ê²Œ, BatchNorm ì‚¬ìš©\n",
    "2. **D/G ê· í˜•**: Dê°€ ë„ˆë¬´ ê°•í•˜ë©´ G í•™ìŠµ ë¶ˆê°€ â†’ D ì—…ë°ì´íŠ¸ íšŸìˆ˜ ì¡°ì ˆ\n",
    "3. **í‰ê°€ ì§€í‘œ**: FID, IS (Inception Score) í™œìš©\n",
    "4. **ìµœì‹  íŠ¸ë Œë“œ**: Diffusion ëª¨ë¸ì´ GANì„ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸\n",
    "5. **ì‹¤ë¬´ ì ìš©**: ë°ì´í„° ì¦ê°•, í•©ì„± ë°ì´í„° ìƒì„±ì— ìœ ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "### ë…¼ë¬¸\n",
    "- GAN ì›ë³¸ ë…¼ë¬¸: \"Generative Adversarial Nets\" (Goodfellow et al., 2014)\n",
    "- DCGAN ë…¼ë¬¸: \"Unsupervised Representation Learning with Deep Convolutional GANs\" (Radford et al., 2015)\n",
    "\n",
    "### ë°œì „ ë°©í–¥\n",
    "- **WGAN**: Wasserstein ê±°ë¦¬ë¥¼ ì‚¬ìš©í•œ ì•ˆì •ì ì¸ í•™ìŠµ\n",
    "- **Progressive GAN**: ì ì§„ì  í•´ìƒë„ ì¦ê°€\n",
    "- **StyleGAN**: ìŠ¤íƒ€ì¼ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ê³ í’ˆì§ˆ ì–¼êµ´ ì´ë¯¸ì§€\n",
    "- **Diffusion Models**: GANì„ ëŒ€ì²´í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ (Stable Diffusion, DALL-E)\n",
    "\n",
    "**ë³¸ ê³¼ì •ì—ì„œëŠ” ìµœì‹  íŠ¸ë Œë“œì¸ LLMê³¼ RAGì— ì§‘ì¤‘í•©ë‹ˆë‹¤. GANì˜ ê°œë…ì„ ì´í•´í–ˆë‹¤ë©´, í•„ìš” ì‹œ ì‹¬í™” í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
