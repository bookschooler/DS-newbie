{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_2: DNN (Deep Neural Network) - 심층 신경망 (정답)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 깊은 신경망의 장점과 문제점 이해하기\n",
    "2. 가중치 초기화 (Xavier, He) 이해하기\n",
    "3. Batch Normalization 이해하기\n",
    "4. Dropout으로 과적합 방지하기\n",
    "5. 조기 종료(Early Stopping) 구현하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. Learning Rate Scheduler 활용하기\n",
    "2. 모델 저장과 로드 (torch.save/load)\n",
    "3. 체크포인트 관리하기\n",
    "4. Fashion-MNIST로 DNN 실습하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.13.9)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. listen EFAULT: bad address in system call argument 127.0.0.1:9006"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# device 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"사용 device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"  EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "# Model Checkpoint 클래스\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath='best_model.pth', verbose=True):\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            if self.verbose:\n",
    "                print(f\"  Validation loss improved ({self.best_loss:.4f} -> {val_loss:.4f}). Saving model...\")\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.filepath)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "print(\"공통 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈 정답\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 가중치 초기화 (기본)\n",
    "\n",
    "**문제**: `nn.Linear(100, 50)` 레이어를 생성하고 He 초기화를 적용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Linear 레이어 생성\n",
    "layer = nn.Linear(100, 50)\n",
    "\n",
    "# 2. 기본 초기화 상태 확인\n",
    "print(\"기본 초기화:\")\n",
    "print(f\"  평균: {layer.weight.data.mean():.6f}\")\n",
    "print(f\"  표준편차: {layer.weight.data.std():.6f}\")\n",
    "\n",
    "# 3. He (Kaiming) 초기화 적용\n",
    "nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "\n",
    "# 4. He 초기화 후 상태 확인\n",
    "print(\"\\nHe 초기화 후:\")\n",
    "print(f\"  평균: {layer.weight.data.mean():.6f}\")\n",
    "print(f\"  표준편차: {layer.weight.data.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert layer.weight.shape == (50, 100), \"레이어 크기가 올바르지 않습니다.\"\n",
    "print(\"Q1 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: `nn.init.kaiming_uniform_()` 함수를 사용하여 He 초기화를 적용합니다.\n",
    "- **핵심 개념**: He 초기화는 ReLU 활성화 함수에 최적화되어 있으며, 가중치 분산을 적절히 유지하여 기울기 소실/폭발을 방지합니다.\n",
    "- **대안**: `nn.init.kaiming_normal_()`을 사용할 수도 있습니다 (균등분포 대신 정규분포).\n",
    "- **실수 주의**: `nonlinearity` 파라미터를 지정하지 않으면 기본값 'leaky_relu'가 사용됩니다.\n",
    "- **실무 팁**: PyTorch의 Linear 레이어는 기본적으로 좋은 초기화를 사용하지만, ReLU 계열 활성화 함수에는 He 초기화가 더 효과적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q2. BatchNorm 적용 (기본)\n",
    "\n",
    "**문제**: 32개의 특성을 가진 배치 데이터에 BatchNorm1d를 적용하고, 적용 전후의 평균과 표준편차를 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. BatchNorm1d 레이어 생성\n",
    "bn = nn.BatchNorm1d(num_features=32)\n",
    "\n",
    "# 2. 배치 데이터 생성 (batch_size=64, features=32)\n",
    "x = torch.randn(64, 32) * 3 + 5  # 평균 5, 표준편차 3인 데이터\n",
    "\n",
    "# 3. 적용 전 통계량\n",
    "print(\"BatchNorm 적용 전:\")\n",
    "print(f\"  평균: {x.mean():.4f}\")\n",
    "print(f\"  표준편차: {x.std():.4f}\")\n",
    "\n",
    "# 4. BatchNorm 적용 (학습 모드)\n",
    "bn.train()\n",
    "x_bn = bn(x)\n",
    "\n",
    "# 5. 적용 후 통계량\n",
    "print(\"\\nBatchNorm 적용 후:\")\n",
    "print(f\"  평균: {x_bn.mean():.4f}\")\n",
    "print(f\"  표준편차: {x_bn.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert abs(x_bn.mean().item()) < 0.1, \"평균이 0에 가깝지 않습니다.\"\n",
    "assert abs(x_bn.std().item() - 1.0) < 0.2, \"표준편차가 1에 가깝지 않습니다.\"\n",
    "print(\"Q2 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: `nn.BatchNorm1d()`로 배치 정규화 레이어를 생성하고 데이터를 통과시킵니다.\n",
    "- **핵심 개념**: BatchNorm은 미니배치의 평균을 0, 표준편차를 1로 정규화하여 학습을 안정화합니다.\n",
    "- **대안**: `nn.BatchNorm2d()`는 이미지 데이터(4D 텐서)에 사용됩니다.\n",
    "- **실수 주의**: `num_features`는 입력의 특성(채널) 수와 일치해야 합니다.\n",
    "- **실무 팁**: BatchNorm은 학습 모드와 평가 모드에서 다르게 동작하므로 `.train()`과 `.eval()`을 적절히 사용해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q3. Dropout 동작 확인 (기본)\n",
    "\n",
    "**문제**: `p=0.5`인 Dropout 레이어를 만들고, 학습 모드와 평가 모드에서의 차이를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Dropout 레이어 생성\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# 2. 입력 데이터 생성\n",
    "x = torch.ones(10)\n",
    "print(f\"원본 데이터: {x}\")\n",
    "\n",
    "# 3. 학습 모드에서 Dropout 적용\n",
    "dropout.train()\n",
    "print(\"\\n학습 모드 (dropout.train()):\")\n",
    "for i in range(3):\n",
    "    output = dropout(x)\n",
    "    print(f\"  시도 {i+1}: {output}\")\n",
    "    print(f\"    -> 0의 비율: {(output == 0).sum().item() / len(output) * 100:.0f}%\")\n",
    "\n",
    "# 4. 평가 모드에서 Dropout 적용\n",
    "dropout.eval()\n",
    "print(\"\\n평가 모드 (dropout.eval()):\")\n",
    "output_eval = dropout(x)\n",
    "print(f\"  출력: {output_eval}\")\n",
    "print(f\"  -> 원본과 동일 (Dropout 비활성화)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "dropout.eval()\n",
    "test_output = dropout(x)\n",
    "assert torch.equal(test_output, x), \"평가 모드에서 Dropout이 적용되면 안 됩니다.\"\n",
    "print(\"Q3 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: Dropout 레이어를 생성하고 `.train()`과 `.eval()` 모드에서 각각 출력을 확인합니다.\n",
    "- **핵심 개념**: 학습 모드에서는 p 확률로 뉴런을 0으로 만들고, 평가 모드에서는 아무 변화 없이 통과합니다.\n",
    "- **대안**: `nn.Dropout2d()`는 2D 특성맵(이미지)에 적용됩니다.\n",
    "- **실수 주의**: 모델 평가 시 `.eval()` 호출을 잊으면 Dropout이 적용되어 성능이 저하됩니다.\n",
    "- **실무 팁**: 학습 시 Dropout으로 비활성화된 뉴런의 출력은 1/(1-p)로 스케일링되어 평가 시와 기대값이 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q4. Early Stopping 구현 (응용)\n",
    "\n",
    "**문제**: patience=3인 Early Stopping 클래스를 사용하여, 아래 검증 손실 시퀀스에서 언제 학습이 중단되는지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "\n",
    "# 1. Early Stopping 인스턴스 생성\n",
    "early_stopper = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# 2. 검증 손실 시퀀스\n",
    "val_losses = [0.8, 0.6, 0.5, 0.48, 0.49, 0.50, 0.51]\n",
    "\n",
    "# 3. 각 에포크에서 Early Stopping 체크\n",
    "print(\"Early Stopping 시뮬레이션 (patience=3):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stop_epoch = None\n",
    "for epoch, loss in enumerate(val_losses):\n",
    "    print(f\"Epoch {epoch}: val_loss = {loss}\")\n",
    "    \n",
    "    if early_stopper(loss):\n",
    "        stop_epoch = epoch\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "if stop_epoch is None:\n",
    "    print(\"\\nEarly stopping이 발동되지 않았습니다.\")\n",
    "\n",
    "print(f\"\\n결과: 에포크 {stop_epoch}에서 학습 중단 (손실 개선 없이 3 에포크 경과)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert stop_epoch == 6, f\"에포크 6에서 중단되어야 합니다. 실제: {stop_epoch}\"\n",
    "print(\"Q4 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: 각 에포크에서 검증 손실을 Early Stopping 객체에 전달하고, True가 반환되면 학습을 중단합니다.\n",
    "- **핵심 개념**: 에포크 3(0.48)이 최저점이고, 이후 0.49, 0.50, 0.51로 3번 연속 개선되지 않아 에포크 6에서 중단됩니다.\n",
    "- **대안**: Keras의 `EarlyStopping` 콜백을 참고하여 구현할 수 있습니다.\n",
    "- **실수 주의**: `min_delta`가 너무 크면 작은 개선도 무시되어 조기에 중단될 수 있습니다.\n",
    "- **실무 팁**: patience는 데이터셋과 모델에 따라 5~10 정도가 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q5. Learning Rate Scheduler (응용)\n",
    "\n",
    "**문제**: StepLR 스케줄러를 사용하여 10 에포크마다 학습률을 0.1배로 감소시키세요. 30 에포크 동안의 학습률 변화를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 간단한 모델과 옵티마이저 생성\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. StepLR 스케줄러 생성\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=10,  # 10 에포크마다\n",
    "    gamma=0.1      # 학습률 * 0.1\n",
    ")\n",
    "\n",
    "# 3. 30 에포크 동안 학습률 변화 기록\n",
    "lrs = []\n",
    "epochs = 30\n",
    "\n",
    "print(\"StepLR 스케줄러 학습률 변화:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    lrs.append(current_lr)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch:2d}: LR = {current_lr:.6f}\")\n",
    "    \n",
    "    # 학습 스텝 (생략)\n",
    "    # ...\n",
    "    \n",
    "    # 스케줄러 업데이트\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(x=list(range(epochs)), y=lrs, markers=True,\n",
    "              title='StepLR: 10 에포크마다 학습률 0.1배 감소',\n",
    "              labels={'x': 'Epoch', 'y': 'Learning Rate'})\n",
    "fig.update_yaxes(type='log')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert abs(lrs[0] - 0.1) < 1e-6, \"초기 학습률이 0.1이어야 합니다.\"\n",
    "assert abs(lrs[10] - 0.01) < 1e-6, \"에포크 10에서 학습률이 0.01이어야 합니다.\"\n",
    "assert abs(lrs[20] - 0.001) < 1e-6, \"에포크 20에서 학습률이 0.001이어야 합니다.\"\n",
    "print(\"Q5 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: `StepLR` 스케줄러를 생성하고 매 에포크마다 `scheduler.step()`을 호출합니다.\n",
    "- **핵심 개념**: `step_size` 에포크마다 학습률이 `gamma`배로 감소합니다.\n",
    "- **대안**: `ExponentialLR`, `CosineAnnealingLR`, `ReduceLROnPlateau` 등 다양한 스케줄러를 사용할 수 있습니다.\n",
    "- **실수 주의**: `scheduler.step()`을 호출하는 위치가 중요합니다 (보통 에포크 끝에서).\n",
    "- **실무 팁**: `ReduceLROnPlateau`는 검증 손실이 정체될 때만 학습률을 감소시켜 더 적응적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q6. 모델 저장/로드 (응용)\n",
    "\n",
    "**문제**: 간단한 MLP 모델을 생성하고, state_dict를 저장한 뒤 새로운 모델에 로드하세요. 두 모델의 첫 번째 레이어 가중치가 동일한지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. MLP 모델 정의\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# 2. 원본 모델 생성\n",
    "model_original = SimpleMLP()\n",
    "print(\"원본 모델 첫 번째 레이어 가중치 (일부):\")\n",
    "print(model_original.fc1.weight.data[0, :5])\n",
    "\n",
    "# 3. state_dict 저장\n",
    "torch.save(model_original.state_dict(), 'q6_model.pth')\n",
    "print(\"\\n모델 저장 완료: q6_model.pth\")\n",
    "\n",
    "# 4. 새로운 모델 생성 및 로드\n",
    "model_loaded = SimpleMLP()\n",
    "model_loaded.load_state_dict(torch.load('q6_model.pth'))\n",
    "model_loaded.eval()\n",
    "\n",
    "print(\"\\n로드된 모델 첫 번째 레이어 가중치 (일부):\")\n",
    "print(model_loaded.fc1.weight.data[0, :5])\n",
    "\n",
    "# 5. 동일성 확인\n",
    "is_equal = torch.equal(model_original.fc1.weight.data, model_loaded.fc1.weight.data)\n",
    "print(f\"\\n두 모델의 가중치가 동일한가? {is_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert is_equal, \"두 모델의 가중치가 동일해야 합니다.\"\n",
    "\n",
    "# 파일 정리\n",
    "import os\n",
    "if os.path.exists('q6_model.pth'):\n",
    "    os.remove('q6_model.pth')\n",
    "\n",
    "print(\"Q6 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: `torch.save()`로 state_dict를 저장하고, `load_state_dict()`로 새 모델에 로드합니다.\n",
    "- **핵심 개념**: state_dict는 모델의 학습 가능한 파라미터(가중치, 편향)를 딕셔너리 형태로 저장합니다.\n",
    "- **대안**: `torch.save(model, 'model.pth')`로 전체 모델을 저장할 수 있지만, 권장되지 않습니다.\n",
    "- **실수 주의**: 모델 구조가 다르면 `load_state_dict()`가 실패합니다.\n",
    "- **실무 팁**: state_dict 방식은 PyTorch 버전 간 호환성이 좋고, 모델 구조를 유연하게 변경할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q7. 체크포인트 저장 (복합)\n",
    "\n",
    "**문제**: 모델, 옵티마이저, 에포크, 손실을 포함한 체크포인트를 저장하고 로드하는 함수를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 체크포인트 저장 함수\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path='checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    학습 상태를 체크포인트로 저장\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"체크포인트 저장: epoch={epoch}, loss={loss:.4f}\")\n",
    "\n",
    "# 2. 체크포인트 로드 함수\n",
    "def load_checkpoint(model, optimizer, path='checkpoint.pth'):\n",
    "    \"\"\"\n",
    "    체크포인트에서 학습 상태 복원\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"체크포인트 로드: epoch={epoch}, loss={loss:.4f}\")\n",
    "    return epoch, loss\n",
    "\n",
    "# 3. 테스트용 모델과 옵티마이저 생성\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. 체크포인트 저장\n",
    "save_checkpoint(model, optimizer, epoch=15, loss=0.123, path='q7_checkpoint.pth')\n",
    "\n",
    "# 5. 새로운 모델/옵티마이저 생성 후 로드\n",
    "model_new = nn.Linear(10, 1)\n",
    "optimizer_new = optim.SGD(model_new.parameters(), lr=0.01)\n",
    "\n",
    "loaded_epoch, loaded_loss = load_checkpoint(model_new, optimizer_new, path='q7_checkpoint.pth')\n",
    "\n",
    "print(f\"\\n복원된 에포크: {loaded_epoch}\")\n",
    "print(f\"복원된 손실: {loaded_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert loaded_epoch == 15, \"에포크가 올바르게 복원되어야 합니다.\"\n",
    "assert abs(loaded_loss - 0.123) < 1e-6, \"손실이 올바르게 복원되어야 합니다.\"\n",
    "\n",
    "# 파일 정리\n",
    "import os\n",
    "if os.path.exists('q7_checkpoint.pth'):\n",
    "    os.remove('q7_checkpoint.pth')\n",
    "\n",
    "print(\"Q7 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: 모델과 옵티마이저의 state_dict, 에포크, 손실을 딕셔너리에 담아 저장합니다.\n",
    "- **핵심 개념**: 체크포인트를 사용하면 학습이 중단되어도 마지막 상태에서 재개할 수 있습니다.\n",
    "- **대안**: 스케줄러 상태(`scheduler.state_dict()`)도 함께 저장할 수 있습니다.\n",
    "- **실수 주의**: 로드 시 모델/옵티마이저 구조가 저장 시와 동일해야 합니다.\n",
    "- **실무 팁**: 긴 학습(예: 100 에포크 이상)에서는 일정 간격으로 체크포인트를 저장하는 것이 안전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q8. 과적합 진단 (복합)\n",
    "\n",
    "**문제**: 학습 손실과 검증 손실 곡선을 보고 과적합 여부를 판단하세요. 아래 데이터를 Plotly로 시각화하고, 과적합이 시작되는 에포크를 찾으세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# 1. 손실 데이터\n",
    "train_losses = [2.0, 1.5, 1.0, 0.7, 0.5, 0.3, 0.2, 0.15, 0.1, 0.08]\n",
    "val_losses = [2.1, 1.6, 1.1, 0.8, 0.7, 0.75, 0.8, 0.9, 1.0, 1.1]\n",
    "epochs = list(range(len(train_losses)))\n",
    "\n",
    "# 2. 과적합 시작 지점 찾기\n",
    "# 검증 손실이 최소인 지점 이후부터 과적합\n",
    "min_val_idx = np.argmin(val_losses)\n",
    "overfit_start = min_val_idx\n",
    "\n",
    "print(f\"검증 손실 최소 지점: 에포크 {min_val_idx} (val_loss = {val_losses[min_val_idx]})\")\n",
    "print(f\"과적합 시작: 에포크 {overfit_start + 1}부터 검증 손실 증가\")\n",
    "\n",
    "# 3. 시각화\n",
    "fig = go.Figure()\n",
    "\n",
    "# 학습 손실\n",
    "fig.add_trace(go.Scatter(x=epochs, y=train_losses, mode='lines+markers',\n",
    "                         name='Train Loss', line=dict(color='blue')))\n",
    "\n",
    "# 검증 손실\n",
    "fig.add_trace(go.Scatter(x=epochs, y=val_losses, mode='lines+markers',\n",
    "                         name='Val Loss', line=dict(color='red')))\n",
    "\n",
    "# 과적합 시작 지점 표시\n",
    "fig.add_vline(x=overfit_start, line_dash='dash', line_color='green',\n",
    "              annotation_text=f'과적합 시작 (epoch {overfit_start})')\n",
    "\n",
    "fig.update_layout(title='학습 곡선: 과적합 진단',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석 결과\n",
    "print(\"\\n과적합 분석 결과:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"1. 에포크 0~4: 학습 손실과 검증 손실 모두 감소 (정상)\")\n",
    "print(f\"2. 에포크 4: 검증 손실 최저점 (val_loss = {val_losses[4]})\")\n",
    "print(f\"3. 에포크 5~: 학습 손실은 계속 감소, 검증 손실 증가 (과적합!)\")\n",
    "print(f\"\\n권장: 에포크 4에서 Early Stopping 또는 모델 저장\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert overfit_start == 4, f\"과적합 시작은 에포크 4여야 합니다. 실제: {overfit_start}\"\n",
    "print(\"Q8 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: 검증 손실이 최소인 지점을 찾고, 그 이후부터 과적합이 시작된다고 판단합니다.\n",
    "- **핵심 개념**: 학습 손실은 계속 감소하지만 검증 손실이 증가하면 과적합입니다.\n",
    "- **대안**: 학습-검증 손실 차이가 커지는 시점을 기준으로 할 수도 있습니다.\n",
    "- **실수 주의**: 검증 손실의 자연스러운 변동(노이즈)과 실제 과적합을 구분해야 합니다.\n",
    "- **실무 팁**: 과적합 방지를 위해 Early Stopping, Dropout, 데이터 증강 등을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q9. Fashion-MNIST DNN (종합)\n",
    "\n",
    "**문제**: 다음 구조의 DNN을 정의하고 Fashion-MNIST에서 10 에포크 학습하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. 데이터 로드\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"학습 데이터: {len(train_dataset)}개\")\n",
    "print(f\"테스트 데이터: {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DNN 모델 정의\n",
    "class FashionDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # 레이어 1: 784 -> 512\n",
    "            nn.Linear(784, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            # 레이어 2: 512 -> 256\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            # 레이어 3: 256 -> 128\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            # 출력층: 128 -> 10\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "        # He 초기화\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.network(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = FashionDNN().to(device)\n",
    "print(model)\n",
    "\n",
    "# 파라미터 수\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n총 파라미터: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 학습 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. 학습 루프\n",
    "epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    # 기록\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n최종 테스트 정확도: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert history['val_acc'][-1] > 85, \"테스트 정확도가 85% 이상이어야 합니다.\"\n",
    "print(\"Q9 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: 784->512->256->128->10 구조의 DNN을 정의하고, BatchNorm, ReLU, Dropout을 순서대로 적용합니다.\n",
    "- **핵심 개념**: He 초기화, BatchNorm, Dropout을 조합하여 안정적이고 일반화된 학습을 달성합니다.\n",
    "- **대안**: Leaky ReLU, SeLU 등 다른 활성화 함수를 사용할 수 있습니다.\n",
    "- **실수 주의**: `.train()`과 `.eval()` 모드 전환을 잊지 않아야 합니다.\n",
    "- **실무 팁**: Fashion-MNIST는 비교적 간단한 데이터셋이므로 DNN으로도 88-90% 정확도를 달성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q10. 종합 파이프라인 (종합)\n",
    "\n",
    "**문제**: Fashion-MNIST 학습 파이프라인을 완성하세요. 다음 요소를 모두 포함해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. 데이터 준비\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DNN 모델 정의 (He 초기화, BatchNorm, Dropout)\n",
    "class CompleteDNN(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dims=[256, 128, 64], output_dim=10, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # He 초기화\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.network(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = CompleteDNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 학습 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ReduceLROnPlateau 스케줄러\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "# Early Stopping (patience=5)\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "# 최고 모델 체크포인트\n",
    "model_ckpt = ModelCheckpoint('q10_best_model.pth')\n",
    "\n",
    "print(\"학습 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 함수\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(test_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 학습 실행\n",
    "epochs = 30\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "print(\"학습 시작!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 학습\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # 검증\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # 학습률\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 기록\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # 스케줄러 업데이트\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # 체크포인트 저장\n",
    "    model_ckpt(val_loss, model)\n",
    "    \n",
    "    # Early Stopping 체크\n",
    "    if early_stopper(val_loss):\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "print(\"\\n학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 학습 곡선 시각화 (Plotly)\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=['Loss', 'Accuracy', 'Learning Rate'])\n",
    "\n",
    "# Loss\n",
    "fig.add_trace(go.Scatter(y=history['train_loss'], name='Train Loss', line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=history['val_loss'], name='Val Loss', line=dict(color='red')), row=1, col=1)\n",
    "\n",
    "# Accuracy\n",
    "fig.add_trace(go.Scatter(y=history['train_acc'], name='Train Acc', line=dict(color='blue')), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=history['val_acc'], name='Val Acc', line=dict(color='red')), row=1, col=2)\n",
    "\n",
    "# Learning Rate\n",
    "fig.add_trace(go.Scatter(y=history['lr'], name='LR', line=dict(color='green')), row=1, col=3)\n",
    "\n",
    "fig.update_layout(title='Q10: 종합 학습 파이프라인', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 최고 성능 모델 로드 및 최종 평가\n",
    "model.load_state_dict(torch.load('q10_best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "final_loss, final_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"\\n최고 성능 모델 테스트 정확도: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert final_acc > 85, \"테스트 정확도가 85% 이상이어야 합니다.\"\n",
    "assert len(history['train_loss']) > 0, \"학습 기록이 있어야 합니다.\"\n",
    "\n",
    "# 파일 정리\n",
    "import os\n",
    "if os.path.exists('q10_best_model.pth'):\n",
    "    os.remove('q10_best_model.pth')\n",
    "\n",
    "print(\"Q10 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**\n",
    "\n",
    "- **접근 방법**: DNN, Adam, ReduceLROnPlateau, Early Stopping, 체크포인트를 모두 조합하여 완전한 학습 파이프라인을 구성합니다.\n",
    "- **핵심 개념**: 각 컴포넌트가 협력하여 안정적이고 효율적인 학습을 달성합니다.\n",
    "- **대안**: SGD+Momentum, CosineAnnealingLR 등 다른 조합도 가능합니다.\n",
    "- **실수 주의**: Early Stopping이 너무 일찍 발동되지 않도록 patience를 적절히 설정해야 합니다.\n",
    "- **실무 팁**: 이 패턴은 대부분의 딥러닝 프로젝트에서 재사용할 수 있는 표준적인 구조입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 언제 사용? |\n",
    "|-----|----------|----------|\n",
    "| 가중치 초기화 | Xavier(Sigmoid/Tanh), He(ReLU) | 모든 신경망 |\n",
    "| BatchNorm | 미니배치 정규화 | 깊은 네트워크, 빠른 수렴 |\n",
    "| Dropout | 무작위 뉴런 비활성화 | 과적합 방지 |\n",
    "| Early Stopping | 검증 손실 모니터링 | 자동 학습 종료 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 메서드 | 언제 사용? |\n",
    "|-----|-----------|----------|\n",
    "| LR Scheduler | StepLR, ReduceLROnPlateau | 학습률 동적 조정 |\n",
    "| 모델 저장 | torch.save(state_dict) | 학습 결과 보존 |\n",
    "| 체크포인트 | 모델+옵티마이저+에포크 | 학습 재개, 최고 모델 저장 |\n",
    "\n",
    "### DNN 모델 구조 패턴\n",
    "\n",
    "```python\n",
    "# 권장 패턴\n",
    "Linear -> BatchNorm -> ReLU -> Dropout\n",
    "Linear -> BatchNorm -> ReLU -> Dropout\n",
    "...\n",
    "Linear (출력층)\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **He 초기화**: ReLU 계열 활성화 함수와 함께 사용\n",
    "2. **BatchNorm**: 학습 속도 향상, 더 높은 학습률 사용 가능\n",
    "3. **Dropout**: 0.3~0.5 범위로 시작, 필요에 따라 조정\n",
    "4. **Early Stopping**: patience=5~10 권장\n",
    "5. **체크포인트**: 긴 학습 시 반드시 사용\n",
    "6. **eval() 모드**: 평가 시 반드시 `model.eval()` 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
