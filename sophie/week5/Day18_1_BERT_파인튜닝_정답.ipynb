{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day18_1: BERT 파인튜닝 - 정답\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1. BERT 개념 이해하기\n",
    "\n",
    "**문제**: BERT가 \"Bidirectional\"이라고 불리는 이유를 설명하고, 기존 단방향 모델(GPT 등)과의 차이점을 서술하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer_q1 = \"\"\"\n",
    "BERT가 'Bidirectional'이라고 불리는 이유:\n",
    "\n",
    "1. 양방향 문맥 이해:\n",
    "   - BERT는 Transformer의 Encoder만 사용하여 입력 시퀀스의 모든 토큰을 동시에 처리합니다.\n",
    "   - 각 토큰은 Self-Attention을 통해 좌측과 우측 모든 토큰의 정보를 참조할 수 있습니다.\n",
    "   - 따라서 \"진정한 양방향(truly bidirectional)\" 모델이라고 합니다.\n",
    "\n",
    "2. 기존 단방향 모델(GPT)과의 차이:\n",
    "   \n",
    "   | 특성 | GPT (단방향) | BERT (양방향) |\n",
    "   |------|-------------|---------------|\n",
    "   | 문맥 참조 | 좌측 토큰만 | 좌측 + 우측 모두 |\n",
    "   | Masking | Causal Masking | 없음 (MLM용 마스킹만) |\n",
    "   | 학습 방식 | 다음 토큰 예측 | 마스킹된 토큰 예측 |\n",
    "   | 적합한 태스크 | 텍스트 생성 | 분류, 이해 |\n",
    "\n",
    "3. 예시:\n",
    "   문장: \"나는 [MASK]에 가서 돈을 찾았다.\"\n",
    "   \n",
    "   - GPT: '나는'만 보고 [MASK] 예측 -> 어려움\n",
    "   - BERT: '나는' + '돈을 찾았다' 모두 보고 예측 -> '은행' 예측 가능\n",
    "\"\"\"\n",
    "\n",
    "print(answer_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: 양방향 문맥의 중요성 시연\n",
    "example = \"나는 [MASK]에 가서 돈을 찾았다.\"\n",
    "\n",
    "print(\"양방향 문맥 이해 예시:\")\n",
    "print(f\"문장: {example}\")\n",
    "print(\"\\n단방향(GPT 스타일):\")\n",
    "print(\"  - '나는'만으로는 [MASK]가 무엇인지 알기 어려움\")\n",
    "print(\"\\n양방향(BERT):\")\n",
    "print(\"  - '돈을 찾았다'를 함께 보면 '은행'임을 알 수 있음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: BERT는 Transformer Encoder의 Self-Attention을 사용하여 모든 토큰이 서로를 참조할 수 있습니다.\n",
    "- **대안**: ELMo는 양방향 LSTM을 사용하지만, 좌->우와 우->좌를 별도로 학습한 후 결합합니다. BERT는 동시에 양방향을 처리합니다.\n",
    "- **실무 팁**: 분류, NER, QA 등 \"이해\" 태스크에는 BERT가 적합하고, 텍스트 생성에는 GPT가 적합합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. MLM과 NSP 이해하기\n",
    "\n",
    "**문제**: BERT의 두 가지 사전학습 과제인 MLM과 NSP의 역할을 각각 한 문장으로 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer_q2 = \"\"\"\n",
    "MLM (Masked Language Model):\n",
    "- 입력 토큰의 15%를 마스킹하고 원래 토큰을 예측하게 하여,\n",
    "  BERT가 양방향 문맥을 바탕으로 단어의 의미와 문법을 학습하게 합니다.\n",
    "\n",
    "NSP (Next Sentence Prediction):\n",
    "- 두 문장이 연속된 문장인지 예측하게 하여,\n",
    "  BERT가 문장 간의 관계와 담화의 일관성을 이해하게 합니다.\n",
    "\n",
    "역할 요약:\n",
    "- MLM: 단어 수준의 언어 이해 (문맥적 의미, 문법)\n",
    "- NSP: 문장 수준의 언어 이해 (문장 관계, 논리적 흐름)\n",
    "\"\"\"\n",
    "\n",
    "print(answer_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: MLM과 NSP 예시\n",
    "print(\"MLM 예시:\")\n",
    "print(\"  원본: '오늘 날씨가 좋다'\")\n",
    "print(\"  입력: '오늘 [MASK]가 좋다'\")\n",
    "print(\"  예측: '날씨' (양방향 문맥 활용)\")\n",
    "\n",
    "print(\"\\nNSP 예시:\")\n",
    "print(\"  문장 A: '오늘 비가 왔다.'\")\n",
    "print(\"  문장 B: '우산을 챙겨야겠다.'\")\n",
    "print(\"  예측: IsNext (두 문장이 논리적으로 연결됨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: MLM은 Cloze Test와 유사하며, NSP는 문장 관계 추론을 학습합니다.\n",
    "- **대안**: RoBERTa는 NSP를 제거하고 더 많은 데이터로 MLM만 학습하여 성능을 개선했습니다.\n",
    "- **실무 팁**: 문장 쌍을 다루는 태스크(문장 유사도, 자연어 추론)에서 NSP 학습이 도움됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. 토크나이저 사용하기\n",
    "\n",
    "**문제**: 아래 텍스트를 토크나이저로 변환하고, 결과를 출력하세요.\n",
    "\n",
    "```python\n",
    "text = \"Hugging Face 라이브러리는 정말 편리합니다!\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text = \"Hugging Face 라이브러리는 정말 편리합니다!\"\n",
    "\n",
    "# 1. 토큰 리스트\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"1. 토큰 리스트: {tokens}\")\n",
    "\n",
    "# 2. 토큰 ID 리스트\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"2. 토큰 ID 리스트: {token_ids}\")\n",
    "\n",
    "# 3. attention_mask\n",
    "encoded = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=20,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(f\"3. attention_mask: {encoded['attention_mask'][0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: 추가 분석\n",
    "print(\"\\n추가 분석:\")\n",
    "print(f\"  총 토큰 수: {len(tokens)}\")\n",
    "print(f\"  input_ids (특수 토큰 포함): {encoded['input_ids'][0].tolist()}\")\n",
    "print(f\"  디코딩 결과: {tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)}\")\n",
    "\n",
    "# WordPiece 토큰화 확인\n",
    "print(\"\\nWordPiece 토큰화 특징:\")\n",
    "for token in tokens:\n",
    "    if token.startswith('##'):\n",
    "        print(f\"  '{token}': 이전 토큰의 연속 (subword)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **접근 방법**: `tokenize()`로 토큰 분리, `convert_tokens_to_ids()`로 ID 변환, `encode_plus()`로 전체 인코딩\n",
    "- **핵심 개념**: `##`로 시작하는 토큰은 WordPiece의 subword를 나타냅니다.\n",
    "- **실수 주의**: `padding='max_length'` 사용 시 attention_mask에서 0은 패딩을 의미합니다.\n",
    "- **실무 팁**: batch 처리 시 `tokenizer(texts, padding=True, truncation=True)`가 더 편리합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. 모델 로드 및 출력 shape\n",
    "\n",
    "**문제**: `bert-base-multilingual-cased` 모델을 로드하고, 아래 텍스트의 출력 shape을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"딥러닝 모델을 학습합니다.\"\n",
    "\n",
    "# 토크나이징\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(f\"입력 텍스트: {text}\")\n",
    "print(f\"입력 토큰 수: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "# 모델 순전파\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 출력 shape 확인\n",
    "print(f\"\\n출력 shape:\")\n",
    "print(f\"  last_hidden_state: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"    (batch_size={outputs.last_hidden_state.shape[0]}, \"\n",
    "      f\"seq_len={outputs.last_hidden_state.shape[1]}, \"\n",
    "      f\"hidden_size={outputs.last_hidden_state.shape[2]})\")\n",
    "print(f\"  pooler_output: {outputs.pooler_output.shape}\")\n",
    "print(f\"    ([CLS] 토큰의 출력, hidden_size={outputs.pooler_output.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: 토큰별 출력 확인\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"토큰별 hidden state:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    hidden = outputs.last_hidden_state[0, i, :5]  # 처음 5개 값만\n",
    "    print(f\"  {token:15s}: {hidden.tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **접근 방법**: `AutoModel.from_pretrained()`로 모델 로드, `model(**inputs)`로 순전파\n",
    "- **핵심 개념**: \n",
    "  - `last_hidden_state`: 모든 토큰의 최종 hidden state (seq_len, hidden_size)\n",
    "  - `pooler_output`: [CLS] 토큰의 hidden state에 추가 레이어 적용된 출력\n",
    "- **대안**: `model.config.output_hidden_states=True`로 모든 레이어의 출력을 받을 수 있습니다.\n",
    "- **실무 팁**: 분류 태스크에서는 `pooler_output`을, 토큰 분류에서는 `last_hidden_state`를 사용합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. 특수 토큰 이해\n",
    "\n",
    "**문제**: BERT의 특수 토큰 [CLS], [SEP], [PAD], [MASK]의 역할을 각각 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer_q5 = \"\"\"\n",
    "BERT 특수 토큰의 역할:\n",
    "\n",
    "1. [CLS] (Classification Token)\n",
    "   - 위치: 모든 입력 시퀀스의 맨 앞\n",
    "   - 역할: 전체 시퀀스의 집약된 표현을 담음\n",
    "   - 용도: 문장 분류, 문장 유사도 등의 태스크에서 최종 출력으로 사용\n",
    "   - 예시: [CLS] 오늘 날씨가 좋다 [SEP] -> [CLS]의 출력으로 긍/부정 분류\n",
    "\n",
    "2. [SEP] (Separator Token)\n",
    "   - 위치: 문장의 끝 또는 문장 사이\n",
    "   - 역할: 문장의 경계를 구분\n",
    "   - 용도: 단일 문장의 끝 표시, 문장 쌍의 구분\n",
    "   - 예시: [CLS] 문장A [SEP] 문장B [SEP]\n",
    "\n",
    "3. [PAD] (Padding Token)\n",
    "   - 위치: 실제 토큰 이후의 빈 공간\n",
    "   - 역할: 배치 내 시퀀스 길이를 동일하게 맞춤\n",
    "   - 용도: attention_mask=0으로 모델이 무시하도록 함\n",
    "   - 예시: [CLS] 안녕 [SEP] [PAD] [PAD] [PAD]\n",
    "\n",
    "4. [MASK] (Mask Token)\n",
    "   - 위치: MLM 사전학습 시 마스킹된 위치\n",
    "   - 역할: 모델이 예측해야 할 토큰을 숨김\n",
    "   - 용도: MLM 학습 (Fine-tuning 시에는 거의 사용 안 함)\n",
    "   - 예시: 오늘 [MASK]가 좋다 -> '날씨' 예측\n",
    "\"\"\"\n",
    "\n",
    "print(answer_q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: 특수 토큰 ID 확인\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "print(\"특수 토큰 ID:\")\n",
    "print(f\"  [CLS]: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"  [SEP]: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"  [PAD]: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  [MASK]: {tokenizer.mask_token} (ID: {tokenizer.mask_token_id})\")\n",
    "print(f\"  [UNK]: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: 특수 토큰은 모델이 입력 구조를 이해하는 데 필수적입니다.\n",
    "- **실수 주의**: [PAD] 토큰에는 반드시 attention_mask=0을 설정해야 합니다.\n",
    "- **실무 팁**: 문장 쌍 입력 시 `token_type_ids`로 어느 문장에 속하는지 구분합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. 다운스트림 태스크 이해\n",
    "\n",
    "**문제**: BERT를 사용한 텍스트 분류에서 왜 [CLS] 토큰의 출력을 사용하는지 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer_q6 = \"\"\"\n",
    "[CLS] 토큰의 출력을 사용하는 이유:\n",
    "\n",
    "1. 전체 시퀀스의 집약된 표현:\n",
    "   - [CLS] 토큰은 Self-Attention을 통해 모든 토큰의 정보를 종합합니다.\n",
    "   - 12개 레이어(BERT-base)를 거치면서 문장 전체의 의미가 [CLS]에 응축됩니다.\n",
    "   - 따라서 [CLS] 출력 하나로 문장 전체를 대표할 수 있습니다.\n",
    "\n",
    "2. 고정된 크기의 출력:\n",
    "   - 문장 길이와 무관하게 항상 동일한 크기(hidden_size=768)의 벡터\n",
    "   - 분류기(Linear Layer)에 바로 입력 가능\n",
    "\n",
    "3. NSP 사전학습의 활용:\n",
    "   - BERT는 NSP 과제에서 [CLS] 토큰으로 문장 관계를 예측하도록 학습됨\n",
    "   - 이미 문장 수준의 표현을 담도록 사전학습되어 있음\n",
    "\n",
    "4. 대안과 비교:\n",
    "   - 모든 토큰 평균: 정보가 희석될 수 있음\n",
    "   - 마지막 토큰: 위치에 따른 편향 발생\n",
    "   - [CLS]: 위치가 고정되고 전체 정보를 담도록 학습됨\n",
    "\"\"\"\n",
    "\n",
    "print(answer_q6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: [CLS] 토큰 출력 확인\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text = \"이 영화는 정말 재미있었습니다.\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# [CLS] 토큰 출력\n",
    "cls_output = outputs.last_hidden_state[:, 0, :]  # 첫 번째 토큰 = [CLS]\n",
    "pooler_output = outputs.pooler_output  # pooler를 거친 출력\n",
    "\n",
    "print(f\"[CLS] hidden state shape: {cls_output.shape}\")\n",
    "print(f\"pooler_output shape: {pooler_output.shape}\")\n",
    "print(f\"\\n차이: pooler_output은 [CLS]에 추가 Linear + Tanh 적용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: [CLS]는 분류를 위해 설계된 특수 토큰으로, 전체 시퀀스 정보를 담습니다.\n",
    "- **대안**: 문장 임베딩에서는 모든 토큰의 평균(mean pooling)이 더 좋을 수 있습니다.\n",
    "- **실무 팁**: `AutoModelForSequenceClassification`은 내부적으로 pooler_output을 사용합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Fine-tuning 전략 비교\n",
    "\n",
    "**문제**: \"전체 Fine-tuning\"과 \"Feature Extraction\" 전략의 차이점을 설명하고, 각각 언제 사용하면 좋은지 서술하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer_q7 = \"\"\"\n",
    "Fine-tuning 전략 비교:\n",
    "\n",
    "1. 전체 Fine-tuning (Full Fine-tuning)\n",
    "   - 방법: BERT 전체 레이어 + 분류 헤드 모두 학습\n",
    "   - 장점:\n",
    "     * 태스크에 최적화된 표현 학습 가능\n",
    "     * 최고 성능 달성 가능\n",
    "   - 단점:\n",
    "     * 많은 연산량과 메모리 필요\n",
    "     * 과적합 위험 (데이터가 적을 때)\n",
    "     * 학습 시간이 오래 걸림\n",
    "   - 사용 시점:\n",
    "     * 레이블된 데이터가 충분할 때 (수천~수만 개 이상)\n",
    "     * 도메인이 사전학습 데이터와 비슷할 때\n",
    "     * 최고 성능이 필요할 때\n",
    "\n",
    "2. Feature Extraction (특징 추출)\n",
    "   - 방법: BERT 가중치 고정, 분류 헤드만 학습\n",
    "   - 장점:\n",
    "     * 빠른 학습 (수 분 내 완료)\n",
    "     * 적은 메모리 사용\n",
    "     * 과적합 방지\n",
    "   - 단점:\n",
    "     * 성능 상한이 존재\n",
    "     * 태스크 특화 표현 학습 불가\n",
    "   - 사용 시점:\n",
    "     * 레이블된 데이터가 매우 적을 때 (수백 개 이하)\n",
    "     * 빠른 프로토타이핑이 필요할 때\n",
    "     * 컴퓨팅 자원이 제한적일 때\n",
    "\n",
    "3. 선택 가이드:\n",
    "   데이터 < 1,000개: Feature Extraction\n",
    "   데이터 1,000~10,000개: 둘 다 시도 후 비교\n",
    "   데이터 > 10,000개: 전체 Fine-tuning\n",
    "\"\"\"\n",
    "\n",
    "print(answer_q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증: 학습 가능 파라미터 수 비교\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "def count_params(model, trainable_only=False):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"파라미터 수 비교:\")\n",
    "print(f\"  전체 파라미터: {count_params(model):,}\")\n",
    "print(f\"  전체 Fine-tuning 시 학습 파라미터: {count_params(model, trainable_only=True):,}\")\n",
    "\n",
    "# BERT 고정\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"  Feature Extraction 시 학습 파라미터: {count_params(model, trainable_only=True):,}\")\n",
    "print(f\"\\n파라미터 비율: {count_params(model, trainable_only=True) / count_params(model) * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: Fine-tuning은 데이터 양과 도메인 유사성에 따라 전략을 선택합니다.\n",
    "- **대안**: Gradual Unfreezing은 상위 레이어부터 점진적으로 학습하는 중간 전략입니다.\n",
    "- **실무 팁**: 먼저 Feature Extraction으로 빠르게 베이스라인을 만들고, 필요시 Fine-tuning으로 개선합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. 한국어 토큰화\n",
    "\n",
    "**문제**: 아래 두 문장을 토크나이저로 변환하고, 토큰화 결과를 비교 분석하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "text1 = \"자연어 처리\"\n",
    "text2 = \"Natural Language Processing\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(text1)\n",
    "tokens2 = tokenizer.tokenize(text2)\n",
    "\n",
    "print(\"토큰화 결과 비교:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n한국어: '{text1}'\")\n",
    "print(f\"  토큰: {tokens1}\")\n",
    "print(f\"  토큰 수: {len(tokens1)}\")\n",
    "\n",
    "print(f\"\\n영어: '{text2}'\")\n",
    "print(f\"  토큰: {tokens2}\")\n",
    "print(f\"  토큰 수: {len(tokens2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석\n",
    "analysis = \"\"\"\n",
    "한국어와 영어 토큰화 차이점:\n",
    "\n",
    "1. 토큰 수:\n",
    "   - 한국어: 더 많은 토큰으로 분리되는 경향\n",
    "   - 영어: 단어 단위로 비교적 적은 토큰\n",
    "   - 이유: mBERT의 어휘(vocab)가 영어 중심으로 구성됨\n",
    "\n",
    "2. 서브워드 분리:\n",
    "   - 한국어: 한글 음절이 개별 토큰으로 분리되기 쉬움\n",
    "   - 영어: 일반적인 단어는 통째로 유지됨\n",
    "\n",
    "3. ##접두사:\n",
    "   - 한국어에서 더 빈번하게 나타남\n",
    "   - 어휘에 없는 한국어 단어가 조각남\n",
    "\n",
    "4. 실무 시사점:\n",
    "   - 한국어 특화 모델(KoBERT, KoELECTRA)이 더 효율적\n",
    "   - mBERT는 범용성이 있지만 한국어 처리에 비효율적\n",
    "   - max_length를 한국어에서는 더 길게 설정 권장\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **핵심 개념**: mBERT의 WordPiece 어휘는 영어 중심이라 한국어가 더 많이 분리됩니다.\n",
    "- **대안**: 한국어 특화 모델(klue/bert-base, monologg/kobert)을 사용하면 더 효율적입니다.\n",
    "- **실무 팁**: 한국어 모델에서는 max_length를 영어보다 1.5~2배 길게 설정하는 것이 좋습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. 감성 분류 모델 개선\n",
    "\n",
    "**문제**: 본문의 NSMC 감성 분류 코드를 수정하여 다음을 구현하세요.\n",
    "\n",
    "1. BERT 레이어를 고정(freeze)한 상태로 학습\n",
    "2. 고정된 상태와 고정하지 않은 상태의 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드 (샘플)\n",
    "try:\n",
    "    train_df = pd.read_csv(\"../datasets/text/nsmc/ratings_train.txt\", sep=\"\\t\")\n",
    "    test_df = pd.read_csv(\"../datasets/text/nsmc/ratings_test.txt\", sep=\"\\t\")\n",
    "except:\n",
    "    train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "    test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    train_df = pd.read_csv(train_url, sep=\"\\t\")\n",
    "    test_df = pd.read_csv(test_url, sep=\"\\t\")\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "# 샘플링\n",
    "train_sample = train_df.sample(n=2000, random_state=42).reset_index(drop=True)\n",
    "test_sample = test_df.sample(n=500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"훈련: {len(train_sample)}, 테스트: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.iloc[idx]['document'])\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_model(freeze_bert=False, epochs=2):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 및 토크나이저\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # BERT 고정 여부\n",
    "    if freeze_bert:\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"BERT 레이어 고정됨 (Feature Extraction)\")\n",
    "    else:\n",
    "        print(\"전체 Fine-tuning\")\n",
    "    \n",
    "    # 데이터셋\n",
    "    train_dataset = NSMCDataset(train_sample, tokenizer)\n",
    "    test_dataset = NSMCDataset(test_sample, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "    \n",
    "    # 학습\n",
    "    train_losses = []\n",
    "    eval_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        \n",
    "        # 평가\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        eval_accs.append(correct / total)\n",
    "        print(f\"  Epoch {epoch+1}: Loss={train_losses[-1]:.4f}, Acc={eval_accs[-1]:.4f}\")\n",
    "    \n",
    "    return train_losses, eval_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 실험\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Feature Extraction (BERT 고정)\")\n",
    "print(\"=\" * 50)\n",
    "frozen_losses, frozen_accs = train_model(freeze_bert=True, epochs=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2. Full Fine-tuning (전체 학습)\")\n",
    "print(\"=\" * 50)\n",
    "full_losses, full_accs = train_model(freeze_bert=False, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 비교 시각화\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Loss', 'Accuracy'])\n",
    "\n",
    "# Loss\n",
    "fig.add_trace(go.Scatter(y=frozen_losses, name='Feature Extraction', mode='lines+markers'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=full_losses, name='Full Fine-tuning', mode='lines+markers'), row=1, col=1)\n",
    "\n",
    "# Accuracy\n",
    "fig.add_trace(go.Scatter(y=frozen_accs, name='Feature Extraction', mode='lines+markers', showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=full_accs, name='Full Fine-tuning', mode='lines+markers', showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Epoch')\n",
    "fig.update_layout(title='Fine-tuning 전략 비교', height=400, template='plotly_white')\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\n최종 정확도:\")\n",
    "print(f\"  Feature Extraction: {frozen_accs[-1]:.4f}\")\n",
    "print(f\"  Full Fine-tuning: {full_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **접근 방법**: `param.requires_grad = False`로 BERT 가중치를 고정합니다.\n",
    "- **핵심 개념**: Feature Extraction은 빠르지만 성능 상한이 있고, Full Fine-tuning은 더 높은 성능을 달성합니다.\n",
    "- **실수 주의**: 옵티마이저에 `filter(lambda p: p.requires_grad, model.parameters())`를 사용해야 합니다.\n",
    "- **실무 팁**: 데이터가 적을 때는 Feature Extraction으로 시작하고, 데이터가 충분하면 Fine-tuning으로 개선합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. 전체 파이프라인 구현\n",
    "\n",
    "**문제**: Hugging Face의 `pipeline` API를 사용하여 감성 분석을 수행하는 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 감성 분석 파이프라인 생성\n",
    "# 한국어에 적합한 사전학습 모델 사용\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",  # 다국어 감성 모델\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "reviews = [\n",
    "    \"이 제품 정말 좋아요! 강추합니다.\",\n",
    "    \"배송이 너무 늦고 포장이 엉망이었어요.\",\n",
    "    \"가격 대비 괜찮은 것 같아요.\"\n",
    "]\n",
    "\n",
    "print(\"Hugging Face Pipeline 감성 분석 결과:\")\n",
    "print(\"=\"*60)\n",
    "results = sentiment_pipeline(reviews)\n",
    "\n",
    "for review, result in zip(reviews, results):\n",
    "    # nlptown 모델은 1~5 별점으로 출력\n",
    "    stars = int(result['label'].split()[0])\n",
    "    sentiment = \"긍정\" if stars >= 4 else \"부정\" if stars <= 2 else \"중립\"\n",
    "    print(f\"\\n리뷰: {review}\")\n",
    "    print(f\"  별점: {'*' * stars} ({stars}/5)\")\n",
    "    print(f\"  감성: {sentiment} (신뢰도: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대안: 학습된 모델로 파이프라인 생성\n",
    "# (이전에 학습한 모델이 있다면)\n",
    "\n",
    "# 방법 1: 모델 저장 후 로드\n",
    "# model.save_pretrained(\"./my_sentiment_model\")\n",
    "# tokenizer.save_pretrained(\"./my_sentiment_model\")\n",
    "# my_pipeline = pipeline(\"sentiment-analysis\", model=\"./my_sentiment_model\")\n",
    "\n",
    "# 방법 2: 모델과 토크나이저 직접 전달\n",
    "# my_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"\\nPipeline API 장점:\")\n",
    "print(\"  1. 간단한 인터페이스: 3줄로 추론 가능\")\n",
    "print(\"  2. 자동 전처리: 토크나이징, 패딩 자동 처리\")\n",
    "print(\"  3. 배치 처리: 여러 문장 한 번에 처리\")\n",
    "print(\"  4. GPU 지원: device 파라미터로 쉽게 설정\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "- **접근 방법**: `pipeline()`으로 간단하게 추론 파이프라인을 생성합니다.\n",
    "- **핵심 개념**: Pipeline API는 전처리, 추론, 후처리를 자동으로 처리합니다.\n",
    "- **대안**: 더 세밀한 제어가 필요하면 직접 토크나이저와 모델을 사용합니다.\n",
    "- **실무 팁**: 프로덕션에서는 `pipeline`보다 직접 배치 처리가 더 효율적일 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "\n",
    "### 퀴즈 핵심 요약\n",
    "\n",
    "| 퀴즈 | 핵심 개념 | 실무 적용 |\n",
    "|------|----------|----------|\n",
    "| Q1 | 양방향 문맥 이해 | 분류 vs 생성 모델 선택 |\n",
    "| Q2 | MLM/NSP 사전학습 | 언어 이해 능력 학습 |\n",
    "| Q3 | 토크나이저 사용 | 입력 데이터 준비 |\n",
    "| Q4 | 모델 출력 shape | 다운스트림 태스크 연결 |\n",
    "| Q5 | 특수 토큰 | 입력 형식 이해 |\n",
    "| Q6 | [CLS] 토큰 | 문장 분류 구현 |\n",
    "| Q7 | Fine-tuning 전략 | 데이터 양에 따른 선택 |\n",
    "| Q8 | 다국어 토큰화 | 한국어 처리 최적화 |\n",
    "| Q9 | 가중치 고정 | 효율적인 학습 |\n",
    "| Q10 | Pipeline API | 빠른 프로토타이핑 |\n",
    "\n",
    "### 실무 체크리스트\n",
    "\n",
    "- [ ] 모델과 토크나이저는 같은 것 사용\n",
    "- [ ] 데이터 양에 따라 Fine-tuning 전략 선택\n",
    "- [ ] 학습률은 2e-5 ~ 5e-5로 설정\n",
    "- [ ] Warmup과 Gradient Clipping 적용\n",
    "- [ ] 한국어는 max_length 여유있게 설정\n",
    "- [ ] 배포 시 모델 저장 및 Pipeline 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
