{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_0: PyTorch 기초 (Tensor & Autograd)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. PyTorch 텐서 생성 및 기본 연산 이해하기\n",
    "2. 텐서의 shape, dtype, device 속성 다루기\n",
    "3. NumPy와 PyTorch 텐서 변환하기\n",
    "4. GPU 가속(CUDA) 개념 이해하기\n",
    "5. 텐서 인덱싱과 슬라이싱 활용하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. Autograd(자동 미분) 원리 이해하기\n",
    "2. requires_grad와 backward() 활용하기\n",
    "3. nn.Module로 신경망 정의하기\n",
    "4. Dataset과 DataLoader로 데이터 공급하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| Tensor | 딥러닝 데이터 표현 | 이미지, 텍스트, 시계열 데이터 |\n",
    "| Autograd | 자동 그래디언트 계산 | 역전파 학습 자동화 |\n",
    "| nn.Module | 신경망 구조 정의 | MLP, CNN, RNN 모델 설계 |\n",
    "| DataLoader | 효율적 데이터 공급 | 배치 처리, 셔플링 |\n",
    "\n",
    "**분석가 관점**: PyTorch는 연구와 프로덕션 모두에서 가장 인기 있는 딥러닝 프레임워크입니다. 직관적인 Python 스타일과 동적 계산 그래프로 디버깅이 쉽습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 PyTorch 소개\n",
    "\n",
    "### 딥러닝 프레임워크 비교\n",
    "\n",
    "| 구분 | PyTorch | TensorFlow |\n",
    "|------|---------|------------|\n",
    "| 개발사 | Meta (Facebook) | Google |\n",
    "| 계산 그래프 | 동적 (Define-by-Run) | 정적 (TF1) / 동적 (TF2) |\n",
    "| 디버깅 | Python 스타일 (쉬움) | 상대적으로 복잡 |\n",
    "| 연구 | 학계에서 선호 | 산업에서 선호 |\n",
    "| 장점 | 직관적, 빠른 프로토타입 | 대규모 배포 용이 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel '.venv (Python 3.13.11)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. listen EFAULT: bad address in system call argument 0.0.0.0:9006"
     ]
    }
   ],
   "source": [
    "# PyTorch 설치 확인\n",
    "import torch\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "\n",
    "# CUDA (GPU) 사용 가능 여부\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CPU 모드로 실행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 텐서 생성\n",
    "\n",
    "### 텐서(Tensor)란?\n",
    "\n",
    "- 다차원 배열 (NumPy의 ndarray와 유사)\n",
    "- GPU에서 연산 가능\n",
    "- 자동 미분 지원\n",
    "\n",
    "```\n",
    "스칼라(0D) -> 벡터(1D) -> 행렬(2D) -> 텐서(3D+)\n",
    "    5         [1,2,3]     [[1,2],[3,4]]   [[[1,2],[3,4]],[[5,6],[7,8]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. torch.tensor() - 직접 값 지정\n",
    "scalar = torch.tensor(5)               # 스칼라 (0차원)\n",
    "vector = torch.tensor([1, 2, 3])       # 벡터 (1차원)\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])  # 행렬 (2차원)\n",
    "\n",
    "print(f\"스칼라: {scalar}, shape: {scalar.shape}\")\n",
    "print(f\"벡터: {vector}, shape: {vector.shape}\")\n",
    "print(f\"행렬:\\n{matrix}, shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 특수 텐서 생성 함수\n",
    "zeros = torch.zeros(3, 4)          # 0으로 채운 3x4 텐서\n",
    "ones = torch.ones(2, 3)            # 1로 채운 2x3 텐서\n",
    "rand = torch.rand(2, 2)            # 0~1 균등 분포\n",
    "randn = torch.randn(2, 2)          # 표준 정규 분포\n",
    "\n",
    "print(f\"zeros (3x4):\\n{zeros}\")\n",
    "print(f\"\\nones (2x3):\\n{ones}\")\n",
    "print(f\"\\nrand (0~1 균등):\\n{rand}\")\n",
    "print(f\"\\nrandn (정규 분포):\\n{randn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 범위/간격 텐서\n",
    "arange = torch.arange(0, 10, 2)    # 0부터 10 미만, 간격 2\n",
    "linspace = torch.linspace(0, 1, 5) # 0~1을 5등분\n",
    "\n",
    "print(f\"arange(0, 10, 2): {arange}\")\n",
    "print(f\"linspace(0, 1, 5): {linspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기존 텐서와 같은 shape/dtype으로 생성\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "\n",
    "zeros_like = torch.zeros_like(x)   # x와 같은 shape, 0으로 채움\n",
    "ones_like = torch.ones_like(x)     # x와 같은 shape, 1로 채움\n",
    "rand_like = torch.rand_like(x)     # x와 같은 shape, 랜덤\n",
    "\n",
    "print(f\"원본 x:\\n{x}\")\n",
    "print(f\"zeros_like:\\n{zeros_like}\")\n",
    "print(f\"ones_like:\\n{ones_like}\")\n",
    "print(f\"rand_like:\\n{rand_like}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 딥러닝 입력 데이터 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 시뮬레이션 (batch_size=32, features=10)\n",
    "batch_size = 32\n",
    "num_features = 10\n",
    "\n",
    "# 입력 데이터 (정규 분포)\n",
    "X_batch = torch.randn(batch_size, num_features)\n",
    "\n",
    "# 레이블 (0 또는 1)\n",
    "y_batch = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "print(f\"입력 데이터 shape: {X_batch.shape}\")\n",
    "print(f\"레이블 shape: {y_batch.shape}\")\n",
    "print(f\"\\n첫 5개 샘플의 처음 3개 특성:\\n{X_batch[:5, :3]}\")\n",
    "print(f\"\\n첫 10개 레이블: {y_batch[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 텐서 속성\n",
    "\n",
    "### shape, dtype, device\n",
    "\n",
    "텐서의 3가지 핵심 속성:\n",
    "- **shape**: 각 차원의 크기\n",
    "- **dtype**: 데이터 타입\n",
    "- **device**: CPU 또는 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성\n",
    "t = torch.randn(3, 4, 5)\n",
    "\n",
    "# 속성 확인\n",
    "print(f\"Shape: {t.shape}\")       # torch.Size([3, 4, 5])\n",
    "print(f\"Size: {t.size()}\")       # shape와 동일\n",
    "print(f\"Dtype: {t.dtype}\")       # torch.float32 (기본값)\n",
    "print(f\"Device: {t.device}\")     # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 지정\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "bool_tensor = torch.tensor([True, False, True], dtype=torch.bool)\n",
    "\n",
    "print(f\"int64: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "print(f\"float32: {float_tensor}, dtype: {float_tensor.dtype}\")\n",
    "print(f\"bool: {bool_tensor}, dtype: {bool_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 변환\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"원본: {x}, dtype: {x.dtype}\")\n",
    "\n",
    "# float으로 변환\n",
    "x_float = x.float()  # 또는 x.to(torch.float32)\n",
    "print(f\"float: {x_float}, dtype: {x_float.dtype}\")\n",
    "\n",
    "# double로 변환\n",
    "x_double = x.double()  # 또는 x.to(torch.float64)\n",
    "print(f\"double: {x_double}, dtype: {x_double.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 딥러닝에서 자주 사용되는 dtype\n",
    "\n",
    "| dtype | 용도 | 메모리 |\n",
    "|-------|-----|--------|\n",
    "| float32 | 기본 학습 | 4 bytes |\n",
    "| float16 | 혼합 정밀도 학습 | 2 bytes |\n",
    "| int64 | 인덱스, 레이블 | 8 bytes |\n",
    "| bool | 마스크 | 1 byte |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 GPU 가속 (CUDA)\n",
    "\n",
    "### to() 메서드로 device 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정 (GPU 있으면 cuda, 없으면 cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서를 device로 이동\n",
    "x = torch.randn(3, 3)\n",
    "print(f\"원본 device: {x.device}\")\n",
    "\n",
    "# GPU로 이동 (GPU 있는 경우)\n",
    "x_device = x.to(device)\n",
    "print(f\"이동 후 device: {x_device.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성 시 바로 device 지정\n",
    "y = torch.randn(3, 3, device=device)\n",
    "print(f\"직접 생성: {y.device}\")\n",
    "\n",
    "# CPU로 다시 이동 (결과 확인용)\n",
    "y_cpu = y.cpu()\n",
    "print(f\"CPU로 이동: {y_cpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의: device가 다른 텐서끼리 연산 불가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 불일치 오류 예시\n",
    "a = torch.randn(3, 3)  # CPU\n",
    "b = torch.randn(3, 3, device=device)  # GPU (있는 경우)\n",
    "\n",
    "if a.device != b.device:\n",
    "    print(f\"주의: a({a.device})와 b({b.device})는 device가 다릅니다!\")\n",
    "    print(\"연산 전에 같은 device로 이동해야 합니다.\")\n",
    "    \n",
    "    # 같은 device로 이동 후 연산\n",
    "    a_device = a.to(device)\n",
    "    result = a_device + b\n",
    "    print(f\"연산 성공! 결과 shape: {result.shape}\")\n",
    "else:\n",
    "    result = a + b\n",
    "    print(f\"연산 성공! 결과 shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 NumPy와 PyTorch 변환\n",
    "\n",
    "### 양방향 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NumPy -> PyTorch\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"NumPy 배열:\\n{np_array}\")\n",
    "print(f\"PyTorch 텐서:\\n{tensor_from_np}\")\n",
    "print(f\"dtype: {tensor_from_np.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch -> NumPy\n",
    "tensor = torch.randn(2, 3)\n",
    "np_from_tensor = tensor.numpy()\n",
    "\n",
    "print(f\"PyTorch 텐서:\\n{tensor}\")\n",
    "print(f\"NumPy 배열:\\n{np_from_tensor}\")\n",
    "print(f\"type: {type(np_from_tensor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의: 메모리 공유!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_numpy는 메모리를 공유합니다!\n",
    "np_arr = np.array([1, 2, 3], dtype=np.float32)\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "\n",
    "print(f\"변환 전 NumPy: {np_arr}\")\n",
    "print(f\"변환 전 Tensor: {tensor}\")\n",
    "\n",
    "# NumPy 배열 수정\n",
    "np_arr[0] = 100\n",
    "\n",
    "print(f\"\\n수정 후 NumPy: {np_arr}\")\n",
    "print(f\"수정 후 Tensor: {tensor}  # 같이 변경됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복사본 만들기 (메모리 공유 방지)\n",
    "np_arr = np.array([1, 2, 3], dtype=np.float32)\n",
    "tensor = torch.tensor(np_arr)  # tensor()는 복사본 생성\n",
    "\n",
    "np_arr[0] = 100\n",
    "\n",
    "print(f\"NumPy: {np_arr}\")\n",
    "print(f\"Tensor: {tensor}  # 독립적\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 텐서 연산\n",
    "\n",
    "### 기본 산술 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# 요소별 연산\n",
    "print(f\"a + b:\\n{a + b}\")\n",
    "print(f\"\\na - b:\\n{a - b}\")\n",
    "print(f\"\\na * b (요소별):\\n{a * b}\")\n",
    "print(f\"\\na / b:\\n{a / b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"txc-formula\" src=\"https://t1.daumcdn.net/cfile/tistory/25111F4D58333A5B07\" width=\"481\" height=\"202\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 곱 (Matrix Multiplication)\n",
    "# 방법 1: torch.matmul()\n",
    "matmul_result = torch.matmul(a, b)\n",
    "print(f\"matmul(a, b):\\n{matmul_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2: @ 연산자 (Python 3.5+)\n",
    "at_result = a @ b\n",
    "print(f\"\\na @ b:\\n{at_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 3: tensor.mm() (2D only)\n",
    "mm_result = a.mm(b)\n",
    "print(f\"\\na.mm(b):\\n{mm_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집계 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "print(f\"텐서 x:\\n{x}\")\n",
    "\n",
    "# 전체 집계\n",
    "print(f\"\\nsum: {x.sum()}\")\n",
    "print(f\"mean: {x.mean()}\")\n",
    "print(f\"max: {x.max()}\")\n",
    "print(f\"min: {x.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축(dim) 기준 집계\n",
    "print(f\"행 방향 합 (dim=0): {x.sum(dim=0)}\")\n",
    "print(f\"열 방향 합 (dim=1): {x.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax: 최댓값의 인덱스\n",
    "print(f\"\\n전체 argmax: {x.argmax()}\")\n",
    "print(f\"열 방향 argmax (dim=1): {x.argmax(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 형태 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)\n",
    "print(f\"원본: {x}, shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape: 형태 변경\n",
    "x_3x4 = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{x_3x4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view: reshape와 유사 (연속 메모리 필요)\n",
    "x_2x6 = x.view(2, 6)\n",
    "print(f\"\\nview(2, 6):\\n{x_2x6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1: 자동 계산\n",
    "x_auto = x.reshape(4, -1)  # 4 x ? -> 4 x 3\n",
    "print(f\"\\nreshape(4, -1):\\n{x_auto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 추가/제거\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"원본 shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsqueeze: 차원 추가\n",
    "x_unsq0 = x.unsqueeze(0)  # (3,) -> (1, 3)\n",
    "x_unsq1 = x.unsqueeze(1)  # (3,) -> (3, 1)\n",
    "print(f\"unsqueeze(0): {x_unsq0.shape}\")\n",
    "print(f\"unsqueeze(1): {x_unsq1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# squeeze: 크기 1인 차원 제거\n",
    "y = torch.zeros(1, 3, 1)\n",
    "y_sq = y.squeeze()\n",
    "print(f\"\\nsqueeze 전: {y.shape}\")\n",
    "print(f\"squeeze 후: {y_sq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.7 텐서 인덱싱과 슬라이싱\n",
    "\n",
    "### NumPy와 동일한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12]])\n",
    "print(f\"텐서 x:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 인덱싱\n",
    "print(f\"x[0]: {x[0]}\")\n",
    "print(f\"x[0, 0]: {x[0, 0]}\")\n",
    "print(f\"x[-1]: {x[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이싱\n",
    "print(f\"x[:2]: {x[:2]}\")\n",
    "print(f\"x[:, 1:3]:\\n{x[:, 1:3]}\")\n",
    "print(f\"x[1:, 2:]:\\n{x[1:, 2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건 인덱싱 (Boolean Indexing)\n",
    "mask = x > 5\n",
    "print(f\"마스크 (x > 5):\\n{mask}\")\n",
    "print(f\"\\n5보다 큰 값: {x[mask]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 팬시 인덱싱\n",
    "indices = torch.tensor([0, 2])\n",
    "print(f\"x[indices] (0, 2번 행):\\n{x[indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 배치에서 특정 샘플 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 (batch_size=8, features=5)\n",
    "batch = torch.randn(8, 5)\n",
    "labels = torch.tensor([0, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "print(f\"배치 shape: {batch.shape}\")\n",
    "print(f\"레이블: {labels}\")\n",
    "\n",
    "# 클래스 1인 샘플만 추출\n",
    "class1_samples = batch[labels == 1]\n",
    "print(f\"\\n클래스 1 샘플 shape: {class1_samples.shape}\")\n",
    "print(f\"클래스 1 샘플:\\n{class1_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Autograd (자동 미분)\n",
    "\n",
    "### 역전파의 핵심\n",
    "\n",
    "**Autograd**: 텐서 연산의 그래디언트(미분값)를 자동으로 계산\n",
    "\n",
    "```\n",
    "순전파 (Forward):  x -> f(x) -> y -> g(y) -> z\n",
    "역전파 (Backward): dz/dx <- dz/dy <- dz/dz=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad=True: 그래디언트 추적 활성화\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연산 수행 (계산 그래프 생성)\n",
    "y = x ** 2 + 3 * x  # y = x^2 + 3x\n",
    "print(f\"y = x^2 + 3x = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 스칼라로 변환 (backward는 스칼라에 대해 호출)\n",
    "z = y.sum()  # z = y[0] + y[1]\n",
    "print(f\"z = sum(y) = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward(): 그래디언트 계산\n",
    "z.backward()\n",
    "\n",
    "# x.grad: dz/dx\n",
    "# y = x^2 + 3x -> dy/dx = 2x + 3\n",
    "# x = [2, 3] -> dy/dx = [7, 9]\n",
    "print(f\"dz/dx = {x.grad}\")\n",
    "print(f\"수동 계산: 2*{x.data} + 3 = {2*x.data + 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 누적 주의!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래디언트는 누적됩니다!\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# 첫 번째 backward\n",
    "y1 = x * 2\n",
    "y1.backward()\n",
    "print(f\"첫 번째 backward 후 grad: {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 두 번째 backward (누적됨!)\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"두 번째 backward 후 grad: {x.grad}  # 2 + 3 = 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해결: 매 iteration마다 grad 초기화\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    # 그래디언트 초기화\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    y = x * (i + 1)\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}: grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 간단한 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표: y = 2x + 1의 기울기(w)와 절편(b) 찾기\n",
    "# 데이터 생성\n",
    "torch.manual_seed(42)\n",
    "X = torch.linspace(-1, 1, 20).reshape(-1, 1)  # (20, 1)\n",
    "y_true = 2 * X + 1 + 0.1 * torch.randn_like(X)  # y = 2x + 1 + noise\n",
    "\n",
    "# 학습할 파라미터 (초기값)\n",
    "w = torch.tensor([0.0], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"경사 하강법으로 w=2, b=1 찾기\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 순전파: 예측\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # 손실 계산: MSE\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "    \n",
    "    # 역전파: 그래디언트 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 파라미터 업데이트 (no_grad 안에서!)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 그래디언트 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")\n",
    "\n",
    "print(f\"\\n최종 결과: w={w.item():.4f} (목표: 2.0), b={b.item():.4f} (목표: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 nn.Module로 신경망 정의\n",
    "\n",
    "### nn.Module 기초\n",
    "\n",
    "PyTorch의 모든 신경망은 `nn.Module`을 상속받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 간단한 선형 모델\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 레이어 정의\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 순전파 정의\n",
    "        return self.linear(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = LinearModel(input_dim=10, output_dim=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 확인\n",
    "print(\"모델 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파 테스트\n",
    "x = torch.randn(5, 10)  # 배치 5개, 특성 10개\n",
    "output = model(x)       # forward() 자동 호출\n",
    "print(f\"입력 shape: {x.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"출력:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론 (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)    # 입력 -> 은닉\n",
    "        self.relu = nn.ReLU()                          # 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)   # 은닉 -> 출력\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)      # 선형 변환\n",
    "        x = self.relu(x)     # 활성화\n",
    "        x = self.fc2(x)      # 선형 변환\n",
    "        return x\n",
    "\n",
    "# MLP 생성\n",
    "mlp = MLP(input_dim=10, hidden_dim=32, output_dim=2)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential로 더 간단하게\n",
    "mlp_sequential = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "print(mlp_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "x = torch.randn(4, 10)\n",
    "output = mlp_sequential(x)\n",
    "print(f\"입력: {x.shape}\")\n",
    "print(f\"출력: {output.shape}\")\n",
    "print(f\"출력 값:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 Dataset과 DataLoader\n",
    "\n",
    "### Dataset 클래스\n",
    "\n",
    "데이터를 관리하는 추상 클래스. `__len__`과 `__getitem__` 구현 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 데이터 생성\n",
    "X_data = torch.randn(100, 5)  # 100개 샘플, 5개 특성\n",
    "y_data = torch.randint(0, 2, (100,))  # 이진 레이블\n",
    "\n",
    "# Dataset 생성\n",
    "dataset = SimpleDataset(X_data, y_data)\n",
    "print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "print(f\"첫 번째 샘플: X={dataset[0][0].shape}, y={dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader: 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,   # 배치 크기\n",
    "    shuffle=True,    # 셔플 여부\n",
    "    drop_last=True   # 마지막 불완전한 배치 버림\n",
    ")\n",
    "\n",
    "print(f\"배치 수: {len(dataloader)}\")\n",
    "\n",
    "# 배치 순회\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "    if batch_idx < 3:  # 처음 3개 배치만\n",
    "        print(f\"Batch {batch_idx}: X={X_batch.shape}, y={y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 학습 루프 (Training Loop)\n",
    "\n",
    "### 전체 흐름\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # 1. 순전파\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        # 2. 역전파\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 그래디언트 계산\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 예제: y = 2x + 1 학습\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 데이터 생성\n",
    "X_train = torch.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y_train = 2 * X_train + 1 + 0.1 * torch.randn_like(X_train)\n",
    "\n",
    "# Dataset, DataLoader\n",
    "train_dataset = SimpleDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model = nn.Linear(1, 1)  # 입력 1, 출력 1\n",
    "criterion = nn.MSELoss()  # 평균 제곱 오차\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"학습 전 파라미터:\")\n",
    "print(f\"  weight: {model.weight.data.item():.4f}\")\n",
    "print(f\"  bias: {model.bias.data.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # 1. 순전파\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        # 2. 역전파\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 그래디언트 계산\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n학습 후 파라미터:\")\n",
    "print(f\"  weight: {model.weight.data.item():.4f} (목표: 2.0)\")\n",
    "print(f\"  bias: {model.bias.data.item():.4f} (목표: 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화 (Plotly)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(epochs),\n",
    "    'Loss': losses\n",
    "})\n",
    "\n",
    "fig = px.line(loss_df, x='Epoch', y='Loss', title='학습 손실 곡선')\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='MSE Loss',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 이진 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터로 고가/저가 분류\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# 데이터 로드\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "\n",
    "median_price = np.median(housing.target)\n",
    "y = (housing.target > median_price).astype(int)  # 고가: 1, 저가: 0\n",
    "\n",
    "# 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"훈련 데이터: {X_train_t.shape}\")\n",
    "print(f\"테스트 데이터: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 모델\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # 확률 출력\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = BinaryClassifier(input_dim=8)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataset = SimpleDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 학습 모드\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "model.eval()  # 평가 모드\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    y_pred_proba = model(X_test_t)\n",
    "    y_pred = (y_pred_proba > 0.5).float()\n",
    "    \n",
    "    accuracy = (y_pred == y_test_t).float().mean()\n",
    "    print(f\"\\n테스트 정확도: {accuracy.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 텐서 생성하기 (기본)\n",
    "\n",
    "**문제**: 다음 조건에 맞는 텐서를 생성하세요.\n",
    "\n",
    "1. 3x4 크기의 모든 요소가 1인 텐서\n",
    "2. 0부터 9까지의 정수 텐서\n",
    "3. 2x3 크기의 표준 정규 분포 랜덤 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. 텐서 속성 확인 (기본)\n",
    "\n",
    "**문제**: 아래 텐서의 shape, dtype, device를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 텐서 연산 (기본)\n",
    "\n",
    "**문제**: 두 행렬 A와 B의 행렬 곱을 계산하세요.\n",
    "\n",
    "```python\n",
    "A = [[1, 2], [3, 4]]\n",
    "B = [[5, 6], [7, 8]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. NumPy 변환 (응용)\n",
    "\n",
    "**문제**: NumPy 배열을 PyTorch 텐서로 변환하고, 다시 NumPy로 변환하세요. 메모리 공유 여부를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. GPU 이동 (응용)\n",
    "\n",
    "**문제**: 텐서를 GPU로 이동하고 (GPU가 없으면 CPU 유지), device를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. 텐서 인덱싱 (응용)\n",
    "\n",
    "**문제**: 아래 텐서에서 5보다 큰 값만 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 8, 3], [4, 2, 9], [7, 6, 5]])\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Autograd 이해 (복합)\n",
    "\n",
    "**문제**: y = x^3 + 2x^2 + 1에서 x=2일 때 dy/dx를 Autograd로 계산하세요.\n",
    "\n",
    "힌트: dy/dx = 3x^2 + 4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. nn.Module 정의 (복합)\n",
    "\n",
    "**문제**: 3개의 은닉층을 가진 MLP를 정의하세요.\n",
    "\n",
    "- 입력: 10\n",
    "- 은닉층: 64 -> 32 -> 16\n",
    "- 출력: 2\n",
    "- 활성화: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Dataset과 DataLoader (종합)\n",
    "\n",
    "**문제**: 1000개의 랜덤 샘플(특성 5개)과 레이블로 Dataset을 만들고, batch_size=32인 DataLoader를 생성하세요. 첫 번째 배치를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 학습 루프 작성 (종합)\n",
    "\n",
    "**문제**: 아래 데이터로 선형 회귀 모델을 학습하세요.\n",
    "\n",
    "요구사항:\n",
    "1. y = 3x - 2의 관계를 학습\n",
    "2. nn.Linear 모델 사용\n",
    "3. MSE 손실, SGD 옵티마이저\n",
    "4. 100 에포크 학습 후 weight와 bias 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 데이터 생성\n",
    "torch.manual_seed(42)\n",
    "X = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "y = 3 * X - 2 + 0.1 * torch.randn_like(X)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 함수 | 실무 활용 |\n",
    "|-----|----------|----------|\n",
    "| 텐서 생성 | torch.tensor(), zeros(), ones(), rand() | 데이터 초기화 |\n",
    "| 속성 | shape, dtype, device | 디버깅, 호환성 확인 |\n",
    "| NumPy 변환 | from_numpy(), .numpy() | 데이터 전처리 연동 |\n",
    "| GPU 이동 | .to(device), .cuda(), .cpu() | 학습 가속 |\n",
    "| 인덱싱 | 슬라이싱, Boolean indexing | 배치 추출, 필터링 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 메서드 | 언제 사용? |\n",
    "|-----|-----------|----------|\n",
    "| Autograd | requires_grad=True, backward() | 그래디언트 자동 계산 |\n",
    "| nn.Module | __init__(), forward() | 신경망 정의 |\n",
    "| Dataset | __len__(), __getitem__() | 데이터 관리 |\n",
    "| DataLoader | batch_size, shuffle | 배치 처리 |\n",
    "\n",
    "### 학습 루프 핵심 패턴\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        output = model(X_batch)           # 순전파\n",
    "        loss = criterion(output, y_batch) # 손실 계산\n",
    "        \n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 역전파\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **device 일관성**: 모든 텐서와 모델이 같은 device에 있는지 확인\n",
    "2. **grad 초기화**: 매 iteration마다 `optimizer.zero_grad()` 필수\n",
    "3. **eval() 모드**: 평가 시 `model.eval()` + `torch.no_grad()`\n",
    "4. **dtype 주의**: 연산 전 dtype 일치 확인\n",
    "5. **메모리 관리**: 불필요한 텐서는 `del`로 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
