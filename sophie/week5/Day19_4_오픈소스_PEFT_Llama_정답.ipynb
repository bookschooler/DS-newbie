{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day19_4: 오픈소스 LLM 파인튜닝 (PEFT & QLoRA) - 정답\n",
    "\n",
    "## 실습 퀴즈 정답\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q1. PEFT 개념 이해 (기본)\n",
    "\n",
    "**문제**: 다음 빈칸을 채우세요.\n",
    "\n",
    "1. PEFT는 (______)의 약자로, 전체 모델이 아닌 아주 작은 부분만 학습합니다.\n",
    "2. LoRA는 (______)을 통해 원본 가중치 행렬을 근사합니다.\n",
    "3. QLoRA는 (______)비트 양자화와 LoRA를 결합한 기술입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "answer1 = \"Parameter-Efficient Fine-Tuning\"\n",
    "answer2 = \"저차원 행렬 분해 (Low-Rank Decomposition)\"\n",
    "answer3 = \"4\"\n",
    "\n",
    "print(f\"1. PEFT = {answer1}\")\n",
    "print(f\"   (매개변수 효율적 파인튜닝)\")\n",
    "print()\n",
    "print(f\"2. LoRA는 {answer2}을 통해 원본 가중치를 근사합니다.\")\n",
    "print(f\"   W' = W + BA (B: d x r, A: r x k, r << d,k)\")\n",
    "print()\n",
    "print(f\"3. QLoRA는 {answer3}비트 양자화 + LoRA를 결합합니다.\")\n",
    "print(f\"   4비트 NormalFloat(NF4)를 사용하여 메모리를 1/4로 절약\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- **PEFT**: Parameter-Efficient Fine-Tuning의 약자로, 모델의 일부 파라미터만 학습하여 효율성을 높이는 기법입니다.\n",
    "- **LoRA**: Low-Rank Adaptation의 약자로, 큰 행렬을 두 개의 작은 행렬의 곱으로 분해하여 학습 파라미터를 줄입니다.\n",
    "- **QLoRA**: Quantized LoRA로, 4비트 양자화를 통해 모델 가중치를 압축하고 LoRA를 적용합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. LoRA 파라미터 비율 계산 (기본)\n",
    "\n",
    "**문제**: 다음 조건에서 LoRA 학습 파라미터 비율을 계산하세요.\n",
    "\n",
    "- 원래 가중치 행렬: 4096 x 4096\n",
    "- LoRA rank (r): 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "d = 4096  # 입력 차원\n",
    "k = 4096  # 출력 차원\n",
    "r = 8     # LoRA rank\n",
    "\n",
    "# 원래 파라미터 수\n",
    "original_params = d * k\n",
    "\n",
    "# LoRA 파라미터 수\n",
    "# B: (d x r) + A: (r x k)\n",
    "lora_params = (d * r) + (r * k)\n",
    "\n",
    "# 비율 계산\n",
    "ratio = (lora_params / original_params) * 100\n",
    "\n",
    "print(\"LoRA 파라미터 비율 계산\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"원래 가중치 행렬: {d} x {k}\")\n",
    "print(f\"LoRA rank (r): {r}\")\n",
    "print()\n",
    "print(f\"원래 파라미터 수: {original_params:,}\")\n",
    "print(f\"LoRA 파라미터 수: {lora_params:,}\")\n",
    "print(f\"  - B 행렬 (d x r): {d} x {r} = {d * r:,}\")\n",
    "print(f\"  - A 행렬 (r x k): {r} x {k} = {r * k:,}\")\n",
    "print()\n",
    "print(f\"학습 파라미터 비율: {ratio:.4f}%\")\n",
    "print(f\"메모리 절약: {100 - ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- LoRA는 원본 가중치 W (d x k)를 BA로 근사합니다.\n",
    "- B: (d x r), A: (r x k)이므로 총 파라미터는 r(d + k)입니다.\n",
    "- r=8, d=k=4096일 때: 8 x (4096 + 4096) = 65,536개\n",
    "- 원본 대비: 65,536 / 16,777,216 = 약 0.39%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 양자화 메모리 계산 (기본)\n",
    "\n",
    "**문제**: 7B 파라미터 모델의 메모리 사용량을 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "params = 7_000_000_000  # 7B 파라미터\n",
    "\n",
    "# 각 정밀도별 바이트\n",
    "precision = {\n",
    "    \"FP32 (32비트)\": 4,    # 4 bytes\n",
    "    \"FP16 (16비트)\": 2,    # 2 bytes\n",
    "    \"INT8 (8비트)\": 1,     # 1 byte\n",
    "    \"INT4 (4비트)\": 0.5    # 0.5 bytes\n",
    "}\n",
    "\n",
    "print(\"7B 파라미터 모델 메모리 사용량 (가중치만)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, bytes_per_param in precision.items():\n",
    "    memory_bytes = params * bytes_per_param\n",
    "    memory_gb = memory_bytes / (1024 ** 3)  # bytes to GB\n",
    "    print(f\"{name}: {memory_gb:.2f} GB\")\n",
    "\n",
    "print()\n",
    "print(\"결론:\")\n",
    "print(\"- FP32 -> INT4: 메모리 1/8로 감소 (28GB -> 3.5GB)\")\n",
    "print(\"- QLoRA 덕분에 일반 GPU(8GB VRAM)에서도 7B 모델 파인튜닝 가능!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- 메모리 = 파라미터 수 x 파라미터당 바이트\n",
    "- FP32: 4바이트, FP16: 2바이트, INT8: 1바이트, INT4: 0.5바이트\n",
    "- 양자화를 통해 메모리를 크게 절약할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. 토크나이저 사용 (응용)\n",
    "\n",
    "**문제**: 아래 텍스트를 Llama 3 Instruct 템플릿으로 변환하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "def create_llama3_prompt(system_message, user_message):\n",
    "    \"\"\"\n",
    "    Llama 3 Instruct 템플릿으로 프롬프트 생성\n",
    "    \n",
    "    Parameters:\n",
    "    - system_message: 시스템 메시지\n",
    "    - user_message: 사용자 메시지\n",
    "    \n",
    "    Returns:\n",
    "    - 포맷팅된 프롬프트\n",
    "    \"\"\"\n",
    "    if system_message:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 테스트\n",
    "system_msg = \"당신은 데이터 분석 전문가입니다.\"\n",
    "user_msg = \"pandas와 numpy의 차이점은?\"\n",
    "\n",
    "prompt = create_llama3_prompt(system_msg, user_msg)\n",
    "\n",
    "print(\"Llama 3 Instruct 템플릿:\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- Llama 3 Instruct 모델은 특수 토큰을 사용한 템플릿을 따릅니다.\n",
    "- `<|begin_of_text|>`: 텍스트 시작\n",
    "- `<|start_header_id|>`: 역할 헤더 시작\n",
    "- `<|end_header_id|>`: 역할 헤더 끝\n",
    "- `<|eot_id|>`: 턴 종료\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. LoRA 설정 변경 (응용)\n",
    "\n",
    "**문제**: 다음 요구사항에 맞는 LoraConfig를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# 정답\n",
    "lora_config = LoraConfig(\n",
    "    r=32,                           # rank\n",
    "    lora_alpha=64,                  # alpha (r의 2배)\n",
    "    lora_dropout=0.1,               # dropout\n",
    "    bias=\"none\",                    # bias 학습 안함\n",
    "    task_type=\"CAUSAL_LM\",          # 언어 모델 태스크\n",
    "    target_modules=[                # 모든 Linear 레이어\n",
    "        \"q_proj\",     # Query projection\n",
    "        \"k_proj\",     # Key projection\n",
    "        \"v_proj\",     # Value projection\n",
    "        \"o_proj\",     # Output projection\n",
    "        \"gate_proj\",  # MLP gate\n",
    "        \"up_proj\",    # MLP up\n",
    "        \"down_proj\"   # MLP down\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"LoRA 설정:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"r (rank): {lora_config.r}\")\n",
    "print(f\"lora_alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"lora_dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"target_modules: {lora_config.target_modules}\")\n",
    "print()\n",
    "print(\"적용 레이어:\")\n",
    "print(\"  - Attention: q_proj, k_proj, v_proj, o_proj\")\n",
    "print(\"  - MLP/FFN: gate_proj, up_proj, down_proj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- `r=32`: 더 높은 rank로 표현력 증가 (메모리 사용량도 증가)\n",
    "- `lora_alpha=64`: 일반적으로 r의 2배로 설정\n",
    "- `target_modules`: Llama 모델의 모든 Linear 레이어에 LoRA 적용\n",
    "  - Attention: Query, Key, Value, Output projection\n",
    "  - MLP: Gate, Up, Down projection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. 학습 인자 최적화 (응용)\n",
    "\n",
    "**문제**: 메모리 제한(8GB VRAM)에서 안정적인 학습을 위한 TrainingArguments를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 정답\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    \n",
    "    # 배치 설정 (메모리 최적화)\n",
    "    per_device_train_batch_size=1,     # 최소 배치 (메모리 절약)\n",
    "    gradient_accumulation_steps=8,     # 효과적 배치 크기 = 1 x 8 = 8\n",
    "    \n",
    "    # 학습률 설정\n",
    "    learning_rate=2e-4,                # QLoRA는 높은 학습률 가능\n",
    "    warmup_ratio=0.1,                  # 10% warmup\n",
    "    \n",
    "    # 에포크 및 스텝\n",
    "    num_train_epochs=2,                # 2 에포크\n",
    "    max_steps=-1,                      # 에포크 기반 학습\n",
    "    \n",
    "    # 메모리 최적화\n",
    "    fp16=True,                         # FP16 사용\n",
    "    gradient_checkpointing=True,       # 그래디언트 체크포인팅\n",
    "    \n",
    "    # 로깅\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "print(\"메모리 최적화 학습 설정:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"그래디언트 누적: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"효과적 배치 크기: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"학습률: {training_args.learning_rate}\")\n",
    "print(f\"에포크: {training_args.num_train_epochs}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "print(f\"Gradient Checkpointing: {training_args.gradient_checkpointing}\")\n",
    "print()\n",
    "print(\"메모리 절약 전략:\")\n",
    "print(\"  1. 배치 크기 1 + 그래디언트 누적 8\")\n",
    "print(\"  2. FP16 학습\")\n",
    "print(\"  3. Gradient Checkpointing (활성화 재계산)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- **배치 크기 1**: 메모리 사용량 최소화\n",
    "- **gradient_accumulation_steps=8**: 작은 배치를 누적하여 효과적 배치 크기 8 달성\n",
    "- **gradient_checkpointing**: 활성화 값을 저장하지 않고 필요시 재계산하여 메모리 절약\n",
    "- **fp16**: 16비트 부동소수점으로 메모리와 속도 최적화\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. 데이터셋 포맷팅 함수 (복합)\n",
    "\n",
    "**문제**: 다양한 형식의 데이터를 Llama 3 템플릿으로 변환하는 함수를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "def format_to_llama3(example):\n",
    "    \"\"\"\n",
    "    다양한 형식의 데이터를 Llama 3 템플릿으로 변환\n",
    "    \n",
    "    지원 형식:\n",
    "    1. {\"question\": ..., \"answer\": ...} - Q&A 형식\n",
    "    2. {\"instruction\": ..., \"input\": ..., \"output\": ...} - Alpaca 형식\n",
    "    3. {\"prompt\": ..., \"response\": ...} - 일반 형식\n",
    "    \n",
    "    Returns:\n",
    "    - Llama 3 Instruct 형식의 텍스트\n",
    "    \"\"\"\n",
    "    # 형식 1: Q&A\n",
    "    if \"question\" in example and \"answer\" in example:\n",
    "        user_content = example[\"question\"]\n",
    "        assistant_content = example[\"answer\"]\n",
    "    \n",
    "    # 형식 2: Alpaca (instruction + input + output)\n",
    "    elif \"instruction\" in example and \"output\" in example:\n",
    "        if example.get(\"input\"):\n",
    "            user_content = f\"{example['instruction']}\\n\\n입력: {example['input']}\"\n",
    "        else:\n",
    "            user_content = example[\"instruction\"]\n",
    "        assistant_content = example[\"output\"]\n",
    "    \n",
    "    # 형식 3: 일반 (prompt + response)\n",
    "    elif \"prompt\" in example and \"response\" in example:\n",
    "        user_content = example[\"prompt\"]\n",
    "        assistant_content = example[\"response\"]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 형식: {example.keys()}\")\n",
    "    \n",
    "    # Llama 3 템플릿으로 변환\n",
    "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{assistant_content}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# 테스트\n",
    "test_examples = [\n",
    "    {\"question\": \"머신러닝이란?\", \"answer\": \"데이터로부터 학습하는 AI입니다.\"},\n",
    "    {\"instruction\": \"번역하세요\", \"input\": \"Hello\", \"output\": \"안녕하세요\"},\n",
    "    {\"prompt\": \"파이썬의 장점\", \"response\": \"읽기 쉽고 배우기 쉽습니다.\"}\n",
    "]\n",
    "\n",
    "print(\"다양한 형식 -> Llama 3 템플릿 변환:\")\n",
    "print(\"=\" * 60)\n",
    "for i, ex in enumerate(test_examples, 1):\n",
    "    print(f\"\\n예시 {i} - 입력 형식: {list(ex.keys())}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(format_to_llama3(ex))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- 입력 딕셔너리의 키를 확인하여 형식을 판별합니다.\n",
    "- Alpaca 형식은 instruction과 input을 조합하여 user 메시지로 만듭니다.\n",
    "- 모든 형식을 동일한 Llama 3 템플릿으로 변환합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. 추론 함수 개선 (복합)\n",
    "\n",
    "**문제**: 생성 파라미터를 조절할 수 있는 개선된 추론 함수를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "def generate_with_params(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    question,\n",
    "    system_prompt=None,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    max_new_tokens=200\n",
    "):\n",
    "    \"\"\"\n",
    "    파라미터 조절 가능한 추론 함수\n",
    "    \n",
    "    Parameters:\n",
    "    - model: 모델\n",
    "    - tokenizer: 토크나이저\n",
    "    - question: 사용자 질문\n",
    "    - system_prompt: 시스템 프롬프트 (선택)\n",
    "    - temperature: 창의성 조절 (0~2)\n",
    "    - top_p: nucleus sampling\n",
    "    - top_k: top-k sampling\n",
    "    - max_new_tokens: 최대 생성 토큰\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {'answer': str, 'input_tokens': int, 'output_tokens': int}\n",
    "    \"\"\"\n",
    "    # 프롬프트 구성\n",
    "    if system_prompt:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # 토큰화\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # 생성\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_length = outputs.shape[1]\n",
    "    new_tokens = output_length - input_length\n",
    "    \n",
    "    # 디코딩\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 답변 부분만 추출\n",
    "    if \"assistant\" in full_response.lower():\n",
    "        parts = full_response.split(\"assistant\")\n",
    "        answer = parts[-1].strip() if len(parts) > 1 else full_response\n",
    "    else:\n",
    "        answer = full_response\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"input_tokens\": input_length,\n",
    "        \"output_tokens\": new_tokens,\n",
    "        \"total_tokens\": output_length\n",
    "    }\n",
    "\n",
    "# 함수 설명\n",
    "print(\"generate_with_params 함수 사용법:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "result = generate_with_params(\n",
    "    model, tokenizer, \n",
    "    \"파이썬의 장점은?\",\n",
    "    system_prompt=\"간결하게 답변하세요.\",\n",
    "    temperature=0.3,  # 낮으면 결정적\n",
    "    top_p=0.9,        # nucleus sampling\n",
    "    top_k=50          # top-k sampling\n",
    ")\n",
    "\n",
    "print(result['answer'])\n",
    "print(f\"토큰: {result['input_tokens']} -> {result['output_tokens']}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- `temperature`: 0에 가까울수록 결정적, 높을수록 다양한 출력\n",
    "- `top_p`: 누적 확률이 p 이하인 토큰만 샘플링\n",
    "- `top_k`: 상위 k개 토큰만 샘플링 대상으로\n",
    "- 토큰 수를 반환하여 비용/성능 분석에 활용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. 전체 파인튜닝 파이프라인 (종합)\n",
    "\n",
    "**문제**: CSV 파일에서 데이터를 로드하고 파인튜닝하는 전체 파이프라인을 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "\n",
    "def finetune_from_csv(csv_path, model_id, output_dir, \n",
    "                       lora_r=16, lora_alpha=32, epochs=3):\n",
    "    \"\"\"\n",
    "    CSV 데이터로 LLM 파인튜닝 파이프라인\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Q&A CSV 파일 경로 (question, answer 컬럼 필요)\n",
    "    - model_id: Hugging Face 모델 ID\n",
    "    - output_dir: 결과 저장 경로\n",
    "    - lora_r: LoRA rank\n",
    "    - lora_alpha: LoRA alpha\n",
    "    - epochs: 학습 에포크\n",
    "    \n",
    "    Returns:\n",
    "    - trainer: 구성된 SFTTrainer\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LLM 파인튜닝 파이프라인\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"\\n[1/6] 데이터 로드...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"  - 로드된 샘플 수: {len(df)}\")\n",
    "    \n",
    "    # 2. 데이터셋 변환\n",
    "    print(\"\\n[2/6] 데이터셋 변환...\")\n",
    "    dataset = Dataset.from_pandas(df[[\"question\", \"answer\"]])\n",
    "    \n",
    "    # 3. 템플릿 포맷팅\n",
    "    print(\"\\n[3/6] 템플릿 포맷팅...\")\n",
    "    def format_prompt(example):\n",
    "        return {\n",
    "            \"text\": f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{example['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{example['answer']}<|eot_id|>\"\"\"\n",
    "        }\n",
    "    \n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    print(f\"  - 포맷팅 완료\")\n",
    "    \n",
    "    # 4. 모델 및 토크나이저 로드\n",
    "    print(f\"\\n[4/6] 모델 로드: {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # GPU/CPU에 따라 양자화 설정\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"  - GPU 환경: 4비트 양자화 적용\")\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"  - CPU 환경: 양자화 없이 로드\")\n",
    "    \n",
    "    # 5. LoRA 설정\n",
    "    print(f\"\\n[5/6] LoRA 설정 (r={lora_r}, alpha={lora_alpha})\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )\n",
    "    \n",
    "    # 6. Trainer 구성\n",
    "    print(f\"\\n[6/6] SFTTrainer 구성...\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        fp16=(device == \"cuda\"),\n",
    "        warmup_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_dataset,\n",
    "        peft_config=lora_config,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"파이프라인 구성 완료!\")\n",
    "    print(\"학습 시작: trainer.train()\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# 사용 예시 (실제 CSV 파일 필요)\n",
    "print(\"사용 예시:\")\n",
    "print(\"\"\"\n",
    "# CSV 파일 준비 (question, answer 컬럼)\n",
    "# qa_data.csv:\n",
    "# question,answer\n",
    "# \"EDA란?\",\"탐색적 데이터 분석입니다.\"\n",
    "# ...\n",
    "\n",
    "trainer = finetune_from_csv(\n",
    "    csv_path=\"data/qa_dataset.csv\",\n",
    "    model_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    output_dir=\"./output\",\n",
    "    lora_r=16,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "# 어댑터 저장\n",
    "trainer.save_model(\"./my-adapter\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- 6단계로 구성된 파인튜닝 파이프라인입니다.\n",
    "- CSV 로드 -> 데이터셋 변환 -> 템플릿 포맷팅 -> 모델 로드 -> LoRA 설정 -> Trainer 구성\n",
    "- GPU/CPU 환경을 자동 감지하여 양자화를 적용합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 모델 비교 평가 (종합)\n",
    "\n",
    "**문제**: 파인튜닝 전후 모델의 응답을 비교 평가하는 함수를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답\n",
    "import pandas as pd\n",
    "\n",
    "def compare_models(base_model, finetuned_model, tokenizer, test_questions, keywords=None):\n",
    "    \"\"\"\n",
    "    파인튜닝 전후 모델 비교\n",
    "    \n",
    "    Parameters:\n",
    "    - base_model: 파인튜닝 전 모델\n",
    "    - finetuned_model: 파인튜닝 후 모델  \n",
    "    - tokenizer: 토크나이저\n",
    "    - test_questions: 테스트 질문 리스트\n",
    "    - keywords: 확인할 키워드 딕셔너리 {질문: [키워드들]} (선택)\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: 비교 결과\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        # 프롬프트 구성\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # 기본 모델 응답\n",
    "        base_model.eval()\n",
    "        with torch.no_grad():\n",
    "            base_outputs = base_model.generate(\n",
    "                inputs[\"input_ids\"].to(base_model.device),\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 파인튜닝 모델 응답\n",
    "        finetuned_model.eval()\n",
    "        with torch.no_grad():\n",
    "            ft_outputs = finetuned_model.generate(\n",
    "                inputs[\"input_ids\"].to(finetuned_model.device),\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        ft_response = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 답변 부분 추출\n",
    "        def extract_answer(response):\n",
    "            if \"assistant\" in response.lower():\n",
    "                return response.split(\"assistant\")[-1].strip()\n",
    "            return response\n",
    "        \n",
    "        base_answer = extract_answer(base_response)\n",
    "        ft_answer = extract_answer(ft_response)\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        result = {\n",
    "            \"question\": question[:50] + \"...\" if len(question) > 50 else question,\n",
    "            \"base_length\": len(base_answer),\n",
    "            \"ft_length\": len(ft_answer),\n",
    "            \"base_answer\": base_answer[:100] + \"...\" if len(base_answer) > 100 else base_answer,\n",
    "            \"ft_answer\": ft_answer[:100] + \"...\" if len(ft_answer) > 100 else ft_answer\n",
    "        }\n",
    "        \n",
    "        # 키워드 확인\n",
    "        if keywords and question in keywords:\n",
    "            kw_list = keywords[question]\n",
    "            base_kw_count = sum(1 for kw in kw_list if kw.lower() in base_answer.lower())\n",
    "            ft_kw_count = sum(1 for kw in kw_list if kw.lower() in ft_answer.lower())\n",
    "            result[\"base_keyword_score\"] = f\"{base_kw_count}/{len(kw_list)}\"\n",
    "            result[\"ft_keyword_score\"] = f\"{ft_kw_count}/{len(kw_list)}\"\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 함수 설명\n",
    "print(\"compare_models 함수 사용법:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "test_questions = [\n",
    "    \"EDA의 목적은 무엇인가요?\",\n",
    "    \"과적합을 방지하는 방법은?\"\n",
    "]\n",
    "\n",
    "# 키워드 정의 (선택)\n",
    "keywords = {\n",
    "    \"EDA의 목적은 무엇인가요?\": [\"탐색\", \"패턴\", \"시각화\"],\n",
    "    \"과적합을 방지하는 방법은?\": [\"정규화\", \"드롭아웃\", \"교차검증\"]\n",
    "}\n",
    "\n",
    "result_df = compare_models(\n",
    "    base_model=model,\n",
    "    finetuned_model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_questions=test_questions,\n",
    "    keywords=keywords\n",
    ")\n",
    "\n",
    "print(result_df)\n",
    "\"\"\")\n",
    "\n",
    "# 예시 출력 형식\n",
    "print(\"\\n예시 출력 DataFrame:\")\n",
    "sample_df = pd.DataFrame({\n",
    "    \"question\": [\"EDA의 목적은?\", \"과적합 방지법?\"],\n",
    "    \"base_length\": [85, 120],\n",
    "    \"ft_length\": [156, 189],\n",
    "    \"base_keyword_score\": [\"1/3\", \"1/3\"],\n",
    "    \"ft_keyword_score\": [\"3/3\", \"3/3\"]\n",
    "})\n",
    "print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀이 설명**:\n",
    "- 동일한 질문에 대해 파인튜닝 전후 모델의 응답을 비교합니다.\n",
    "- **응답 길이**: 파인튜닝 후 더 상세한 응답을 생성하는지 확인\n",
    "- **키워드 점수**: 중요 키워드가 응답에 포함되는지 확인\n",
    "- DataFrame으로 결과를 정리하여 비교 분석이 용이하게 합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "\n",
    "### 핵심 개념 정리\n",
    "\n",
    "| 개념 | 핵심 내용 | 핵심 공식/설정 |\n",
    "|------|----------|---------------|\n",
    "| PEFT | 일부 파라미터만 학습 | 전체의 0.1~1% |\n",
    "| LoRA | 저차원 행렬 분해 | W' = W + BA |\n",
    "| QLoRA | 4비트 양자화 + LoRA | 메모리 1/8 절약 |\n",
    "| BitsAndBytes | 양자화 라이브러리 | NF4, Double Quant |\n",
    "| SFTTrainer | LLM 파인튜닝 트레이너 | trl 라이브러리 |\n",
    "\n",
    "### LoRA 파라미터 가이드\n",
    "\n",
    "```python\n",
    "LoraConfig(\n",
    "    r=16,           # 8~64 (높을수록 표현력 증가)\n",
    "    lora_alpha=32,  # r x 2 권장\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Attention\n",
    ")\n",
    "```\n",
    "\n",
    "### 메모리 최적화 전략\n",
    "\n",
    "```python\n",
    "TrainingArguments(\n",
    "    per_device_train_batch_size=1,      # 최소 배치\n",
    "    gradient_accumulation_steps=8,      # 누적으로 효과적 배치 증가\n",
    "    gradient_checkpointing=True,        # 활성화 재계산\n",
    "    fp16=True                           # 16비트 학습\n",
    ")\n",
    "```\n",
    "\n",
    "### 실무 체크리스트\n",
    "\n",
    "- [ ] 모델 라이선스 확인 (상업적 사용 가능 여부)\n",
    "- [ ] 데이터 포맷을 모델 템플릿에 맞게 변환\n",
    "- [ ] GPU 메모리에 맞는 양자화/배치 설정\n",
    "- [ ] 학습 데이터에 없는 질문으로 평가\n",
    "- [ ] 어댑터 파일 저장 및 버전 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
