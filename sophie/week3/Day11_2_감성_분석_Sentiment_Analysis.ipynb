{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day11_2: ê°ì„± ë¶„ì„ (Sentiment Analysis)\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "**Part 1: ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„**\n",
    "1. ê°ì„± ë¶„ì„ì˜ ê°œë…ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ í™œìš© ì´í•´í•˜ê¸°\n",
    "2. VADER ê°ì„± ë¶„ì„ê¸°ë¡œ ì˜ì–´ í…ìŠ¤íŠ¸ ë¶„ì„í•˜ê¸°\n",
    "3. KNU í•œêµ­ì–´ ê°ì„± ì‚¬ì „ í™œìš©ë²• ë°°ìš°ê¸°\n",
    "4. ì»¤ìŠ¤í…€ ê°ì„± ì‚¬ì „ êµ¬ì¶•í•˜ê¸°\n",
    "5. ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¥˜ êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "**Part 2: ê°ì„± ë¶„ì„ ì‹œê°í™”ì™€ ì‘ìš©**\n",
    "1. pandasë¡œ ê°ì„± ì ìˆ˜ ì§‘ê³„ ë° ë¶„ì„í•˜ê¸°\n",
    "2. Plotlyë¡œ ê°ì„± ë¶„í¬ ì‹œê°í™”í•˜ê¸° (íŒŒì´/ë§‰ëŒ€ ì°¨íŠ¸)\n",
    "3. ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ ë¶„ì„í•˜ê¸°\n",
    "4. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„ ì†Œê°œ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ì™œ ì´ê²ƒì„ ë°°ìš°ë‚˜ìš”?\n",
    "\n",
    "| ê°œë… | ì‹¤ë¬´ í™œìš© | ì˜ˆì‹œ |\n",
    "|------|----------|------|\n",
    "| ê°ì„± ë¶„ì„ | ê³ ê° ë°˜ì‘ íŒŒì•… | \"ì´ ì œí’ˆì— ëŒ€í•œ ê³ ê° ì˜ê²¬ì´ ê¸ì •ì ì¸ê°€?\" |\n",
    "| ì‚¬ì „ ê¸°ë°˜ ë°©ë²• | ë¹ ë¥¸ ì´ˆê¸° ë¶„ì„ | ë¼ë²¨ë§ ì—†ì´ ì¦‰ì‹œ ê°ì„± ì ìˆ˜ ì‚°ì¶œ |\n",
    "| ê°ì„± ì‹œê°í™” | ì¸ì‚¬ì´íŠ¸ ì „ë‹¬ | ë¸Œëœë“œë³„ ê°ì„± ë¹„êµ ëŒ€ì‹œë³´ë“œ |\n",
    "| íŠ¸ë Œë“œ ë¶„ì„ | ìœ„ê¸° ê°ì§€ | SNS ë¶€ì • ì—¬ë¡  ê¸‰ì¦ ëª¨ë‹ˆí„°ë§ |\n",
    "\n",
    "**ë¶„ì„ê°€ ê´€ì **: ê°ì„± ë¶„ì„ì€ ìˆ˜ì²œ ê°œì˜ ë¦¬ë·°ë¥¼ í•œëˆˆì— íŒŒì•…í•˜ê²Œ í•´ì£¼ëŠ” ë§ˆë²• ê°™ì€ ë„êµ¬ì…ë‹ˆë‹¤!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 ê°ì„± ë¶„ì„ì´ë€?\n",
    "\n",
    "### ì •ì˜\n",
    "\n",
    "**ê°ì„± ë¶„ì„(Sentiment Analysis)**ì€ í…ìŠ¤íŠ¸ì—ì„œ **ì˜ê²¬, ê°ì •, íƒœë„**ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ê°ì„± ë¶„ì„ì˜ ìœ í˜•\n",
    "\n",
    "| ìœ í˜• | ì„¤ëª… | ì˜ˆì‹œ |\n",
    "|------|------|------|\n",
    "| ê·¹ì„± ë¶„ë¥˜ | ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¥˜ | \"ì¢‹ì•„ìš”\" â†’ ê¸ì • |\n",
    "| ê°ì • ë¶„ë¥˜ | ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸ ë“± | \"í™”ê°€ ë‚œë‹¤\" â†’ ë¶„ë…¸ |\n",
    "| ê°•ë„ ì¸¡ì • | ê°ì„±ì˜ ê°•ë„ ì ìˆ˜í™” | \"ë§¤ìš° ì¢‹ì•„\" > \"ì¢‹ì•„\" |\n",
    "| ì¸¡ë©´ ê¸°ë°˜ | íŠ¹ì • ì†ì„±ë³„ ê°ì„± | \"ìŒì‹ì€ ì¢‹ì§€ë§Œ ì„œë¹„ìŠ¤ê°€ ë³„ë¡œ\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# VADER ê°ì„± ë¶„ì„ê¸° (ì˜ì–´ìš©)\n",
    "# pip install vaderSentiment\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    vader_available = True\n",
    "    print(\"VADER ê°ì„± ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ\")\n",
    "except ImportError:\n",
    "    vader_available = False\n",
    "    print(\"VADER ë¯¸ì„¤ì¹˜. pip install vaderSentiment ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.\")\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°ì„± ë¶„ì„ ë°©ë²•ë¡  ë¹„êµ\n",
    "\n",
    "| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì í•©í•œ ìƒí™© |\n",
    "|------|------|------|------------|\n",
    "| ì‚¬ì „ ê¸°ë°˜ | ë¼ë²¨ë§ ë¶ˆí•„ìš”, ë¹ ë¦„ | ë„ë©”ì¸ í•œì •ì  | ì´ˆê¸° ë¶„ì„, ì¼ë°˜ í…ìŠ¤íŠ¸ |\n",
    "| ê·œì¹™ ê¸°ë°˜ | í•´ì„ ê°€ëŠ¥, ì»¤ìŠ¤í…€ | ê·œì¹™ ì‘ì„± ë…¸ë ¥ | íŠ¹ì • ë„ë©”ì¸ |\n",
    "| ML ê¸°ë°˜ | ë†’ì€ ì •í™•ë„ | ëŒ€ëŸ‰ ë¼ë²¨ ë°ì´í„° í•„ìš” | ì •ë°€ ë¶„ì„ |\n",
    "| ë”¥ëŸ¬ë‹ | ë¬¸ë§¥ ì´í•´ | ê³ ë¹„ìš©, ë³µì¡í•¨ | ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 VADER ê°ì„± ë¶„ì„ (ì˜ì–´)\n",
    "\n",
    "### VADERë€?\n",
    "\n",
    "**VADER (Valence Aware Dictionary and sEntiment Reasoner)**ëŠ” ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„ ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### íŠ¹ì§•\n",
    "\n",
    "- ì´ëª¨í‹°ì½˜, ëŒ€ë¬¸ì, ëŠë‚Œí‘œ ë“± SNS íŠ¹ì„± ë°˜ì˜\n",
    "- ê°•ì¡°ì–´(very, extremely) ì²˜ë¦¬\n",
    "- ë¶€ì •ì–´(not, never) ë°˜ì „ ì²˜ë¦¬\n",
    "- -1 (ë§¤ìš° ë¶€ì •) ~ +1 (ë§¤ìš° ê¸ì •) ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER ê°ì„± ë¶„ì„ ì˜ˆì‹œ\n",
    "if vader_available:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "    test_sentences = [\n",
    "        \"I love this product! It's amazing!\",\n",
    "        \"This is the worst experience ever.\",\n",
    "        \"The movie was okay, nothing special.\",\n",
    "        \"AMAZING!!! Best purchase I've ever made!!!\",\n",
    "        \"Not bad, but could be better.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"VADER ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        print(f\"\\në¬¸ì¥: {sentence}\")\n",
    "        print(f\"  ê¸ì •: {scores['pos']:.3f}, ë¶€ì •: {scores['neg']:.3f}, ì¤‘ë¦½: {scores['neu']:.3f}\")\n",
    "        print(f\"  ì¢…í•© ì ìˆ˜ (compound): {scores['compound']:.3f}\")\n",
    "else:\n",
    "    print(\"VADERê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ì œí’ˆ ë¦¬ë·° ê°ì„± ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œí’ˆ ë¦¬ë·° ë°ì´í„° (ì˜ì–´)\n",
    "product_reviews = pd.DataFrame({\n",
    "    'review_id': range(1, 11),\n",
    "    'product': ['Laptop', 'Laptop', 'Phone', 'Phone', 'Phone', \n",
    "                'Headphones', 'Headphones', 'Tablet', 'Tablet', 'Tablet'],\n",
    "    'review': [\n",
    "        \"Excellent laptop! Fast and reliable.\",\n",
    "        \"Battery life is terrible. Very disappointed.\",\n",
    "        \"Great phone, love the camera!\",\n",
    "        \"Screen cracked after 2 weeks. Awful quality.\",\n",
    "        \"Average phone. Does the job.\",\n",
    "        \"Best headphones I've ever owned! Amazing sound!\",\n",
    "        \"Comfortable but sound quality is mediocre.\",\n",
    "        \"Perfect for reading and browsing.\",\n",
    "        \"Too slow. Not worth the money.\",\n",
    "        \"Good tablet, fast delivery.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"ì œí’ˆ ë¦¬ë·° ë°ì´í„°\")\n",
    "print(product_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADERë¡œ ê°ì„± ë¶„ì„ ìˆ˜í–‰\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"VADERë¥¼ ì‚¬ìš©í•œ ê°ì„± ë¶„ì„\"\"\"\n",
    "    if not vader_available:\n",
    "        return {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 1}\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "def classify_sentiment(compound_score):\n",
    "    \"\"\"compound ì ìˆ˜ë¡œ ê°ì„± ë¶„ë¥˜\"\"\"\n",
    "    if compound_score >= 0.05:\n",
    "        return 'ê¸ì •'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'ë¶€ì •'\n",
    "    else:\n",
    "        return 'ì¤‘ë¦½'\n",
    "\n",
    "# ê°ì„± ë¶„ì„ ì ìš©\n",
    "if vader_available:\n",
    "    product_reviews['sentiment_scores'] = product_reviews['review'].apply(analyze_sentiment_vader)\n",
    "    product_reviews['compound'] = product_reviews['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "    product_reviews['sentiment'] = product_reviews['compound'].apply(classify_sentiment)\n",
    "    \n",
    "    print(\"ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "    print(product_reviews[['product', 'review', 'compound', 'sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 í•œêµ­ì–´ ê°ì„± ë¶„ì„ - KNU ê°ì„± ì‚¬ì „\n",
    "\n",
    "### KNU í•œêµ­ì–´ ê°ì„± ì‚¬ì „\n",
    "\n",
    "êµ°ì‚°ëŒ€í•™êµ(KNU)ì—ì„œ ê°œë°œí•œ í•œêµ­ì–´ ê°ì„± ì‚¬ì „ìœ¼ë¡œ, ê¸ì •/ë¶€ì • ë‹¨ì–´ì™€ ì ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ê°ì„± ì‚¬ì „ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ë‹¨ì–´, ì ìˆ˜\n",
    "ì¢‹ë‹¤, 2\n",
    "ì‹«ë‹¤, -2\n",
    "ê·¸ì €ê·¸ë ‡ë‹¤, 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í•œêµ­ì–´ ê°ì„± ì‚¬ì „ (KNU ìŠ¤íƒ€ì¼)\n",
    "# ì‹¤ì œë¡œëŠ” https://github.com/park1200656/KnuSentiLex ì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "korean_sentiment_dict = {\n",
    "    # ê¸ì • ë‹¨ì–´ (ì ìˆ˜: 1 ~ 2)\n",
    "    'ì¢‹ë‹¤': 2, 'ì¢‹ì•„': 2, 'ì¢‹ì€': 2, 'ì¢‹ì•„ìš”': 2,\n",
    "    'í›Œë¥­í•˜ë‹¤': 2, 'í›Œë¥­í•œ': 2, 'í›Œë¥­í•´ìš”': 2,\n",
    "    'ìµœê³ ': 2, 'ìµœê³ ë‹¤': 2, 'ìµœê³ ì˜ˆìš”': 2,\n",
    "    'ë§Œì¡±': 2, 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤': 2, 'ë§Œì¡±í•´ìš”': 2,\n",
    "    'ì¶”ì²œ': 1, 'ì¶”ì²œí•´ìš”': 1, 'ì¶”ì²œí•©ë‹ˆë‹¤': 1,\n",
    "    'ë§›ìˆë‹¤': 2, 'ë§›ìˆì–´ìš”': 2, 'ë§›ìˆëŠ”': 2,\n",
    "    'ì¹œì ˆí•˜ë‹¤': 2, 'ì¹œì ˆí•´ìš”': 2, 'ì¹œì ˆí•œ': 2,\n",
    "    'ë¹ ë¥´ë‹¤': 1, 'ë¹¨ë¼ìš”': 1, 'ë¹ ë¥¸': 1,\n",
    "    'í¸í•˜ë‹¤': 1, 'í¸í•´ìš”': 1, 'í¸í•œ': 1,\n",
    "    'ê¹¨ë—í•˜ë‹¤': 1, 'ê¹¨ë—í•´ìš”': 1, 'ê¹¨ë—í•œ': 1,\n",
    "    'í–‰ë³µ': 2, 'ê¸°ì˜ë‹¤': 2, 'ì¦ê²ë‹¤': 2,\n",
    "    'ê°ì‚¬': 1, 'ê³ ë§™ë‹¤': 1, 'ê°ë™': 2,\n",
    "    \n",
    "    # ë¶€ì • ë‹¨ì–´ (ì ìˆ˜: -1 ~ -2)\n",
    "    'ë‚˜ì˜ë‹¤': -2, 'ë‚˜ë¹ ìš”': -2, 'ë‚˜ìœ': -2,\n",
    "    'ì‹«ë‹¤': -2, 'ì‹«ì–´ìš”': -2, 'ì‹«ì€': -2,\n",
    "    'ë³„ë¡œ': -1, 'ë³„ë¡œë‹¤': -1, 'ë³„ë¡œì˜ˆìš”': -1,\n",
    "    'ì‹¤ë§': -2, 'ì‹¤ë§ìŠ¤ëŸ½ë‹¤': -2, 'ì‹¤ë§ì´ì—ìš”': -2,\n",
    "    'ë¶ˆë§Œì¡±': -2, 'ë¶ˆë§Œ': -2,\n",
    "    'ìµœì•…': -2, 'ìµœì•…ì´ë‹¤': -2,\n",
    "    'ëŠë¦¬ë‹¤': -1, 'ëŠë ¤ìš”': -1, 'ëŠë¦°': -1,\n",
    "    'ë¶ˆì¹œì ˆ': -2, 'ë¶ˆì¹œì ˆí•˜ë‹¤': -2,\n",
    "    'ë¶ˆí¸í•˜ë‹¤': -1, 'ë¶ˆí¸í•´ìš”': -1,\n",
    "    'ë”ëŸ½ë‹¤': -1, 'ë”ëŸ¬ì›Œìš”': -1,\n",
    "    'ì§œì¦': -2, 'í™”ë‚˜ë‹¤': -2, 'ìŠ¬í”„ë‹¤': -1,\n",
    "    'í›„íšŒ': -2, 'ì•„ì‰½ë‹¤': -1,\n",
    "    \n",
    "    # ê°•ì¡°ì–´\n",
    "    'ë§¤ìš°': 1.5, 'ì •ë§': 1.5, 'ë„ˆë¬´': 1.3, 'ì•„ì£¼': 1.3,\n",
    "    'ì§„ì§œ': 1.3, 'ì™„ì „': 1.5, 'ì—„ì²­': 1.5,\n",
    "    \n",
    "    # ë¶€ì •ì–´\n",
    "    'ì•ˆ': -1, 'ì•Šë‹¤': -1, 'ëª»': -1, 'ì—†ë‹¤': -0.5\n",
    "}\n",
    "\n",
    "print(f\"í•œêµ­ì–´ ê°ì„± ì‚¬ì „ í¬ê¸°: {len(korean_sentiment_dict)}ê°œ ë‹¨ì–´\")\n",
    "print(\"\\nê¸ì • ë‹¨ì–´ ì˜ˆì‹œ:\", [k for k, v in korean_sentiment_dict.items() if v > 0][:5])\n",
    "print(\"ë¶€ì • ë‹¨ì–´ ì˜ˆì‹œ:\", [k for k, v in korean_sentiment_dict.items() if v < 0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_korean_sentiment(text, sentiment_dict):\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ (ì‚¬ì „ ê¸°ë°˜)\n",
    "    \n",
    "    Parameters:\n",
    "    - text: ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "    - sentiment_dict: ê°ì„± ì‚¬ì „ (ë‹¨ì–´: ì ìˆ˜)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: ê°ì„± ì ìˆ˜, ë¶„ë¥˜, ë°œê²¬ëœ í‚¤ì›Œë“œ\n",
    "    \"\"\"\n",
    "    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "    text = text.lower()\n",
    "    \n",
    "    total_score = 0\n",
    "    found_words = []\n",
    "    multiplier = 1.0\n",
    "    \n",
    "    # ê°•ì¡°ì–´ í™•ì¸\n",
    "    intensifiers = ['ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì•„ì£¼', 'ì§„ì§œ', 'ì™„ì „', 'ì—„ì²­']\n",
    "    for word in intensifiers:\n",
    "        if word in text:\n",
    "            multiplier = sentiment_dict.get(word, 1.0)\n",
    "            break\n",
    "    \n",
    "    # ë¶€ì •ì–´ í™•ì¸\n",
    "    negation = False\n",
    "    if 'ì•ˆ ' in text or 'ì•Š' in text or 'ëª» ' in text:\n",
    "        negation = True\n",
    "    \n",
    "    # ê°ì„± ë‹¨ì–´ ê²€ìƒ‰\n",
    "    for word, score in sentiment_dict.items():\n",
    "        if word in text and word not in intensifiers and word not in ['ì•ˆ', 'ì•Šë‹¤', 'ëª»', 'ì—†ë‹¤']:\n",
    "            # ê°•ì¡°ì–´ ê³±í•˜ê¸°\n",
    "            adjusted_score = score * multiplier\n",
    "            # ë¶€ì •ì–´ ìˆìœ¼ë©´ ë°˜ì „\n",
    "            if negation:\n",
    "                adjusted_score = -adjusted_score\n",
    "            total_score += adjusted_score\n",
    "            found_words.append((word, score))\n",
    "    \n",
    "    # ì •ê·œí™” (-1 ~ 1)\n",
    "    if found_words:\n",
    "        normalized_score = total_score / (len(found_words) * 2)  # ìµœëŒ€ ì ìˆ˜ 2 ê¸°ì¤€\n",
    "        normalized_score = max(-1, min(1, normalized_score))  # -1 ~ 1 í´ë¦¬í•‘\n",
    "    else:\n",
    "        normalized_score = 0\n",
    "    \n",
    "    # ë¶„ë¥˜\n",
    "    if normalized_score >= 0.1:\n",
    "        sentiment = 'ê¸ì •'\n",
    "    elif normalized_score <= -0.1:\n",
    "        sentiment = 'ë¶€ì •'\n",
    "    else:\n",
    "        sentiment = 'ì¤‘ë¦½'\n",
    "    \n",
    "    return {\n",
    "        'score': normalized_score,\n",
    "        'raw_score': total_score,\n",
    "        'sentiment': sentiment,\n",
    "        'keywords': found_words\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_korean = [\n",
    "    \"ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ì¶”ì²œí•©ë‹ˆë‹¤.\",\n",
    "    \"ë°°ì†¡ì´ ë„ˆë¬´ ëŠë¦¬ê³  ë¶ˆì¹œì ˆí•´ìš”.\",\n",
    "    \"ê·¸ëƒ¥ ê·¸ë˜ìš”. ë³´í†µì´ì—ìš”.\",\n",
    "    \"ìŒì‹ì´ ë§¤ìš° ë§›ìˆì–´ìš”! ìµœê³ !\",\n",
    "    \"ì„œë¹„ìŠ¤ê°€ ì•ˆ ì¢‹ì•„ìš”.\"\n",
    "]\n",
    "\n",
    "print(\"í•œêµ­ì–´ ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_korean:\n",
    "    result = analyze_korean_sentiment(text, korean_sentiment_dict)\n",
    "    print(f\"\\në¬¸ì¥: {text}\")\n",
    "    print(f\"  ì ìˆ˜: {result['score']:.3f}, ë¶„ë¥˜: {result['sentiment']}\")\n",
    "    print(f\"  í‚¤ì›Œë“œ: {result['keywords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ì‡¼í•‘ëª° ë¦¬ë·° ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‡¼í•‘ëª° ë¦¬ë·° ë°ì´í„° (í•œêµ­ì–´)\n",
    "korean_reviews = pd.DataFrame({\n",
    "    'review_id': range(1, 16),\n",
    "    'product': ['ìŠ¤ë§ˆíŠ¸í°'] * 5 + ['ë…¸íŠ¸ë¶'] * 5 + ['ì´ì–´í°'] * 5,\n",
    "    'review': [\n",
    "        # ìŠ¤ë§ˆíŠ¸í° ë¦¬ë·°\n",
    "        \"ì¹´ë©”ë¼ í™”ì§ˆì´ ì •ë§ ì¢‹ì•„ìš”! ë§Œì¡±í•©ë‹ˆë‹¤.\",\n",
    "        \"ë°°í„°ë¦¬ê°€ ë„ˆë¬´ ë¹¨ë¦¬ ë‹³ì•„ìš”. ì‹¤ë§ì´ì—ìš”.\",\n",
    "        \"ë””ìì¸ì€ ì˜ˆìœë° ê°€ê²©ì´ ì¢€ ë¹„ì‹¸ìš”.\",\n",
    "        \"ì™„ì „ ìµœê³ ! ì¶”ì²œí•©ë‹ˆë‹¤!\",\n",
    "        \"ê·¸ëƒ¥ ê·¸ë˜ìš”. ê¸°ëŒ€í–ˆë˜ ê²ƒë³´ë‹¤ ë³„ë¡œ.\",\n",
    "        # ë…¸íŠ¸ë¶ ë¦¬ë·°\n",
    "        \"ê°€ë³ê³  ë¹¨ë¼ì„œ ì¢‹ì•„ìš”. ë§¤ìš° ë§Œì¡±!\",\n",
    "        \"ë°œì—´ì´ ì‹¬í•˜ê³  íŒ¬ ì†ŒìŒì´ ì»¤ìš”.\",\n",
    "        \"ê°€ì„±ë¹„ ìµœê³ ! ì¶”ì²œí•´ìš”.\",\n",
    "        \"í‚¤ë³´ë“œê°€ ë¶ˆí¸í•´ìš”. í›„íšŒë©ë‹ˆë‹¤.\",\n",
    "        \"ì—…ë¬´ìš©ìœ¼ë¡œ ë”± ì¢‹ì•„ìš”.\",\n",
    "        # ì´ì–´í° ë¦¬ë·°\n",
    "        \"ìŒì§ˆì´ ì •ë§ í›Œë¥­í•´ìš”!\",\n",
    "        \"ì°©ìš©ê°ì´ ë¶ˆí¸í•˜ê³  ì—°ê²°ì´ ìì£¼ ëŠê²¨ìš”.\",\n",
    "        \"ê°€ê²© ëŒ€ë¹„ ê´œì°®ì•„ìš”.\",\n",
    "        \"ìµœì•…ì´ì—ìš”. ëˆ ì•„ê¹ë‹¤.\",\n",
    "        \"ë…¸ì´ì¦ˆìº”ìŠ¬ë§ì´ ì¢‹ì•„ìš”. ì¶”ì²œ!\"\n",
    "    ],\n",
    "    'rating': [5, 2, 3, 5, 3, 5, 2, 5, 1, 4, 5, 2, 4, 1, 5]\n",
    "})\n",
    "\n",
    "print(\"ì‡¼í•‘ëª° ë¦¬ë·° ë°ì´í„°\")\n",
    "print(korean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ê°ì„± ë¶„ì„ ì ìš©\n",
    "korean_reviews['sentiment_result'] = korean_reviews['review'].apply(\n",
    "    lambda x: analyze_korean_sentiment(x, korean_sentiment_dict)\n",
    ")\n",
    "korean_reviews['sentiment_score'] = korean_reviews['sentiment_result'].apply(lambda x: x['score'])\n",
    "korean_reviews['sentiment'] = korean_reviews['sentiment_result'].apply(lambda x: x['sentiment'])\n",
    "korean_reviews['keywords'] = korean_reviews['sentiment_result'].apply(lambda x: [w[0] for w in x['keywords']])\n",
    "\n",
    "print(\"í•œêµ­ì–´ ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "print(korean_reviews[['product', 'review', 'rating', 'sentiment_score', 'sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 ì»¤ìŠ¤í…€ ê°ì„± ì‚¬ì „ êµ¬ì¶•\n",
    "\n",
    "### ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „ì˜ ì¤‘ìš”ì„±\n",
    "\n",
    "ì¼ë°˜ ê°ì„± ì‚¬ì „ì€ ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "- ê¸ˆìœµ: \"í­ë“±\" (ì¼ë°˜ì ìœ¼ë¡œ ë¶€ì •, ì£¼ì‹ì—ì„œëŠ” ê¸ì •)\n",
    "- ê²Œì„: \"í‚¬\" (ì¼ë°˜ì ìœ¼ë¡œ ë¶€ì •, ê²Œì„ì—ì„œëŠ” ì¤‘ë¦½)\n",
    "- ìŒì‹: \"ë§¤ì½¤í•˜ë‹¤\" (í˜¸ë¶ˆí˜¸ ìˆìŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„ë©”ì¸ íŠ¹í™” ê°ì„± ì‚¬ì „ (ìŒì‹ ë¦¬ë·°)\n",
    "food_sentiment_dict = korean_sentiment_dict.copy()\n",
    "\n",
    "# ìŒì‹ ë„ë©”ì¸ ì¶”ê°€ ë‹¨ì–´\n",
    "food_words = {\n",
    "    # ê¸ì •\n",
    "    'ì‹ ì„ í•˜ë‹¤': 2, 'ì‹ ì„ í•´ìš”': 2, 'ì‹ ì„ í•œ': 2,\n",
    "    'ë°”ì‚­í•˜ë‹¤': 1, 'ë°”ì‚­í•´ìš”': 1, 'ë°”ì‚­í•œ': 1,\n",
    "    'ì´‰ì´‰í•˜ë‹¤': 1, 'ì´‰ì´‰í•´ìš”': 1,\n",
    "    'í’ë¯¸': 2, 'ê³ ì†Œí•˜ë‹¤': 1, 'ë‹¬ì½¤í•˜ë‹¤': 1,\n",
    "    'ì–‘ë§ë‹¤': 1, 'ê°€ì„±ë¹„': 1, 'í‘¸ì§í•˜ë‹¤': 2,\n",
    "    'ë¶„ìœ„ê¸°ì¢‹ë‹¤': 1,\n",
    "    \n",
    "    # ë¶€ì •\n",
    "    'ì§œë‹¤': -1, 'ì§œìš”': -1, 'ì§ ': -1,\n",
    "    'ëŠë¼í•˜ë‹¤': -1, 'ëŠë¼í•´ìš”': -1,\n",
    "    'ì‹ì—ˆë‹¤': -1, 'ì‹ì–´ìš”': -1, 'ì°¨ê°‘ë‹¤': -1,\n",
    "    'ì–‘ì ë‹¤': -1, 'ì–‘ì—†ë‹¤': -1,\n",
    "    'ë¹„ì‹¸ë‹¤': -1, 'ë¹„ì‹¸ìš”': -1,\n",
    "    'ì˜¤ë˜ê¸°ë‹¤ë ¸ë‹¤': -1, 'ì›¨ì´íŒ…': -0.5\n",
    "}\n",
    "\n",
    "food_sentiment_dict.update(food_words)\n",
    "\n",
    "# ìŒì‹ ë¦¬ë·° í…ŒìŠ¤íŠ¸\n",
    "food_reviews = [\n",
    "    \"íŒŒìŠ¤íƒ€ê°€ ì •ë§ ë§›ìˆì–´ìš”! ë©´ì´ ì´‰ì´‰í•˜ê³  ì–‘ë„ í‘¸ì§í•´ìš”.\",\n",
    "    \"í”¼ìê°€ ë„ˆë¬´ ì§œê³  ëŠë¼í•´ìš”. ê¸°ë¦„ì´ ë‘¥ë‘¥.\",\n",
    "    \"ë¶„ìœ„ê¸°ëŠ” ì¢‹ì€ë° ê°€ê²©ì´ ì¢€ ë¹„ì‹¸ìš”.\",\n",
    "    \"ì‹ ì„ í•œ ì¬ë£Œë¡œ ë§Œë“  ìƒëŸ¬ë“œ! ìµœê³ !\"\n",
    "]\n",
    "\n",
    "print(\"ìŒì‹ ë¦¬ë·° ê°ì„± ë¶„ì„ (ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for review in food_reviews:\n",
    "    result = analyze_korean_sentiment(review, food_sentiment_dict)\n",
    "    print(f\"\\në¦¬ë·°: {review}\")\n",
    "    print(f\"  ì ìˆ˜: {result['score']:.3f}, ë¶„ë¥˜: {result['sentiment']}\")\n",
    "    print(f\"  í‚¤ì›Œë“œ: {[w[0] for w in result['keywords']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¥˜ í•¨ìˆ˜\n",
    "\n",
    "### ì¢…í•© ë¶„ë¥˜ í•¨ìˆ˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    ë‹¤êµ­ì–´ ê°ì„± ë¶„ì„ê¸°\n",
    "    - ì˜ì–´: VADER ì‚¬ìš©\n",
    "    - í•œêµ­ì–´: ì»¤ìŠ¤í…€ ì‚¬ì „ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, korean_dict=None):\n",
    "        self.korean_dict = korean_dict or korean_sentiment_dict\n",
    "        if vader_available:\n",
    "            self.vader = SentimentIntensityAnalyzer()\n",
    "        else:\n",
    "            self.vader = None\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        \"\"\"ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€\"\"\"\n",
    "        korean_chars = len(re.findall('[ê°€-í£]', text))\n",
    "        english_chars = len(re.findall('[a-zA-Z]', text))\n",
    "        return 'korean' if korean_chars > english_chars else 'english'\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„\"\"\"\n",
    "        lang = self.detect_language(text)\n",
    "        \n",
    "        if lang == 'english' and self.vader:\n",
    "            scores = self.vader.polarity_scores(text)\n",
    "            score = scores['compound']\n",
    "        else:\n",
    "            result = analyze_korean_sentiment(text, self.korean_dict)\n",
    "            score = result['score']\n",
    "        \n",
    "        # ë¶„ë¥˜\n",
    "        if score >= 0.05:\n",
    "            sentiment = 'ê¸ì •'\n",
    "            label = 'positive'\n",
    "        elif score <= -0.05:\n",
    "            sentiment = 'ë¶€ì •'\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            sentiment = 'ì¤‘ë¦½'\n",
    "            label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'language': lang,\n",
    "            'score': score,\n",
    "            'sentiment_kr': sentiment,\n",
    "            'sentiment_en': label\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, texts):\n",
    "        \"\"\"ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì¼ê´„ ë¶„ì„\"\"\"\n",
    "        return [self.analyze(text) for text in texts]\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "mixed_texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!\",\n",
    "    \"Terrible experience.\",\n",
    "    \"ìµœì•…ì´ì—ìš”. ì‹¤ë§ì…ë‹ˆë‹¤.\",\n",
    "    \"It's okay.\",\n",
    "    \"ê·¸ëƒ¥ ë³´í†µì´ì—ìš”.\"\n",
    "]\n",
    "\n",
    "print(\"ë‹¤êµ­ì–´ ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = analyzer.analyze_batch(mixed_texts)\n",
    "for r in results:\n",
    "    print(f\"\\n{r['text']}\")\n",
    "    print(f\"  ì–¸ì–´: {r['language']}, ì ìˆ˜: {r['score']:.3f}, ê°ì„±: {r['sentiment_kr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: ê°ì„± ë¶„ì„ ì‹œê°í™”ì™€ ì‘ìš©\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 pandasë¡œ ê°ì„± ì ìˆ˜ ì§‘ê³„\n",
    "\n",
    "### ì œí’ˆë³„/ì¹´í…Œê³ ë¦¬ë³„ ê°ì„± ì§‘ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œí’ˆë³„ ê°ì„± ì§‘ê³„\n",
    "product_sentiment = korean_reviews.groupby('product').agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count'],\n",
    "    'rating': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "product_sentiment.columns = ['í‰ê· _ê°ì„±ì ìˆ˜', 'ê°ì„±ì ìˆ˜_í‘œì¤€í¸ì°¨', 'ë¦¬ë·°ìˆ˜', 'í‰ê· _í‰ì ']\n",
    "product_sentiment = product_sentiment.sort_values('í‰ê· _ê°ì„±ì ìˆ˜', ascending=False)\n",
    "\n",
    "print(\"ì œí’ˆë³„ ê°ì„± ë¶„ì„ ìš”ì•½\")\n",
    "print(product_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„±ë³„ ë¦¬ë·° ìˆ˜ ì§‘ê³„\n",
    "sentiment_counts = korean_reviews['sentiment'].value_counts()\n",
    "sentiment_by_product = korean_reviews.groupby(['product', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"ì „ì²´ ê°ì„± ë¶„í¬\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nì œí’ˆë³„ ê°ì„± ë¶„í¬\")\n",
    "print(sentiment_by_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ì ê³¼ ê°ì„± ì ìˆ˜ ìƒê´€ê´€ê³„\n",
    "correlation = korean_reviews['rating'].corr(korean_reviews['sentiment_score'])\n",
    "print(f\"í‰ì ê³¼ ê°ì„± ì ìˆ˜ì˜ ìƒê´€ê³„ìˆ˜: {correlation:.3f}\")\n",
    "\n",
    "# í‰ì ë³„ í‰ê·  ê°ì„± ì ìˆ˜\n",
    "rating_sentiment = korean_reviews.groupby('rating')['sentiment_score'].agg(['mean', 'count'])\n",
    "print(\"\\ní‰ì ë³„ í‰ê·  ê°ì„± ì ìˆ˜\")\n",
    "print(rating_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: VOC(Voice of Customer) ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "def generate_voc_report(df, product_col='product', sentiment_col='sentiment', score_col='sentiment_score'):\n",
    "    \"\"\"\n",
    "    VOC ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # ì „ì²´ ìš”ì•½\n",
    "    total = len(df)\n",
    "    positive = (df[sentiment_col] == 'ê¸ì •').sum()\n",
    "    negative = (df[sentiment_col] == 'ë¶€ì •').sum()\n",
    "    neutral = (df[sentiment_col] == 'ì¤‘ë¦½').sum()\n",
    "    \n",
    "    report['ì „ì²´ ìš”ì•½'] = {\n",
    "        'ì´ ë¦¬ë·° ìˆ˜': total,\n",
    "        'ê¸ì • ë¹„ìœ¨': f\"{positive/total*100:.1f}%\",\n",
    "        'ë¶€ì • ë¹„ìœ¨': f\"{negative/total*100:.1f}%\",\n",
    "        'ì¤‘ë¦½ ë¹„ìœ¨': f\"{neutral/total*100:.1f}%\",\n",
    "        'í‰ê·  ê°ì„± ì ìˆ˜': f\"{df[score_col].mean():.3f}\"\n",
    "    }\n",
    "    \n",
    "    # ì œí’ˆë³„ ìš”ì•½\n",
    "    product_summary = df.groupby(product_col).agg({\n",
    "        score_col: 'mean',\n",
    "        sentiment_col: lambda x: (x == 'ê¸ì •').sum() / len(x) * 100\n",
    "    }).round(1)\n",
    "    product_summary.columns = ['í‰ê·  ì ìˆ˜', 'ê¸ì •ë¥ (%)']\n",
    "    report['ì œí’ˆë³„ ìš”ì•½'] = product_summary\n",
    "    \n",
    "    # ê°œì„  í•„ìš” ì œí’ˆ (ë¶€ì • ë¹„ìœ¨ ë†’ì€)\n",
    "    needs_improvement = df[df[sentiment_col] == 'ë¶€ì •'].groupby(product_col).size()\n",
    "    report['ê°œì„  í•„ìš”'] = needs_improvement.sort_values(ascending=False)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# VOC ë¦¬í¬íŠ¸ ìƒì„±\n",
    "voc_report = generate_voc_report(korean_reviews)\n",
    "\n",
    "print(\"VOC ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n[ì „ì²´ ìš”ì•½]\")\n",
    "for k, v in voc_report['ì „ì²´ ìš”ì•½'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n[ì œí’ˆë³„ ìš”ì•½]\")\n",
    "print(voc_report['ì œí’ˆë³„ ìš”ì•½'])\n",
    "\n",
    "print(\"\\n[ê°œì„  í•„ìš” ì œí’ˆ (ë¶€ì • ë¦¬ë·° ìˆ˜)]\")\n",
    "print(voc_report['ê°œì„  í•„ìš”'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Plotlyë¡œ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
    "\n",
    "### íŒŒì´ ì°¨íŠ¸: ì „ì²´ ê°ì„± ë¶„í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„± ë¶„í¬ íŒŒì´ ì°¨íŠ¸\n",
    "sentiment_counts = korean_reviews['sentiment'].value_counts()\n",
    "\n",
    "# ìƒ‰ìƒ ì •ì˜\n",
    "colors = {'ê¸ì •': '#2ecc71', 'ë¶€ì •': '#e74c3c', 'ì¤‘ë¦½': '#95a5a6'}\n",
    "\n",
    "fig = px.pie(\n",
    "    values=sentiment_counts.values,\n",
    "    names=sentiment_counts.index,\n",
    "    title='ì „ì²´ ë¦¬ë·° ê°ì„± ë¶„í¬',\n",
    "    color=sentiment_counts.index,\n",
    "    color_discrete_map=colors,\n",
    "    hole=0.4  # ë„ë„› ì°¨íŠ¸\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='inside',\n",
    "    textinfo='percent+label+value',\n",
    "    textfont_size=14\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=450,\n",
    "    annotations=[dict(text='ê°ì„± ë¶„í¬', x=0.5, y=0.5, font_size=16, showarrow=False)]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§‰ëŒ€ ì°¨íŠ¸: ì œí’ˆë³„ ê°ì„± ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œí’ˆë³„ ê°ì„± ì ìˆ˜ ë§‰ëŒ€ ì°¨íŠ¸\n",
    "product_scores = korean_reviews.groupby('product')['sentiment_score'].mean().sort_values()\n",
    "\n",
    "# ìƒ‰ìƒ: ì ìˆ˜ì— ë”°ë¼ (ê¸ì •=ì´ˆë¡, ë¶€ì •=ë¹¨ê°•)\n",
    "bar_colors = ['#2ecc71' if x >= 0 else '#e74c3c' for x in product_scores.values]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=product_scores.index,\n",
    "    y=product_scores.values,\n",
    "    marker_color=bar_colors,\n",
    "    text=[f'{v:.2f}' for v in product_scores.values],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "# ê¸°ì¤€ì„  ì¶”ê°€\n",
    "fig.add_hline(y=0, line_dash='dash', line_color='gray')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ì œí’ˆë³„ í‰ê·  ê°ì„± ì ìˆ˜',\n",
    "    xaxis_title='ì œí’ˆ',\n",
    "    yaxis_title='ê°ì„± ì ìˆ˜ (-1 ~ 1)',\n",
    "    yaxis=dict(range=[-1, 1]),\n",
    "    width=700,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œí’ˆë³„ ê°ì„± ë¶„í¬ ëˆ„ì  ë§‰ëŒ€ ì°¨íŠ¸\n",
    "sentiment_by_product = korean_reviews.groupby(['product', 'sentiment']).size().unstack(fill_value=0)\n",
    "# ìˆœì„œ ì§€ì •\n",
    "sentiment_order = ['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •']\n",
    "sentiment_by_product = sentiment_by_product.reindex(columns=sentiment_order, fill_value=0)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for sentiment in sentiment_order:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sentiment_by_product.index,\n",
    "        y=sentiment_by_product[sentiment],\n",
    "        name=sentiment,\n",
    "        marker_color=colors[sentiment]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ì œí’ˆë³„ ê°ì„± ë¶„í¬ (ë¦¬ë·° ìˆ˜)',\n",
    "    xaxis_title='ì œí’ˆ',\n",
    "    yaxis_title='ë¦¬ë·° ìˆ˜',\n",
    "    barmode='stack',\n",
    "    legend_title='ê°ì„±',\n",
    "    width=700,\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ë¸Œëœë“œ í‰íŒ ëŒ€ì‹œë³´ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•© ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{'type': 'pie'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'scatter'}]],\n",
    "    subplot_titles=('ì „ì²´ ê°ì„± ë¶„í¬', 'ì œí’ˆë³„ í‰ê·  ì ìˆ˜', \n",
    "                   'ì œí’ˆë³„ ê°ì„± ë¶„í¬', 'í‰ì  vs ê°ì„± ì ìˆ˜')\n",
    ")\n",
    "\n",
    "# 1. íŒŒì´ ì°¨íŠ¸\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=sentiment_counts.index,\n",
    "        values=sentiment_counts.values,\n",
    "        marker_colors=[colors[s] for s in sentiment_counts.index],\n",
    "        hole=0.3\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. ì œí’ˆë³„ í‰ê·  ì ìˆ˜\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=product_scores.index,\n",
    "        y=product_scores.values,\n",
    "        marker_color=bar_colors,\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. ì œí’ˆë³„ ê°ì„± ë¶„í¬ (ëˆ„ì )\n",
    "for sentiment in sentiment_order:\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=sentiment_by_product.index,\n",
    "            y=sentiment_by_product[sentiment],\n",
    "            name=sentiment,\n",
    "            marker_color=colors[sentiment],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. í‰ì  vs ê°ì„± ì ìˆ˜ ì‚°ì ë„\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=korean_reviews['rating'],\n",
    "        y=korean_reviews['sentiment_score'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=korean_reviews['sentiment_score'],\n",
    "            colorscale='RdYlGn',\n",
    "            showscale=True\n",
    "        ),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='ë¸Œëœë“œ í‰íŒ ë¶„ì„ ëŒ€ì‹œë³´ë“œ',\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    barmode='stack'\n",
    ")\n",
    "\n",
    "# ì¶• ë ˆì´ë¸”\n",
    "fig.update_xaxes(title_text='ì œí’ˆ', row=1, col=2)\n",
    "fig.update_yaxes(title_text='ê°ì„± ì ìˆ˜', row=1, col=2)\n",
    "fig.update_xaxes(title_text='ì œí’ˆ', row=2, col=1)\n",
    "fig.update_yaxes(title_text='ë¦¬ë·° ìˆ˜', row=2, col=1)\n",
    "fig.update_xaxes(title_text='í‰ì ', row=2, col=2)\n",
    "fig.update_yaxes(title_text='ê°ì„± ì ìˆ˜', row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ë‚ ì§œë³„ ë¦¬ë·°)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 30ì¼ê°„ì˜ ë¦¬ë·° ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜\n",
    "dates = pd.date_range('2024-01-01', periods=30, freq='D')\n",
    "n_reviews_per_day = np.random.randint(5, 15, size=30)\n",
    "\n",
    "time_series_reviews = []\n",
    "for date, n in zip(dates, n_reviews_per_day):\n",
    "    for _ in range(n):\n",
    "        # ê°ì„± ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜ (ì‹œê°„ì— ë”°ë¼ ë³€í™”)\n",
    "        base_score = 0.2 + 0.3 * np.sin(date.day / 10)  # ì£¼ê¸°ì  ë³€í™”\n",
    "        score = base_score + np.random.normal(0, 0.3)\n",
    "        score = max(-1, min(1, score))  # í´ë¦¬í•‘\n",
    "        \n",
    "        sentiment = 'ê¸ì •' if score > 0.1 else ('ë¶€ì •' if score < -0.1 else 'ì¤‘ë¦½')\n",
    "        \n",
    "        time_series_reviews.append({\n",
    "            'date': date,\n",
    "            'sentiment_score': score,\n",
    "            'sentiment': sentiment\n",
    "        })\n",
    "\n",
    "ts_df = pd.DataFrame(time_series_reviews)\n",
    "print(f\"ì´ ë¦¬ë·° ìˆ˜: {len(ts_df)}\")\n",
    "print(ts_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë³„ ê°ì„± ì ìˆ˜ ì¶”ì´\n",
    "daily_sentiment = ts_df.groupby('date').agg({\n",
    "    'sentiment_score': ['mean', 'std', 'count']\n",
    "}).round(3)\n",
    "daily_sentiment.columns = ['í‰ê· _ì ìˆ˜', 'í‘œì¤€í¸ì°¨', 'ë¦¬ë·°ìˆ˜']\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "\n",
    "# ì´ë™ í‰ê·  ì¶”ê°€\n",
    "daily_sentiment['MA7'] = daily_sentiment['í‰ê· _ì ìˆ˜'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# ì¼ë³„ ì ìˆ˜\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=daily_sentiment['date'],\n",
    "    y=daily_sentiment['í‰ê· _ì ìˆ˜'],\n",
    "    mode='lines+markers',\n",
    "    name='ì¼ë³„ í‰ê· ',\n",
    "    line=dict(color='#3498db', width=1),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "# 7ì¼ ì´ë™í‰ê· \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=daily_sentiment['date'],\n",
    "    y=daily_sentiment['MA7'],\n",
    "    mode='lines',\n",
    "    name='7ì¼ ì´ë™í‰ê· ',\n",
    "    line=dict(color='#e74c3c', width=3)\n",
    "))\n",
    "\n",
    "# ê¸°ì¤€ì„ \n",
    "fig.add_hline(y=0, line_dash='dash', line_color='gray')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ì¼ë³„ ê°ì„± ì ìˆ˜ ì¶”ì´ (7ì¼ ì´ë™í‰ê· )',\n",
    "    xaxis_title='ë‚ ì§œ',\n",
    "    yaxis_title='ê°ì„± ì ìˆ˜',\n",
    "    yaxis=dict(range=[-1, 1]),\n",
    "    width=900,\n",
    "    height=450,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë³„ ê°ì„± ë¹„ìœ¨ ë³€í™” (ìŠ¤íƒ ì˜ì—­ ì°¨íŠ¸)\n",
    "daily_sentiment_dist = ts_df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)\n",
    "daily_sentiment_pct = daily_sentiment_dist.div(daily_sentiment_dist.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for sentiment in ['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •']:\n",
    "    if sentiment in daily_sentiment_pct.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=daily_sentiment_pct.index,\n",
    "            y=daily_sentiment_pct[sentiment],\n",
    "            mode='lines',\n",
    "            name=sentiment,\n",
    "            line=dict(color=colors[sentiment]),\n",
    "            stackgroup='one',\n",
    "            fillcolor=colors[sentiment]\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ì¼ë³„ ê°ì„± ë¹„ìœ¨ ë³€í™” (%)',\n",
    "    xaxis_title='ë‚ ì§œ',\n",
    "    yaxis_title='ë¹„ìœ¨ (%)',\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    width=900,\n",
    "    height=450,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„ ì†Œê°œ\n",
    "\n",
    "### ì‚¬ì „ ê¸°ë°˜ vs ML ê¸°ë°˜ ë¹„êµ\n",
    "\n",
    "| êµ¬ë¶„ | ì‚¬ì „ ê¸°ë°˜ | ML ê¸°ë°˜ |\n",
    "|------|----------|--------|\n",
    "| ì¤€ë¹„ ì‹œê°„ | ë¹ ë¦„ (ì‚¬ì „ë§Œ ìˆìœ¼ë©´) | ëŠë¦¼ (ë¼ë²¨ ë°ì´í„° í•„ìš”) |\n",
    "| ì •í™•ë„ | ë³´í†µ (70-80%) | ë†’ìŒ (85-95%) |\n",
    "| ë„ë©”ì¸ ì ì‘ | ì‚¬ì „ ì—…ë°ì´íŠ¸ í•„ìš” | ì¬í•™ìŠµ í•„ìš” |\n",
    "| í•´ì„ ê°€ëŠ¥ì„± | ë†’ìŒ (í‚¤ì›Œë“œ í™•ì¸) | ë‚®ìŒ (ë¸”ë™ë°•ìŠ¤) |\n",
    "| ìƒˆë¡œìš´ í‘œí˜„ | ì²˜ë¦¬ ì–´ë ¤ì›€ | í•™ìŠµ ë°ì´í„°ì— ë”°ë¼ ê°€ëŠ¥ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML ê¸°ë°˜ ê°ì„± ë¶„ì„ ë§›ë³´ê¸° (scikit-learn)\n",
    "# ì‹¤ì œë¡œëŠ” Week 4ì—ì„œ ìì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ê°„ë‹¨í•œ ë¼ë²¨ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” ë§ì€ ë°ì´í„° í•„ìš”)\n",
    "ml_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'ì •ë§ ì¢‹ì•„ìš”', 'ìµœê³ ì…ë‹ˆë‹¤', 'ë§Œì¡±í•´ìš”', 'ì¶”ì²œí•©ë‹ˆë‹¤', 'í›Œë¥­í•´ìš”',\n",
    "        'ë³„ë¡œì˜ˆìš”', 'ì‹¤ë§ì´ì—ìš”', 'ìµœì•…ì…ë‹ˆë‹¤', 'í›„íšŒí•´ìš”', 'ë‚˜ë¹ ìš”',\n",
    "        'ê·¸ëƒ¥ ê·¸ë˜ìš”', 'ë³´í†µì´ì—ìš”', 'ê´œì°®ì•„ìš”', 'ë¬´ë‚œí•´ìš”', 'í‰ë²”í•´ìš”'\n",
    "    ],\n",
    "    'label': ['ê¸ì •']*5 + ['ë¶€ì •']*5 + ['ì¤‘ë¦½']*5\n",
    "})\n",
    "\n",
    "# TF-IDF ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(ml_data['text'])\n",
    "y = ml_data['label']\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ (ë°ì´í„°ê°€ ì ì–´ì„œ ì „ì²´ë¡œ í•™ìŠµ)\n",
    "model = MultinomialNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "# ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "new_texts = ['ì´ ì œí’ˆ ì§„ì§œ ì¢‹ì•„ìš”', 'ë³„ë¡œë„¤ìš” ì‹¤ë§', 'ê·¸ëƒ¥ í‰ë²”í•´ìš”']\n",
    "new_X = vectorizer.transform(new_texts)\n",
    "predictions = model.predict(new_X)\n",
    "probabilities = model.predict_proba(new_X)\n",
    "\n",
    "print(\"ML ê¸°ë°˜ ê°ì„± ë¶„ì„ ì˜ˆì¸¡ ê²°ê³¼\")\n",
    "print(\"=\" * 50)\n",
    "for text, pred, prob in zip(new_texts, predictions, probabilities):\n",
    "    print(f\"\\ní…ìŠ¤íŠ¸: {text}\")\n",
    "    print(f\"  ì˜ˆì¸¡: {pred}\")\n",
    "    print(f\"  í™•ë¥ : {dict(zip(model.classes_, prob.round(3)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì‹¤ë¬´ íŒ: ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ?\n",
    "\n",
    "| ìƒí™© | ê¶Œì¥ ë°©ë²• |\n",
    "|------|----------|\n",
    "| ë¹ ë¥¸ ì´ˆê¸° ë¶„ì„ | ì‚¬ì „ ê¸°ë°˜ |\n",
    "| ë¼ë²¨ ë°ì´í„° ì—†ìŒ | ì‚¬ì „ ê¸°ë°˜ |\n",
    "| ë†’ì€ ì •í™•ë„ í•„ìš” | ML ê¸°ë°˜ |\n",
    "| ì‹¤ì‹œê°„ ëŒ€ëŸ‰ ì²˜ë¦¬ | ì‚¬ì „ ê¸°ë°˜ (ì†ë„) |\n",
    "| íŠ¹ì • ë„ë©”ì¸ | ì»¤ìŠ¤í…€ ì‚¬ì „ + ML |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ì‹¤ìŠµ í€´ì¦ˆ\n",
    "\n",
    "**ë‚œì´ë„**: â­ (ì‰¬ì›€) ~ â­â­â­â­â­ (ì–´ë ¤ì›€)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. ê°ì„± ì ìˆ˜ë¡œ ë¶„ë¥˜í•˜ê¸° â­\n",
    "\n",
    "**ë¬¸ì œ**: ë‹¤ìŒ ê°ì„± ì ìˆ˜ë“¤ì„ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "- 0.05 ì´ìƒ: ê¸ì •\n",
    "- -0.05 ì´í•˜: ë¶€ì •\n",
    "- ê·¸ ì™¸: ì¤‘ë¦½\n",
    "\n",
    "```python\n",
    "scores = [0.8, -0.3, 0.02, -0.7, 0.5, 0.0, -0.04]\n",
    "```\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼**: `['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½', 'ë¶€ì •', 'ê¸ì •', 'ì¤‘ë¦½', 'ì¤‘ë¦½']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [0.8, -0.3, 0.02, -0.7, 0.5, 0.0, -0.04]\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. ê°ì„± ì‚¬ì „ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°í•˜ê¸° â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ì£¼ì–´ì§„ ê°ì„± ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "simple_dict = {'ì¢‹ë‹¤': 1, 'ë‚˜ì˜ë‹¤': -1, 'ìµœê³ ': 2, 'ìµœì•…': -2, 'ë³´í†µ': 0}\n",
    "texts = ['ì´ ìŒì‹ ìµœê³ ë‹¤', 'ì„œë¹„ìŠ¤ê°€ ë‚˜ì˜ë‹¤', 'ê·¸ëƒ¥ ë³´í†µì´ë‹¤']\n",
    "```\n",
    "\n",
    "**ì¶œë ¥**: ê° í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dict = {'ì¢‹ë‹¤': 1, 'ë‚˜ì˜ë‹¤': -1, 'ìµœê³ ': 2, 'ìµœì•…': -2, 'ë³´í†µ': 0}\n",
    "texts = ['ì´ ìŒì‹ ìµœê³ ë‹¤', 'ì„œë¹„ìŠ¤ê°€ ë‚˜ì˜ë‹¤', 'ê·¸ëƒ¥ ë³´í†µì´ë‹¤']\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. pandasë¡œ ê°ì„± ë¶„í¬ ì§‘ê³„í•˜ê¸° â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ë‹¤ìŒ ë¦¬ë·° ë°ì´í„°ì—ì„œ ê°ì„±ë³„ ë¦¬ë·° ìˆ˜ì™€ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "reviews_df = pd.DataFrame({\n",
    "    'review': ['ì¢‹ì•„ìš”', 'ë³„ë¡œì˜ˆìš”', 'ìµœê³ !', 'ê·¸ëƒ¥ ê·¸ë˜ìš”', 'ì‹¤ë§ì´ì—ìš”', 'ë§Œì¡±í•´ìš”'],\n",
    "    'sentiment': ['ê¸ì •', 'ë¶€ì •', 'ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •', 'ê¸ì •']\n",
    "})\n",
    "```\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼**: ê°ì„±ë³„ ê°œìˆ˜ì™€ ë¹„ìœ¨(%) ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame({\n",
    "    'review': ['ì¢‹ì•„ìš”', 'ë³„ë¡œì˜ˆìš”', 'ìµœê³ !', 'ê·¸ëƒ¥ ê·¸ë˜ìš”', 'ì‹¤ë§ì´ì—ìš”', 'ë§Œì¡±í•´ìš”'],\n",
    "    'sentiment': ['ê¸ì •', 'ë¶€ì •', 'ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •', 'ê¸ì •']\n",
    "})\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Plotly íŒŒì´ ì°¨íŠ¸ ê·¸ë¦¬ê¸° â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: Q3ì˜ ê°ì„± ë¶„í¬ë¥¼ Plotly íŒŒì´ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•˜ì„¸ìš”.\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­**:\n",
    "- ê¸ì •=ì´ˆë¡, ë¶€ì •=ë¹¨ê°•, ì¤‘ë¦½=íšŒìƒ‰\n",
    "- í¼ì„¼íŠ¸ì™€ ë ˆì´ë¸” í‘œì‹œ\n",
    "- ë„ë„› ì°¨íŠ¸ í˜•íƒœ (hole=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3ì˜ ë°ì´í„° ì‚¬ìš©\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. ì¹´í…Œê³ ë¦¬ë³„ ê°ì„± ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸ â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ë‹¤ìŒ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ë§‰ëŒ€ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "category_data = pd.DataFrame({\n",
    "    'category': ['ìŒì‹', 'ìŒì‹', 'ì„œë¹„ìŠ¤', 'ì„œë¹„ìŠ¤', 'ê°€ê²©', 'ê°€ê²©'],\n",
    "    'sentiment_score': [0.8, 0.6, -0.3, 0.2, 0.4, -0.5]\n",
    "})\n",
    "```\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­**:\n",
    "- í‰ê·  ì ìˆ˜ ê³„ì‚°\n",
    "- ì–‘ìˆ˜=ì´ˆë¡, ìŒìˆ˜=ë¹¨ê°• ìƒ‰ìƒ\n",
    "- 0 ê¸°ì¤€ì„  í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = pd.DataFrame({\n",
    "    'category': ['ìŒì‹', 'ìŒì‹', 'ì„œë¹„ìŠ¤', 'ì„œë¹„ìŠ¤', 'ê°€ê²©', 'ê°€ê²©'],\n",
    "    'sentiment_score': [0.8, 0.6, -0.3, 0.2, 0.4, -0.5]\n",
    "})\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 ì‹¬í™” í€´ì¦ˆ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. ê°•ì¡°ì–´ ì²˜ë¦¬í•˜ëŠ” ê°ì„± ë¶„ì„ í•¨ìˆ˜ â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ê°•ì¡°ì–´(ë§¤ìš°, ì •ë§, ë„ˆë¬´)ê°€ ìˆìœ¼ë©´ ì ìˆ˜ë¥¼ 1.5ë°° í•˜ëŠ” ê°ì„± ë¶„ì„ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "sentiment_dict = {'ì¢‹ë‹¤': 1, 'ë‚˜ì˜ë‹¤': -1}\n",
    "texts = ['ì¢‹ë‹¤', 'ë§¤ìš° ì¢‹ë‹¤', 'ì •ë§ ë‚˜ì˜ë‹¤', 'ë‚˜ì˜ë‹¤']\n",
    "```\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼**: `[1, 1.5, -1.5, -1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {'ì¢‹ë‹¤': 1, 'ë‚˜ì˜ë‹¤': -1}\n",
    "texts = ['ì¢‹ë‹¤', 'ë§¤ìš° ì¢‹ë‹¤', 'ì •ë§ ë‚˜ì˜ë‹¤', 'ë‚˜ì˜ë‹¤']\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. ì œí’ˆë³„ ê¸ì •ë¥  ê³„ì‚° ë° ìˆœìœ„ â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ì œí’ˆë³„ ê¸ì •ë¥ (ê¸ì • ë¦¬ë·° / ì „ì²´ ë¦¬ë·° * 100)ì„ ê³„ì‚°í•˜ê³  ë†’ì€ ìˆœì„œë¡œ ì •ë ¬í•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "product_reviews = pd.DataFrame({\n",
    "    'product': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
    "    'sentiment': ['ê¸ì •', 'ê¸ì •', 'ë¶€ì •', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •', 'ê¸ì •', 'ê¸ì •', 'ê¸ì •', 'ì¤‘ë¦½']\n",
    "})\n",
    "```\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼**: ì œí’ˆë³„ ê¸ì •ë¥ (%) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_reviews = pd.DataFrame({\n",
    "    'product': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
    "    'sentiment': ['ê¸ì •', 'ê¸ì •', 'ë¶€ì •', 'ê¸ì •', 'ë¶€ì •', 'ë¶€ì •', 'ê¸ì •', 'ê¸ì •', 'ê¸ì •', 'ì¤‘ë¦½']\n",
    "})\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. ëˆ„ì  ë§‰ëŒ€ ì°¨íŠ¸ë¡œ ê°ì„± ë¶„í¬ ì‹œê°í™” â­â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: Q7ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œí’ˆë³„ ê°ì„± ë¶„í¬ ëˆ„ì  ë§‰ëŒ€ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ì„¸ìš”.\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­**:\n",
    "- ê¸ì •=ì´ˆë¡, ë¶€ì •=ë¹¨ê°•, ì¤‘ë¦½=íšŒìƒ‰\n",
    "- ë¹„ìœ¨(%) ê¸°ì¤€ìœ¼ë¡œ í‘œì‹œ\n",
    "- barmode='stack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7ì˜ ë°ì´í„° ì‚¬ìš©\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. ì‹œê³„ì—´ ê°ì„± íŠ¸ë Œë“œ ë¶„ì„ â­â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ë‹¤ìŒ ì¼ë³„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ì ìˆ˜ ì¶”ì´ ë¼ì¸ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ì„¸ìš”. 3ì¼ ì´ë™í‰ê· ë„ í•¨ê»˜ í‘œì‹œí•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "daily_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'sentiment_score': [0.3, 0.5, 0.2, -0.1, -0.3, 0.1, 0.4, 0.6, 0.3, 0.5]\n",
    "})\n",
    "```\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­**:\n",
    "- ì¼ë³„ ì ìˆ˜: íŒŒë€ìƒ‰ ì„  + ë§ˆì»¤\n",
    "- 3ì¼ ì´ë™í‰ê· : ë¹¨ê°„ìƒ‰ êµµì€ ì„ \n",
    "- y=0 ê¸°ì¤€ì„  í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'sentiment_score': [0.3, 0.5, 0.2, -0.1, -0.3, 0.1, 0.4, 0.6, 0.3, 0.5]\n",
    "})\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. ì¢…í•© VOC ë¶„ì„ ëŒ€ì‹œë³´ë“œ â­â­â­â­â­\n",
    "\n",
    "**ë¬¸ì œ**: ë‹¤ìŒ VOC ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¢…í•© ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“œì„¸ìš”.\n",
    "\n",
    "```python\n",
    "voc_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=20).repeat(3),\n",
    "    'category': ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ë°°ì†¡'] * 20,\n",
    "    'review': ['ì¢‹ì•„ìš” ë§Œì¡±í•©ë‹ˆë‹¤', 'ë³„ë¡œì˜ˆìš”', 'ë¹¨ë¼ìš”'] * 20,\n",
    "    'sentiment_score': np.random.uniform(-1, 1, 60)\n",
    "})\n",
    "```\n",
    "\n",
    "**ë¶„ì„ í•­ëª©**:\n",
    "1. ì „ì²´ ê°ì„± ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)\n",
    "2. ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜ (ë§‰ëŒ€ ì°¨íŠ¸)\n",
    "3. ì¼ë³„ ê°ì„± ì¶”ì´ (ë¼ì¸ ì°¨íŠ¸)\n",
    "4. ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¹„ìœ¨ (ë§‰ëŒ€ ì°¨íŠ¸)\n",
    "\n",
    "**ìš”êµ¬ì‚¬í•­**:\n",
    "- `make_subplots`ë¡œ 2x2 ë ˆì´ì•„ì›ƒ\n",
    "- ì ì ˆí•œ ìƒ‰ìƒê³¼ ë ˆì´ë¸”\n",
    "- ì¸ì‚¬ì´íŠ¸ 3ê°œ ì´ìƒ ë„ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "voc_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=20).repeat(3),\n",
    "    'category': ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ë°°ì†¡'] * 20,\n",
    "    'review': ['ì¢‹ì•„ìš” ë§Œì¡±í•©ë‹ˆë‹¤', 'ë³„ë¡œì˜ˆìš”', 'ë¹¨ë¼ìš”'] * 20,\n",
    "    'sentiment_score': np.random.uniform(-1, 1, 60)\n",
    "})\n",
    "\n",
    "# ê°ì„± ë¶„ë¥˜ ì¶”ê°€\n",
    "voc_data['sentiment'] = voc_data['sentiment_score'].apply(\n",
    "    lambda x: 'ê¸ì •' if x > 0.1 else ('ë¶€ì •' if x < -0.1 else 'ì¤‘ë¦½')\n",
    ")\n",
    "\n",
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š í•™ìŠµ ì •ë¦¬\n",
    "\n",
    "### Part 1: ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„ í•µì‹¬ ìš”ì•½\n",
    "\n",
    "| ê°œë… | ë„êµ¬/ë°©ë²• | í™œìš© |\n",
    "|------|----------|------|\n",
    "| VADER | `SentimentIntensityAnalyzer` | ì˜ì–´ SNS í…ìŠ¤íŠ¸ ë¶„ì„ |\n",
    "| í•œêµ­ì–´ ì‚¬ì „ | KNU ê°ì„± ì‚¬ì „, ì»¤ìŠ¤í…€ ì‚¬ì „ | í•œê¸€ ë¦¬ë·° ë¶„ì„ |\n",
    "| ê°ì„± ë¶„ë¥˜ | compound ì ìˆ˜ ê¸°ì¤€ | ê¸ì •/ë¶€ì •/ì¤‘ë¦½ |\n",
    "| ê°•ì¡°ì–´ ì²˜ë¦¬ | ì ìˆ˜ ë°°ìˆ˜ ì ìš© | \"ë§¤ìš° ì¢‹ë‹¤\" ì²˜ë¦¬ |\n",
    "| ë¶€ì •ì–´ ì²˜ë¦¬ | ì ìˆ˜ ë°˜ì „ | \"ì•ˆ ì¢‹ë‹¤\" ì²˜ë¦¬ |\n",
    "\n",
    "### Part 2: ê°ì„± ë¶„ì„ ì‹œê°í™” í•µì‹¬ ìš”ì•½\n",
    "\n",
    "| ì‹œê°í™” | Plotly í•¨ìˆ˜ | ì–¸ì œ ì“°ë‚˜? |\n",
    "|--------|------------|----------|\n",
    "| íŒŒì´ ì°¨íŠ¸ | `px.pie()` | ê°ì„± ë¶„í¬ ë¹„ìœ¨ |\n",
    "| ë§‰ëŒ€ ì°¨íŠ¸ | `go.Bar()` | ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ |\n",
    "| ëˆ„ì  ë§‰ëŒ€ | `barmode='stack'` | êµ¬ì„± ë¹„ìœ¨ ë¹„êµ |\n",
    "| ë¼ì¸ ì°¨íŠ¸ | `go.Scatter()` | ì‹œê³„ì—´ íŠ¸ë Œë“œ |\n",
    "| ëŒ€ì‹œë³´ë“œ | `make_subplots()` | ì¢…í•© ë¶„ì„ |\n",
    "\n",
    "### ğŸ’¡ ì‹¤ë¬´ íŒ\n",
    "\n",
    "1. **ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „**: ì¼ë°˜ ì‚¬ì „ + ë„ë©”ì¸ ë‹¨ì–´ë¡œ ì •í™•ë„ í–¥ìƒ\n",
    "2. **ê°•ì¡°ì–´/ë¶€ì •ì–´**: ë°˜ë“œì‹œ ì²˜ë¦¬í•´ì•¼ ì •í™•í•œ ë¶„ì„ ê°€ëŠ¥\n",
    "3. **í‰ì  vs ê°ì„±**: ìƒê´€ê´€ê³„ í™•ì¸í•˜ì—¬ ë¶„ì„ ì‹ ë¢°ë„ ê²€ì¦\n",
    "4. **ì‹œê³„ì—´ ë¶„ì„**: ì´ë™í‰ê· ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°, íŠ¸ë Œë“œ íŒŒì•…\n",
    "5. **VOC ëŒ€ì‹œë³´ë“œ**: í•œëˆˆì— í˜„í™© íŒŒì•…, ê°œì„ ì  ë„ì¶œ\n",
    "6. **ML ë³‘í–‰**: ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° ë¶„ì„, MLë¡œ ì •í™•ë„ ê°œì„ \n",
    "7. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ë¶€ì • ê¸‰ì¦ ì‹œ ì•Œë¦¼ ì„¤ì • ê¶Œì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
