{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day11_1: í…ìŠ¤íŠ¸ ë¶„ì„ ê¸°ë²• (Text Analysis Techniques) - ì •ë‹µ\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "**Part 1: ê¸°ì´ˆ**\n",
        "1. ë‹¨ì–´ ë¹ˆë„(Counter)ë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘í•˜ê¸°\n",
        "2. n-gram ê°œë… ì´í•´í•˜ê³  í™œìš©í•˜ê¸°\n",
        "3. CountVectorizerë¡œ ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ ë§Œë“¤ê¸°\n",
        "4. TF-IDF ê°œë… ì´í•´í•˜ê¸°\n",
        "5. TfidfVectorizerë¡œ í‚¤ì›Œë“œ ì¤‘ìš”ë„ ì¶”ì¶œí•˜ê¸°\n",
        "\n",
        "**Part 2: ì‹¬í™”**\n",
        "1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë¬¸ì„œ ë¹„êµí•˜ê¸°\n",
        "2. ìì¹´ë“œ ìœ ì‚¬ë„ ì´í•´í•˜ê¸°\n",
        "3. ì¤‘ë³µ ë¬¸ì„œ íƒì§€í•˜ê¸°\n",
        "4. ê°„ë‹¨í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ì‹¤ìŠµ í€´ì¦ˆ ì •ë‹µ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚° â­\n",
        "\n",
        "**ë¬¸ì œ**: ë‹¤ìŒ ë¦¬ë·° í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ì™€ ê·¸ ë¹ˆë„ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "review = \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆì´ ì¢‹ì•„ìš” ë°°ì†¡ë„ ë¹ ë¥´ê³  ì„œë¹„ìŠ¤ë„ ì¢‹ì•„ìš”\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `('ì¢‹ì•„ìš”', 2)` ë˜ëŠ” `('ë¹ ë¥´ê³ ', 2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q1 ì •ë‹µ\n",
        "from collections import Counter\n",
        "\n",
        "review = \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆì´ ì¢‹ì•„ìš” ë°°ì†¡ë„ ë¹ ë¥´ê³  ì„œë¹„ìŠ¤ë„ ì¢‹ì•„ìš”\"\n",
        "\n",
        "# 1. ë‹¨ì–´ ë¶„ë¦¬\n",
        "words = review.split()\n",
        "\n",
        "# 2. ë¹ˆë„ ê³„ì‚°\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# 3. ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´\n",
        "most_common_word = word_counts.most_common(1)[0]\n",
        "\n",
        "print(f\"ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´: {most_common_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "assert word_counts.most_common(1)[0][1] == 2, \"ìµœë‹¤ ë¹ˆë„ê°€ 2ì—¬ì•¼ í•©ë‹ˆë‹¤\"\n",
        "assert most_common_word[0] in ['ì¢‹ì•„ìš”', 'ë¹ ë¥´ê³ '], \"ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ëŠ” 'ì¢‹ì•„ìš”' ë˜ëŠ” 'ë¹ ë¥´ê³ 'ì—¬ì•¼ í•©ë‹ˆë‹¤\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "`split()`ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•˜ê³  `Counter`ë¡œ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- `Counter`: ìš”ì†Œ ë¹ˆë„ë¥¼ ìë™ìœ¼ë¡œ ì„¸ëŠ” ë”•ì…”ë„ˆë¦¬ ì„œë¸Œí´ë˜ìŠ¤\n",
        "- `most_common(n)`: ìƒìœ„ nê°œ ë¹ˆì¶œ ìš”ì†Œ ë°˜í™˜\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# ë”•ì…”ë„ˆë¦¬ë¡œ ì§ì ‘ êµ¬í˜„\n",
        "word_counts = {}\n",
        "for word in words:\n",
        "    word_counts[word] = word_counts.get(word, 0) + 1\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `most_common()` ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ íŠœí”Œì„ì„ ìŠìŒ\n",
        "- ì¸ë±ì‹± ì—†ì´ `most_common(1)` ìì²´ë¥¼ ì¶œë ¥\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ê³ ê° ë¦¬ë·°ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ë•Œ Counterë¥¼ í™œìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q2. bigram ì¶”ì¶œ â­\n",
        "\n",
        "**ë¬¸ì œ**: ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  bigram(ì—°ì†ëœ 2ë‹¨ì–´)ì„ ì¶”ì¶œí•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "text = \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„ ì…ë¬¸\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `['íŒŒì´ì¬ ë°ì´í„°', 'ë°ì´í„° ë¶„ì„', 'ë¶„ì„ ì…ë¬¸']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q2 ì •ë‹µ\n",
        "text = \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„ ì…ë¬¸\"\n",
        "\n",
        "# 1. ë‹¨ì–´ ë¶„ë¦¬\n",
        "words = text.split()\n",
        "\n",
        "# 2. bigram ìƒì„±\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):  # n-1ë²ˆ ë°˜ë³µ (n=2)\n",
        "    bigram = words[i] + ' ' + words[i + 1]\n",
        "    bigrams.append(bigram)\n",
        "\n",
        "print(f\"Bigrams: {bigrams}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "expected = ['íŒŒì´ì¬ ë°ì´í„°', 'ë°ì´í„° ë¶„ì„', 'ë¶„ì„ ì…ë¬¸']\n",
        "assert bigrams == expected, f\"Expected {expected}, got {bigrams}\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "ì¸ë±ìŠ¤ ië¶€í„° i+1ê¹Œì§€ ìŠ¬ë¼ì´ë”©í•˜ë©° ì—°ì†ëœ 2ë‹¨ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- n-gram: ì—°ì†ëœ nê°œ ìš”ì†Œì˜ ì‹œí€€ìŠ¤\n",
        "- ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: ê³ ì • í¬ê¸° ì°½ì„ ì´ë™ì‹œí‚¤ë©° ì²˜ë¦¬\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì‚¬ìš©\n",
        "bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
        "\n",
        "# zip í™œìš©\n",
        "bigrams = [f\"{w1} {w2}\" for w1, w2 in zip(words[:-1], words[1:])]\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `range(len(words))`ë¡œ í•˜ë©´ ë§ˆì§€ë§‰ì— ì¸ë±ìŠ¤ ì´ˆê³¼ ì—ëŸ¬\n",
        "- ë‹¨ì–´ ì‚¬ì´ ê³µë°±ì„ ìŠìŒ\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "bigramì€ \"ë°°ì†¡ì´ ë¹ ë¥´ë‹¤\", \"ë§›ìˆê³  ì‹ ì„ \"ì²˜ëŸ¼ í‘œí˜„ íŒ¨í„´ì„ ì°¾ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q3. CountVectorizer ê¸°ì´ˆ â­â­\n",
        "\n",
        "**ë¬¸ì œ**: CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ì„ ë§Œë“¤ê³ , 'ë°°ì†¡'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ê° ë¬¸ì„œì— ëª‡ ë²ˆ ë“±ì¥í•˜ëŠ”ì§€ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"ë°°ì†¡ì´ ë¹¨ë¼ìš” ë°°ì†¡ ìµœê³ \",\n",
        "    \"ìƒí’ˆì´ ì¢‹ì•„ìš”\",\n",
        "    \"ë°°ì†¡ë„ ë¹ ë¥´ê³  ìƒí’ˆë„ ì¢‹ì•„ìš”\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q3 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "documents = [\n",
        "    \"ë°°ì†¡ì´ ë¹¨ë¼ìš” ë°°ì†¡ ìµœê³ \",\n",
        "    \"ìƒí’ˆì´ ì¢‹ì•„ìš”\",\n",
        "    \"ë°°ì†¡ë„ ë¹ ë¥´ê³  ìƒí’ˆë„ ì¢‹ì•„ìš”\"\n",
        "]\n",
        "\n",
        "# 1. CountVectorizer ìƒì„± ë° ë³€í™˜\n",
        "vectorizer = CountVectorizer()\n",
        "count_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# 2. ë‹¨ì–´ ëª©ë¡ í™•ì¸\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"ë‹¨ì–´ ëª©ë¡:\", feature_names)\n",
        "\n",
        "# 3. 'ë°°ì†¡' ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "if 'ë°°ì†¡' in feature_names:\n",
        "    idx = list(feature_names).index('ë°°ì†¡')\n",
        "    # ê° ë¬¸ì„œì—ì„œ 'ë°°ì†¡' ë¹ˆë„\n",
        "    for i, doc in enumerate(documents):\n",
        "        count = count_matrix.toarray()[i][idx]\n",
        "        print(f\"ë¬¸ì„œ{i+1}: 'ë°°ì†¡' = {count}íšŒ\")\n",
        "else:\n",
        "    print(\"'ë°°ì†¡' ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤ (í˜•íƒœì†Œê°€ ë¶„ë¦¬ë  ìˆ˜ ìˆìŒ)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "matrix_array = count_matrix.toarray()\n",
        "assert matrix_array.shape[0] == 3, \"ë¬¸ì„œ ìˆ˜ê°€ 3ì´ì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "print(\"\\në¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬:\")\n",
        "df = pd.DataFrame(matrix_array, columns=feature_names)\n",
        "print(df)\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "CountVectorizerë¡œ ë¬¸ì„œë¥¼ ë²¡í„°í™”í•œ í›„, íŠ¹ì • ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ ë¹ˆë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- `fit_transform()`: í•™ìŠµê³¼ ë³€í™˜ì„ ë™ì‹œì— ìˆ˜í–‰\n",
        "- `get_feature_names_out()`: ë‹¨ì–´ ëª©ë¡ ë°˜í™˜\n",
        "- `toarray()`: í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ ë°°ì—´ë¡œ ë³€í™˜\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# DataFrameìœ¼ë¡œ ì§ì ‘ ì¡°íšŒ\n",
        "df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
        "print(df['ë°°ì†¡'] if 'ë°°ì†¡' in df.columns else \"ì—†ìŒ\")\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `toarray()` ì—†ì´ í¬ì†Œ í–‰ë ¬ ì ‘ê·¼ ì‹œë„\n",
        "- í•œê¸€ í˜•íƒœì†Œê°€ ë¶„ë¦¬ë˜ì–´ ì›í•˜ëŠ” ë‹¨ì–´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "í•œê¸€ì€ CountVectorizer ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì •í™•í•œ ë¶„ì„ì´ ì–´ë ¤ìš°ë¯€ë¡œ, KoNLPyë¡œ ì „ì²˜ë¦¬ í›„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q4. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ â­â­\n",
        "\n",
        "**ë¬¸ì œ**: TfidfVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ TF-IDF ê°’ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ë¥¼ ì°¾ìœ¼ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ\",\n",
        "    \"í˜„ëŒ€ìë™ì°¨ ì „ê¸°ì°¨ íŒë§¤\",\n",
        "    \"LGì „ì ê°€ì „ ìˆ˜ì¶œ\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q4 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ\",\n",
        "    \"í˜„ëŒ€ìë™ì°¨ ì „ê¸°ì°¨ íŒë§¤\",\n",
        "    \"LGì „ì ê°€ì „ ìˆ˜ì¶œ\"\n",
        "]\n",
        "\n",
        "# 1. TF-IDF ë²¡í„°í™”\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 2. ë‹¨ì–´ ëª©ë¡\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "# 3. ì²« ë²ˆì§¸ ë¬¸ì„œì˜ TF-IDF ê°’\n",
        "first_doc_tfidf = tfidf_matrix[0].toarray().flatten()\n",
        "\n",
        "# 4. ê°€ì¥ ë†’ì€ TF-IDF ë‹¨ì–´ ì°¾ê¸°\n",
        "max_idx = first_doc_tfidf.argmax()\n",
        "max_word = feature_names[max_idx]\n",
        "max_score = first_doc_tfidf[max_idx]\n",
        "\n",
        "print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ìµœê³  TF-IDF ë‹¨ì–´: '{max_word}' (ì ìˆ˜: {max_score:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# 'ë°˜ë„ì²´'ì™€ 'ì‚¼ì„±ì „ì'ëŠ” ì²« ë²ˆì§¸ ë¬¸ì„œì—ë§Œ ë“±ì¥í•˜ë¯€ë¡œ ë†’ì€ TF-IDF\n",
        "assert max_word in ['ë°˜ë„ì²´', 'ì‚¼ì„±ì „ì'], f\"Expected 'ë°˜ë„ì²´' or 'ì‚¼ì„±ì „ì', got '{max_word}'\"\n",
        "print(\"\\nëª¨ë“  ë‹¨ì–´ì˜ TF-IDF:\")\n",
        "for word, score in sorted(zip(feature_names, first_doc_tfidf), key=lambda x: -x[1]):\n",
        "    if score > 0:\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "TfidfVectorizerë¡œ ë²¡í„°í™” í›„, argmax()ë¡œ ìµœëŒ€ ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- TF-IDF: ë¬¸ì„œ ë‚´ ë¹ˆë„(TF) x í¬ì†Œì„±(IDF)\n",
        "- íŠ¹ì • ë¬¸ì„œì—ë§Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ = ë†’ì€ IDF = ë†’ì€ TF-IDF\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# pandasë¡œ ì •ë ¬\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfidf_matrix[0].toarray(), columns=feature_names)\n",
        "top_word = df.iloc[0].idxmax()\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `argmax()`ê°€ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì„ ìŠìŒ\n",
        "- í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ ë°°ì—´ë¡œ ë³€í™˜í•˜ì§€ ì•ŠìŒ\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "TF-IDF ìƒìœ„ í‚¤ì›Œë“œëŠ” ë¬¸ì„œ ìë™ íƒœê¹…, í‚¤ì›Œë“œ ê´‘ê³  ìµœì í™”ì— í™œìš©ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q5. ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚° â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ë‘ ë¬¸ì„œì˜ ìì¹´ë“œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "doc1 = \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„\"\n",
        "doc2 = \"íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„\"\n",
        "```\n",
        "\n",
        "**íŒíŠ¸**: êµì§‘í•© / í•©ì§‘í•©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q5 ì •ë‹µ\n",
        "doc1 = \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„\"\n",
        "doc2 = \"íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„\"\n",
        "\n",
        "# 1. ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë³€í™˜\n",
        "set1 = set(doc1.split())\n",
        "set2 = set(doc2.split())\n",
        "\n",
        "print(f\"ë¬¸ì„œ1 ë‹¨ì–´: {set1}\")\n",
        "print(f\"ë¬¸ì„œ2 ë‹¨ì–´: {set2}\")\n",
        "\n",
        "# 2. êµì§‘í•©ê³¼ í•©ì§‘í•©\n",
        "intersection = set1 & set2  # ë˜ëŠ” set1.intersection(set2)\n",
        "union = set1 | set2         # ë˜ëŠ” set1.union(set2)\n",
        "\n",
        "print(f\"\\nêµì§‘í•©: {intersection}\")\n",
        "print(f\"í•©ì§‘í•©: {union}\")\n",
        "\n",
        "# 3. ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "jaccard = len(intersection) / len(union)\n",
        "\n",
        "print(f\"\\nìì¹´ë“œ ìœ ì‚¬ë„: {jaccard:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# êµì§‘í•©: {'íŒŒì´ì¬', 'ë¶„ì„'} = 2ê°œ\n",
        "# í•©ì§‘í•©: {'íŒŒì´ì¬', 'ë°ì´í„°', 'ë¶„ì„', 'ë¨¸ì‹ ëŸ¬ë‹'} = 4ê°œ\n",
        "# ìì¹´ë“œ = 2/4 = 0.5\n",
        "expected_jaccard = 2 / 4\n",
        "assert abs(jaccard - expected_jaccard) < 0.001, f\"Expected {expected_jaccard}, got {jaccard}\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "ë¬¸ìì—´ì„ ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì§‘í•© ì—°ì‚°ìœ¼ë¡œ êµì§‘í•©ê³¼ í•©ì§‘í•©ì„ êµ¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- ìì¹´ë“œ ìœ ì‚¬ë„ = |A intersection B| / |A union B|\n",
        "- ì§‘í•© ì—°ì‚°: `&` (êµì§‘í•©), `|` (í•©ì§‘í•©)\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# ë©”ì„œë“œ ì‚¬ìš©\n",
        "intersection = set1.intersection(set2)\n",
        "union = set1.union(set2)\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- ë¦¬ìŠ¤íŠ¸ë¡œ ê³„ì‚°í•˜ë©´ ì¤‘ë³µì´ í¬í•¨ë¨ (ì§‘í•© ë³€í™˜ í•„ìˆ˜)\n",
        "- êµì§‘í•©ê³¼ í•©ì§‘í•©ì„ í˜¼ë™\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ìì¹´ë“œ ìœ ì‚¬ë„ëŠ” í•´ì‹œíƒœê·¸ ë¹„êµ, ì§§ì€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •ì— ì í•©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 ì‹¬í™” í€´ì¦ˆ ì •ë‹µ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸° â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ê¸°ì¤€ ë¬¸ì„œì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ê³  ìœ ì‚¬ë„ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹\",  # ê¸°ì¤€ ë¬¸ì„œ (ì¸ë±ìŠ¤ 0)\n",
        "    \"ìë°” ì›¹ ê°œë°œ\",\n",
        "    \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„ ë¨¸ì‹ ëŸ¬ë‹\",\n",
        "    \"ìë°”ìŠ¤í¬ë¦½íŠ¸ í”„ë¡ íŠ¸ì—”ë“œ\"\n",
        "]\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: ë¬¸ì„œ ì¸ë±ìŠ¤ì™€ ìœ ì‚¬ë„ ê°’"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q6 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹\",\n",
        "    \"ìë°” ì›¹ ê°œë°œ\",\n",
        "    \"íŒŒì´ì¬ ë°ì´í„° ë¶„ì„ ë¨¸ì‹ ëŸ¬ë‹\",\n",
        "    \"ìë°”ìŠ¤í¬ë¦½íŠ¸ í”„ë¡ íŠ¸ì—”ë“œ\"\n",
        "]\n",
        "\n",
        "# 1. TF-IDF ë²¡í„°í™”\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# 3. ê¸°ì¤€ ë¬¸ì„œ(ì¸ë±ìŠ¤ 0)ì™€ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì˜ ìœ ì‚¬ë„\n",
        "target_idx = 0\n",
        "similarities = list(enumerate(cosine_sim[target_idx]))\n",
        "\n",
        "# 4. ìê¸° ìì‹  ì œì™¸í•˜ê³  ì •ë ¬\n",
        "similarities = [(i, sim) for i, sim in similarities if i != target_idx]\n",
        "similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 5. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ\n",
        "most_similar_idx, most_similar_score = similarities[0]\n",
        "\n",
        "print(f\"ê¸°ì¤€ ë¬¸ì„œ: '{documents[target_idx]}'\")\n",
        "print(f\"\\nê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ:\")\n",
        "print(f\"  ì¸ë±ìŠ¤: {most_similar_idx}\")\n",
        "print(f\"  ë‚´ìš©: '{documents[most_similar_idx]}'\")\n",
        "print(f\"  ìœ ì‚¬ë„: {most_similar_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# ë¬¸ì„œ0ê³¼ ë¬¸ì„œ2ëŠ” 'íŒŒì´ì¬', 'ë¨¸ì‹ ëŸ¬ë‹'ì„ ê³µìœ  -> ê°€ì¥ ìœ ì‚¬\n",
        "assert most_similar_idx == 2, f\"Expected index 2, got {most_similar_idx}\"\n",
        "assert most_similar_score > 0.3, \"ìœ ì‚¬ë„ê°€ 0.3 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "TF-IDFë¡œ ë²¡í„°í™” -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° -> ìê¸° ìì‹  ì œì™¸í•˜ê³  ìµœëŒ€ê°’ ì°¾ê¸°\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- `cosine_similarity()`: ëª¨ë“  ë¬¸ì„œ ìŒì˜ ìœ ì‚¬ë„ í–‰ë ¬ ë°˜í™˜\n",
        "- ëŒ€ê°ì„ ì€ ìê¸° ìì‹ (1.0)ì´ë¯€ë¡œ ì œì™¸\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# numpy argsort ì‚¬ìš©\n",
        "import numpy as np\n",
        "sim_scores = cosine_sim[0].copy()\n",
        "sim_scores[0] = -1  # ìê¸° ìì‹  ì œì™¸\n",
        "most_similar = np.argmax(sim_scores)\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- ìê¸° ìì‹ ì„ ì œì™¸í•˜ì§€ ì•Šìœ¼ë©´ í•­ìƒ ìì‹ ì´ ê°€ì¥ ìœ ì‚¬\n",
        "- ì¸ë±ìŠ¤ì™€ ìœ ì‚¬ë„ ê°’ì„ í˜¼ë™\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ \"ì´ ê¸°ì‚¬ë¥¼ ì½ì€ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•œ ê¸°ì‚¬\"ë¥¼ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q7. n-gram + TF-IDF ì¡°í•© â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: unigramê³¼ bigramì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” TfidfVectorizerë¥¼ ë§Œë“¤ê³ , ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ TF-IDF ê°’ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ 3ê°œ featureë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆ ì¢‹ì•„ìš”\",\n",
        "    \"ìƒí’ˆ í’ˆì§ˆì€ ì¢‹ì€ë° ë°°ì†¡ì´ ëŠë ¤ìš”\",\n",
        "    \"ê°€ê²© ëŒ€ë¹„ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q7 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆ ì¢‹ì•„ìš”\",\n",
        "    \"ìƒí’ˆ í’ˆì§ˆì€ ì¢‹ì€ë° ë°°ì†¡ì´ ëŠë ¤ìš”\",\n",
        "    \"ê°€ê²© ëŒ€ë¹„ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”\"\n",
        "]\n",
        "\n",
        "# 1. unigram + bigram TF-IDF ë²¡í„°í™”\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 2))  # (1, 2) = unigram + bigram\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 2. feature ì´ë¦„\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "# 3. ì²« ë²ˆì§¸ ë¬¸ì„œì˜ TF-IDF ê°’\n",
        "first_doc_tfidf = tfidf_matrix[0].toarray().flatten()\n",
        "\n",
        "# 4. ë‹¨ì–´ì™€ ì ìˆ˜ë¥¼ ë¬¶ì–´ì„œ ì •ë ¬\n",
        "word_scores = list(zip(feature_names, first_doc_tfidf))\n",
        "word_scores_sorted = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 5. ìƒìœ„ 3ê°œ ì¶œë ¥\n",
        "print(\"ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ìƒìœ„ 3ê°œ feature:\")\n",
        "for word, score in word_scores_sorted[:3]:\n",
        "    print(f\"  '{word}': {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# ìƒìœ„ 3ê°œ ëª¨ë‘ TF-IDF > 0 ì´ì–´ì•¼ í•¨\n",
        "top_3 = word_scores_sorted[:3]\n",
        "assert all(score > 0 for _, score in top_3), \"ìƒìœ„ 3ê°œ TF-IDFëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤\"\n",
        "\n",
        "# bigramë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "has_bigram = any(' ' in word for word, _ in word_scores_sorted[:10] if word)\n",
        "print(f\"\\nì „ì²´ feature ìˆ˜: {len(feature_names)}\")\n",
        "print(f\"bigram í¬í•¨ ì—¬ë¶€: {has_bigram}\")\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "`ngram_range=(1, 2)` ì˜µì…˜ìœ¼ë¡œ unigramê³¼ bigramì„ ëª¨ë‘ ìƒì„±í•˜ê³ , ì •ë ¬í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- `ngram_range`: (min_n, max_n) í˜•íƒœë¡œ n-gram ë²”ìœ„ ì§€ì •\n",
        "- bigramì€ \"ë°°ì†¡ì´ ë¹ ë¥´ê³ \"ì²˜ëŸ¼ ì—°ì† 2ë‹¨ì–´\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# DataFrameìœ¼ë¡œ ì •ë ¬\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'word': feature_names, 'tfidf': first_doc_tfidf})\n",
        "top_3 = df.nlargest(3, 'tfidf')\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `ngram_range`ë¥¼ ë¦¬ìŠ¤íŠ¸ `[1, 2]`ë¡œ ì“°ë©´ ì—ëŸ¬ (íŠœí”Œì´ì–´ì•¼ í•¨)\n",
        "- `(2, 2)`ë¡œ í•˜ë©´ bigramë§Œ ìƒì„±\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ë¦¬ë·° ë¶„ì„ì—ì„œ bigramì„ í¬í•¨í•˜ë©´ \"ë°°ì†¡ ëŠë¦¼\", \"í’ˆì§ˆ ì¢‹ìŒ\" ê°™ì€ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ ì¡ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q8. ì¤‘ë³µ ë¬¸ì„œ íƒì§€ â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ 0.6 ì´ìƒì¸ ë¬¸ì„œ ìŒì„ ëª¨ë‘ ì°¾ì•„ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ ì¦ê°€\",\n",
        "    \"í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ íŒë§¤ í˜¸ì¡°\",\n",
        "    \"ì‚¼ì„± ë°˜ë„ì²´ ìˆ˜ì¶œ ì‚¬ìƒ ìµœëŒ€\",  # 0ë²ˆê³¼ ìœ ì‚¬\n",
        "    \"LGì „ì ê°€ì „ ì‚¬ì—… ì„±ì¥\",\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ ì‹ ê¸°ë¡\"   # 0ë²ˆê³¼ ìœ ì‚¬\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q8 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ ì¦ê°€\",\n",
        "    \"í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ íŒë§¤ í˜¸ì¡°\",\n",
        "    \"ì‚¼ì„± ë°˜ë„ì²´ ìˆ˜ì¶œ ì‚¬ìƒ ìµœëŒ€\",\n",
        "    \"LGì „ì ê°€ì „ ì‚¬ì—… ì„±ì¥\",\n",
        "    \"ì‚¼ì„±ì „ì ë°˜ë„ì²´ ìˆ˜ì¶œ ì‹ ê¸°ë¡\"\n",
        "]\n",
        "\n",
        "# 1. TF-IDF ë²¡í„°í™”\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# 3. ì„ê³„ê°’ ì´ìƒì¸ ìŒ ì°¾ê¸°\n",
        "threshold = 0.6\n",
        "duplicates = []\n",
        "\n",
        "n = len(documents)\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):  # ìƒì‚¼ê° í–‰ë ¬ë§Œ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)\n",
        "        if cosine_sim[i][j] >= threshold:\n",
        "            duplicates.append({\n",
        "                'idx1': i,\n",
        "                'idx2': j,\n",
        "                'doc1': documents[i],\n",
        "                'doc2': documents[j],\n",
        "                'similarity': round(cosine_sim[i][j], 4)\n",
        "            })\n",
        "\n",
        "# 4. ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"=== ìœ ì‚¬ë„ {threshold} ì´ìƒì¸ ë¬¸ì„œ ìŒ ===\")\n",
        "for dup in duplicates:\n",
        "    print(f\"\\nìœ ì‚¬ë„: {dup['similarity']}\")\n",
        "    print(f\"  ë¬¸ì„œ{dup['idx1']}: {dup['doc1']}\")\n",
        "    print(f\"  ë¬¸ì„œ{dup['idx2']}: {dup['doc2']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# ë¬¸ì„œ 0, 2, 4ê°€ ì„œë¡œ ìœ ì‚¬í•´ì•¼ í•¨\n",
        "expected_pairs = {(0, 2), (0, 4), (2, 4)}\n",
        "found_pairs = {(d['idx1'], d['idx2']) for d in duplicates}\n",
        "\n",
        "print(f\"\\në°œê²¬ëœ ìŒ: {found_pairs}\")\n",
        "assert len(duplicates) >= 2, \"ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ ì‚¬ ìŒì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "ëª¨ë“  ë¬¸ì„œ ìŒì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì„ê³„ê°’ ì´ìƒì¸ ìŒë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- ìƒì‚¼ê° í–‰ë ¬: i < jì¸ ìŒë§Œ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ë°©ì§€\n",
        "- ì„ê³„ê°’(threshold): ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì¡°ì •\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# numpy where ì‚¬ìš©\n",
        "import numpy as np\n",
        "pairs = np.where((cosine_sim >= threshold) & (np.triu(np.ones_like(cosine_sim), k=1).astype(bool)))\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- ì „ì²´ í–‰ë ¬ì„ ìˆœíšŒí•˜ë©´ (i, j)ì™€ (j, i)ê°€ ì¤‘ë³µ\n",
        "- ëŒ€ê°ì„ (ìê¸° ìì‹ )ì„ ì œì™¸í•˜ì§€ ì•ŠìŒ\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ë‰´ìŠ¤ ì¤‘ë³µ ê¸°ì‚¬ íƒì§€, í‘œì ˆ ê²€ì‚¬ì— í™œìš©ë©ë‹ˆë‹¤. ì„ê³„ê°’ì€ ë³´í†µ 0.7~0.9 ì‚¬ì´ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q9. ê²€ìƒ‰ ì¿¼ë¦¬ ë§¤ì¹­ â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ê²€ìƒ‰ ì¿¼ë¦¬ \"ë°ì´í„° ë¶„ì„\"ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ì€ ë¬¸ì„œ 2ê°œë¥¼ ì°¾ì•„ ìœ ì‚¬ë„ì™€ í•¨ê»˜ ì¶œë ¥í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"íŒŒì´ì¬ ê¸°ì´ˆ ë¬¸ë²• ê°•ì¢Œ\",\n",
        "    \"ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ íŒë‹¤ìŠ¤\",\n",
        "    \"ì›¹ ê°œë°œ ì…ë¬¸ ê°€ì´ë“œ\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ì „ì²˜ë¦¬\",\n",
        "    \"SQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆ\"\n",
        "]\n",
        "\n",
        "query = \"ë°ì´í„° ë¶„ì„\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q9 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"íŒŒì´ì¬ ê¸°ì´ˆ ë¬¸ë²• ê°•ì¢Œ\",\n",
        "    \"ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ íŒë‹¤ìŠ¤\",\n",
        "    \"ì›¹ ê°œë°œ ì…ë¬¸ ê°€ì´ë“œ\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ì „ì²˜ë¦¬\",\n",
        "    \"SQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆ\"\n",
        "]\n",
        "\n",
        "query = \"ë°ì´í„° ë¶„ì„\"\n",
        "\n",
        "# 1. ë¬¸ì„œë¡œ TF-IDF í•™ìŠµ\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 2. ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê¸°ì¡´ vocabulary ì‚¬ìš©)\n",
        "query_vector = tfidf.transform([query])\n",
        "\n",
        "# 3. ì¿¼ë¦¬ì™€ ëª¨ë“  ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "# 4. ìƒìœ„ 2ê°œ ë¬¸ì„œ ì°¾ê¸°\n",
        "top_indices = similarities.argsort()[::-1][:2]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 2ê°œ\n",
        "\n",
        "print(f\"ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'\")\n",
        "print(\"\\nê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 2ê°œ):\")\n",
        "for rank, idx in enumerate(top_indices, 1):\n",
        "    print(f\"  {rank}. [{similarities[idx]:.4f}] {documents[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# \"ë°ì´í„° ë¶„ì„\"ì´ í¬í•¨ëœ ë¬¸ì„œ1ì´ 1ìœ„ì—¬ì•¼ í•¨\n",
        "assert top_indices[0] == 1, f\"Expected index 1, got {top_indices[0]}\"\n",
        "assert similarities[top_indices[0]] > similarities[top_indices[1]], \"1ìœ„ê°€ 2ìœ„ë³´ë‹¤ ìœ ì‚¬ë„ ë†’ì•„ì•¼ í•¨\"\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "ë¬¸ì„œë¡œ í•™ìŠµëœ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ë³€í™˜í•˜ê³ , ëª¨ë“  ë¬¸ì„œì™€ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- `fit_transform(documents)`: ë¬¸ì„œë¡œ í•™ìŠµ + ë³€í™˜\n",
        "- `transform([query])`: ê¸°ì¡´ vocabularyë¡œ ì¿¼ë¦¬ ë³€í™˜\n",
        "- ì¿¼ë¦¬ì— ì—†ëŠ” ë‹¨ì–´ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬ë¨\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# numpy argpartition (ëŒ€ìš©ëŸ‰ì— ë” íš¨ìœ¨ì )\n",
        "import numpy as np\n",
        "top_2 = np.argpartition(similarities, -2)[-2:]\n",
        "top_2_sorted = top_2[np.argsort(similarities[top_2])[::-1]]\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- `fit_transform()`ì„ ì¿¼ë¦¬ì—ë„ ì‚¬ìš©í•˜ë©´ vocabularyê°€ ë‹¬ë¼ì§\n",
        "- `argsort()`ê°€ ì˜¤ë¦„ì°¨ìˆœì„ì„ ìŠê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ ì‹œ ì¿¼ë¦¬ í™•ì¥(query expansion)ì„ ì¶”ê°€í•˜ë©´ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Q10. ì¢…í•©: ë¬¸ì„œ ë¶„ë¥˜ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ â­â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ë¶„ì„ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "1. TF-IDFë¡œ ê° ë¬¸ì„œì˜ ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
        "2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸°\n",
        "3. Plotlyë¡œ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ íˆíŠ¸ë§µ ì‹œê°í™”\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ìŠ¤ë§ˆíŠ¸í° ê°¤ëŸ­ì‹œ ì‹ ì œí’ˆ ì¶œì‹œ\",\n",
        "    \"ì• í”Œ ì•„ì´í° ì‹ ì œí’ˆ ë°œí‘œ\",\n",
        "    \"í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ì•„ì´ì˜¤ë‹‰ íŒë§¤ ì¦ê°€\",\n",
        "    \"í…ŒìŠ¬ë¼ ì „ê¸°ì°¨ ëª¨ë¸Y ì¸ê¸°\",\n",
        "    \"ì‚¼ì„± ê°¤ëŸ­ì‹œ í´ë“œ ìŠ¤ë§ˆíŠ¸í° í˜ì‹ \"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q10 ì •ë‹µ\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import plotly.figure_factory as ff\n",
        "import pandas as pd\n",
        "\n",
        "documents = [\n",
        "    \"ì‚¼ì„±ì „ì ìŠ¤ë§ˆíŠ¸í° ê°¤ëŸ­ì‹œ ì‹ ì œí’ˆ ì¶œì‹œ\",\n",
        "    \"ì• í”Œ ì•„ì´í° ì‹ ì œí’ˆ ë°œí‘œ\",\n",
        "    \"í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ì•„ì´ì˜¤ë‹‰ íŒë§¤ ì¦ê°€\",\n",
        "    \"í…ŒìŠ¬ë¼ ì „ê¸°ì°¨ ëª¨ë¸Y ì¸ê¸°\",\n",
        "    \"ì‚¼ì„± ê°¤ëŸ­ì‹œ í´ë“œ ìŠ¤ë§ˆíŠ¸í° í˜ì‹ \"\n",
        "]\n",
        "\n",
        "# === 1. TF-IDFë¡œ ê° ë¬¸ì„œì˜ ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ===\n",
        "print(\"=== 1. ë¬¸ì„œë³„ ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ ===\")\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    # í•´ë‹¹ ë¬¸ì„œì˜ TF-IDF ì ìˆ˜\n",
        "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
        "    \n",
        "    # ìƒìœ„ 2ê°œ ì¸ë±ìŠ¤\n",
        "    top_indices = tfidf_scores.argsort()[::-1][:2]\n",
        "    \n",
        "    # í‚¤ì›Œë“œ ì¶”ì¶œ\n",
        "    top_keywords = [(feature_names[idx], round(tfidf_scores[idx], 3)) for idx in top_indices]\n",
        "    \n",
        "    print(f\"ë¬¸ì„œ{i}: {top_keywords}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ì°¾ê¸° ===\n",
        "print(\"\\n=== 2. ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ===\")\n",
        "\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# ìê¸° ìì‹  ì œì™¸í•˜ê³  ìµœëŒ€ ìœ ì‚¬ë„ ì°¾ê¸°\n",
        "max_sim = 0\n",
        "max_pair = (0, 0)\n",
        "\n",
        "n = len(documents)\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        if cosine_sim[i][j] > max_sim:\n",
        "            max_sim = cosine_sim[i][j]\n",
        "            max_pair = (i, j)\n",
        "\n",
        "print(f\"ìœ ì‚¬ë„: {max_sim:.4f}\")\n",
        "print(f\"ë¬¸ì„œ{max_pair[0]}: {documents[max_pair[0]]}\")\n",
        "print(f\"ë¬¸ì„œ{max_pair[1]}: {documents[max_pair[1]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. Plotly íˆíŠ¸ë§µ ì‹œê°í™” ===\n",
        "print(\"\\n=== 3. ìœ ì‚¬ë„ íˆíŠ¸ë§µ ===\")\n",
        "\n",
        "# ë¬¸ì„œ ë¼ë²¨ (ì§§ê²Œ ì¤„ì„)\n",
        "labels = [f\"ë¬¸ì„œ{i}\" for i in range(len(documents))]\n",
        "\n",
        "# íˆíŠ¸ë§µ ìƒì„±\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=cosine_sim.round(2),\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ íˆíŠ¸ë§µ',\n",
        "    xaxis_title='ë¬¸ì„œ',\n",
        "    yaxis_title='ë¬¸ì„œ',\n",
        "    width=600,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦\n",
        "# ë¬¸ì„œ0ê³¼ ë¬¸ì„œ4 (ì‚¼ì„±, ê°¤ëŸ­ì‹œ, ìŠ¤ë§ˆíŠ¸í°) ë˜ëŠ” ë¬¸ì„œ2ì™€ ë¬¸ì„œ3 (ì „ê¸°ì°¨)ê°€ ìœ ì‚¬í•´ì•¼ í•¨\n",
        "assert max_sim > 0.2, \"ìµœëŒ€ ìœ ì‚¬ë„ê°€ 0.2 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "assert max_pair in [(0, 4), (2, 3)], f\"ì˜ˆìƒ ìŒ: (0, 4) ë˜ëŠ” (2, 3), ì‹¤ì œ: {max_pair}\"\n",
        "print(\"\\nëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ í’€ì´ ì„¤ëª…\n",
        "\n",
        "**ì ‘ê·¼ ë°©ë²•**:\n",
        "1. TF-IDF ë²¡í„°í™”ë¡œ ê° ë¬¸ì„œì˜ ë‹¨ì–´ ì¤‘ìš”ë„ ê³„ì‚°\n",
        "2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ì—ì„œ ìµœëŒ€ê°’ ì°¾ê¸°\n",
        "3. Plotly annotated heatmapìœ¼ë¡œ ì‹œê°í™”\n",
        "\n",
        "**í•µì‹¬ ê°œë…**:\n",
        "- ë¬¸ì„œë³„ í‚¤ì›Œë“œ: TF-IDF ìƒìœ„ ê°’ì˜ ë‹¨ì–´\n",
        "- ìœ ì‚¬ ë¬¸ì„œ: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœëŒ€ ìŒ\n",
        "- íˆíŠ¸ë§µ: í–‰ë ¬ ë°ì´í„°ë¥¼ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„\n",
        "\n",
        "**ëŒ€ì•ˆ ì†”ë£¨ì…˜**:\n",
        "```python\n",
        "# pandas + seaborn ìŠ¤íƒ€ì¼\n",
        "import plotly.express as px\n",
        "df_sim = pd.DataFrame(cosine_sim, index=labels, columns=labels)\n",
        "fig = px.imshow(df_sim, text_auto='.2f', color_continuous_scale='Blues')\n",
        "```\n",
        "\n",
        "**í”í•œ ì‹¤ìˆ˜**:\n",
        "- íˆíŠ¸ë§µ ë¼ë²¨ ìˆœì„œë¥¼ ì˜ëª» ì§€ì •\n",
        "- ìƒ‰ìƒ ìŠ¤ì¼€ì¼ ë°©í–¥ì´ ë°˜ëŒ€ (ë‚®ì€ ê°’ì´ ì§„í•œ ìƒ‰)\n",
        "\n",
        "**ì‹¤ë¬´ íŒ**:\n",
        "ì´ ì‹œìŠ¤í…œì€ ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„°ë§, ë¬¸ì„œ ì¶”ì²œ, ì¤‘ë³µ íƒì§€ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì „ì²˜ë¦¬(í˜•íƒœì†Œ ë¶„ì„, ë¶ˆìš©ì–´ ì œê±°)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š í•™ìŠµ ì •ë¦¬\n",
        "\n",
        "### Part 1: ê¸°ì´ˆ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê°œë… | í•µì‹¬ ë©”ì†Œë“œ/í´ë˜ìŠ¤ | ì‹¤ë¬´ í™œìš© |\n",
        "|------|------------------|----------|\n",
        "| ë‹¨ì–´ ë¹ˆë„ | `Counter`, `most_common()` | í‚¤ì›Œë“œ ìˆœìœ„, ë¶ˆë§Œ ë¶„ì„ |\n",
        "| n-gram | ì§ì ‘ êµ¬í˜„ ë˜ëŠ” `ngram_range` | ì—°ì† í‘œí˜„ íŒ¨í„´ íƒì§€ |\n",
        "| CountVectorizer | `fit_transform()`, `get_feature_names_out()` | ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ ìƒì„± |\n",
        "| TF-IDF | `TfidfVectorizer` | í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ, ì¤‘ìš”ë„ ì¸¡ì • |\n",
        "\n",
        "### Part 2: ì‹¬í™” í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê¸°ë²• | ì‚¬ìš©ë²• | ì–¸ì œ ì“°ë‚˜? |\n",
        "|------|--------|----------|\n",
        "| ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | `cosine_similarity(tfidf_matrix)` | ê¸´ ë¬¸ì„œ ë¹„êµ, ì¶”ì²œ ì‹œìŠ¤í…œ |\n",
        "| ìì¹´ë“œ ìœ ì‚¬ë„ | `len(A&B) / len(A|B)` | ì§§ì€ í…ìŠ¤íŠ¸, íƒœê·¸ ë¹„êµ |\n",
        "| ì¤‘ë³µ íƒì§€ | ìœ ì‚¬ë„ > ì„ê³„ê°’ | í‘œì ˆ ê²€ì‚¬, ìŠ¤íŒ¸ í•„í„°ë§ |\n",
        "| ë¬¸ì„œ ê²€ìƒ‰ | ì¿¼ë¦¬ ë²¡í„° + ìœ ì‚¬ë„ | FAQ ë´‡, ê²€ìƒ‰ ì—”ì§„ |\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ íŒ\n",
        "\n",
        "1. **ì „ì²˜ë¦¬ ì¤‘ìš”**: TF-IDF ì „ì— ë¶ˆìš©ì–´ ì œê±°, í˜•íƒœì†Œ ë¶„ì„ ì ìš©\n",
        "2. **n-gram ë²”ìœ„**: bigramê¹Œì§€ê°€ ì¼ë°˜ì , trigramì€ ë°ì´í„° ë§ì„ ë•Œë§Œ\n",
        "3. **ìœ ì‚¬ë„ ì„ê³„ê°’**: ì¤‘ë³µ íƒì§€ ì‹œ 0.7~0.8, ì¶”ì²œì€ 0.3~0.5\n",
        "4. **í¬ì†Œ í–‰ë ¬**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œëŠ” `sparse matrix` ìœ ì§€ (toarray() ìì œ)\n",
        "5. **í•œê¸€ ë¶„ì„**: KoNLPy í˜•íƒœì†Œ ë¶„ì„ê³¼ ì¡°í•©í•˜ë©´ ì •í™•ë„ í–¥ìƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
