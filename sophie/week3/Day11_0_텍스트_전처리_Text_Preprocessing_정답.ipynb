{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 11-0: 텍스트 전처리 (Text Preprocessing) - 정답\n\n",
        "**학습 시간**: 3시간\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 라이브러리\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK (영어 처리)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# KoNLPy (한글 처리)\n",
        "from konlpy.tag import Okt, Komoran\n",
        "\n",
        "print(\"라이브러리 임포트 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 🎯 실습 퀴즈 정답\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. 기본 텍스트 정제 ⭐\n\n",
        "**문제**: 아래 텍스트를 정제하는 함수를 작성하세요.\n\n",
        "**요구사항**:\n",
        "- 소문자 변환\n",
        "- 앞뒤 공백 제거\n",
        "- 중복 공백을 단일 공백으로 변환\n\n",
        "```python\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n",
        "```\n\n",
        "**기대 결과**: `\"hello world! welcome to nlp.\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q1 정답\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"기본 텍스트 정제 함수\"\"\"\n",
        "    # 1. 소문자 변환\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "    \n",
        "    # 3. 중복 공백을 단일 공백으로\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# 실행\n",
        "result = clean_text(raw_text)\n",
        "print(f\"원본: '{raw_text}'\")\n",
        "print(f\"결과: '{result}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert clean_text(raw_text) == \"hello world! welcome to nlp.\"\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "텍스트 정제는 순서가 중요합니다. 일반적으로 소문자 변환 → strip → 중복 공백 제거 순서로 진행합니다.\n\n",
        "**핵심 개념**:\n",
        "- `lower()`: 문자열을 모두 소문자로 변환\n",
        "- `strip()`: 문자열 양쪽 끝의 공백 제거\n",
        "- `re.sub(r'\\s+', ' ', text)`: 정규표현식으로 1개 이상의 공백(`\\s+`)을 단일 공백으로 치환\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# split + join으로 중복 공백 제거\n",
        "' '.join(text.lower().split())\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ strip() 전에 중복 공백을 먼저 제거하면 앞뒤 공백이 남을 수 있음\n",
        "- ❌ 정규표현식에서 `\\s` 대신 공백 문자 ` `만 사용하면 탭, 줄바꿈이 남음\n\n",
        "**실무 팁**:\n",
        "텍스트 정제 함수를 만들어두면 데이터 전처리 파이프라인에서 재사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q2. 정규표현식 - 특수문자 제거 ⭐\n\n",
        "**문제**: 정규표현식을 사용하여 특수문자를 제거하세요.\n\n",
        "**요구사항**:\n",
        "- 한글, 영어, 숫자, 공백만 남기기\n",
        "- 나머지 문자는 모두 제거\n\n",
        "```python\n",
        "text = \"배송 빠르네요!!! 가격 10,000원 @#$% 추천해요~~\"\n",
        "```\n\n",
        "**기대 결과**: `\"배송 빠르네요 가격 10000원  추천해요\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q2 정답\n",
        "text = \"배송 빠르네요!!! 가격 10,000원 @#$% 추천해요~~\"\n",
        "\n",
        "# 한글(가-힣), 영어(a-zA-Z), 숫자(0-9), 공백(\\s)만 남기기\n",
        "# [^...]: 대괄호 안의 문자를 제외한 모든 문자\n",
        "cleaned = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "print(f\"원본: {text}\")\n",
        "print(f\"결과: {cleaned}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert cleaned == \"배송 빠르네요 가격 10000원  추천해요\"\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "정규표현식의 부정 문자 클래스 `[^...]`를 사용하여 허용할 문자만 정의하고 나머지는 제거합니다.\n\n",
        "**핵심 개념**:\n",
        "- `[^...]`: 대괄호 안의 문자를 **제외한** 모든 문자\n",
        "- `가-힣`: 한글 완성형 문자 범위 (가~힣)\n",
        "- `a-zA-Z`: 영어 대소문자\n",
        "- `0-9`: 숫자\n",
        "- `\\s`: 공백 문자 (스페이스, 탭, 줄바꿈)\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 특정 문자만 제거하고 싶을 때\n",
        "re.sub(r'[!@#$%^&*()~]+', '', text)\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ `[가-힣]` 대신 `[ㄱ-ㅎ가-힣]`로 자음/모음까지 포함하면 불완전한 글자도 남음\n",
        "- ❌ 공백 `\\s`를 빼먹으면 모든 공백이 제거됨\n\n",
        "**실무 팁**:\n",
        "도메인에 따라 허용할 문자를 다르게 설정하세요. 예: 이메일은 `@`, `.` 유지 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q3. 정규표현식 - 해시태그 추출 ⭐⭐\n\n",
        "**문제**: SNS 게시물에서 모든 해시태그를 추출하세요.\n\n",
        "**요구사항**:\n",
        "- `re.findall()` 사용\n",
        "- 해시태그 형식: `#단어`\n\n",
        "```python\n",
        "post = \"오늘 날씨 좋다 #서울 #맑음 #주말나들이 점심은 #맛집 가야지\"\n",
        "```\n\n",
        "**기대 결과**: `['#서울', '#맑음', '#주말나들이', '#맛집']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q3 정답\n",
        "post = \"오늘 날씨 좋다 #서울 #맑음 #주말나들이 점심은 #맛집 가야지\"\n",
        "\n",
        "# 해시태그 패턴: # + 한글/영어/숫자 (1개 이상)\n",
        "hashtags = re.findall(r'#[가-힣a-zA-Z0-9]+', post)\n",
        "\n",
        "print(f\"게시물: {post}\")\n",
        "print(f\"해시태그: {hashtags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert hashtags == ['#서울', '#맑음', '#주말나들이', '#맛집']\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "`re.findall()`은 패턴과 일치하는 모든 문자열을 리스트로 반환합니다.\n\n",
        "**핵심 개념**:\n",
        "- `re.findall(pattern, string)`: 패턴과 일치하는 모든 부분 문자열 반환\n",
        "- `#`: 해시태그 시작 기호 (리터럴 매칭)\n",
        "- `[가-힣a-zA-Z0-9]+`: 한글/영어/숫자 1개 이상\n",
        "- `+`: 앞의 패턴이 1회 이상 반복\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# \\w 사용 (단어 문자 = 영숫자 + 밑줄)\n",
        "re.findall(r'#\\w+', post)  # 한글은 \\w에 포함됨 (Python 3)\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ `+` 대신 `*`를 사용하면 `#`만 있는 것도 매칭됨\n",
        "- ❌ 언더스코어(_)가 필요하면 `[가-힣a-zA-Z0-9_]+`로 수정\n\n",
        "**실무 팁**:\n",
        "실제 SNS에서는 `#` 뒤에 이모지가 올 수도 있으므로 패턴을 유연하게 조정하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q4. NLTK 토큰화 ⭐⭐\n\n",
        "**문제**: 영어 문장을 단어와 문장으로 토큰화하세요.\n\n",
        "**요구사항**:\n",
        "- `word_tokenize()`: 단어 토큰화\n",
        "- `sent_tokenize()`: 문장 토큰화\n\n",
        "```python\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q4 정답\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n",
        "\n",
        "# 단어 토큰화\n",
        "word_tokens = word_tokenize(text)\n",
        "print(f\"단어 토큰: {word_tokens}\")\n",
        "\n",
        "# 문장 토큰화\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(f\"\\n문장 토큰: {sent_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert len(word_tokens) > 10  # 10개 이상의 토큰\n",
        "assert len(sent_tokens) == 4   # 4개의 문장\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "NLTK의 토큰화 함수는 단순 split()보다 정교하게 토큰화합니다. 축약형(I'm, It's)도 올바르게 분리합니다.\n\n",
        "**핵심 개념**:\n",
        "- `word_tokenize()`: Penn Treebank 스타일 토큰화\n",
        "  - 축약형 분리: \"I'm\" → [\"I\", \"'m\"]\n",
        "  - 구두점 분리: \"Hello!\" → [\"Hello\", \"!\"]\n",
        "- `sent_tokenize()`: 문장 경계 인식\n",
        "  - 마침표, 물음표, 느낌표로 문장 분리\n",
        "  - \"Dr.\"같은 약어는 분리 안 함\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 단순 split (정확도 낮음)\n",
        "text.split()  # 구두점이 단어에 붙어있음\n",
        "\n",
        "# 정규표현식 기반\n",
        "re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ NLTK 데이터 미다운로드: `nltk.download('punkt')` 필수\n",
        "- ❌ 한글에 word_tokenize() 사용: 공백 기준으로만 분리됨 (형태소 분석 필요)\n\n",
        "**실무 팁**:\n",
        "영어는 NLTK, 한글은 KoNLPy로 토큰화하세요. 혼용 텍스트는 언어 감지 후 분리 처리합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q5. 불용어 제거 ⭐⭐⭐\n\n",
        "**문제**: 영어 문장에서 불용어를 제거하세요.\n\n",
        "**요구사항**:\n",
        "- 소문자 변환 후 토큰화\n",
        "- NLTK 불용어 제거\n",
        "- 알파벳으로만 이루어진 단어만 유지\n\n",
        "```python\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n",
        "```\n\n",
        "**기대 결과**: `['movie', 'really', 'amazing', 'loved', 'much']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q5 정답\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n",
        "\n",
        "# 영어 불용어 목록\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 1. 소문자 변환\n",
        "sentence_lower = sentence.lower()\n",
        "\n",
        "# 2. 토큰화\n",
        "tokens = word_tokenize(sentence_lower)\n",
        "\n",
        "# 3. 불용어 제거 + 알파벳만 유지\n",
        "filtered = [token for token in tokens \n",
        "            if token not in stop_words and token.isalpha()]\n",
        "\n",
        "print(f\"원본: {sentence}\")\n",
        "print(f\"토큰: {tokens}\")\n",
        "print(f\"필터: {filtered}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert filtered == ['movie', 'really', 'amazing', 'loved', 'much']\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "불용어 제거는 토큰화 후에 수행합니다. set 자료형을 사용하면 O(1) 탐색으로 빠릅니다.\n\n",
        "**핵심 개념**:\n",
        "- `stopwords.words('english')`: 179개의 영어 불용어 목록\n",
        "- `set()`: 빠른 멤버십 테스트 (리스트는 O(n), set은 O(1))\n",
        "- `token.isalpha()`: 알파벳으로만 구성된 문자열인지 확인\n",
        "- 리스트 컴프리헨션: 조건을 만족하는 토큰만 필터링\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 함수형 스타일\n",
        "list(filter(lambda t: t not in stop_words and t.isalpha(), tokens))\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ 소문자 변환 안 함: \"The\"는 불용어지만 \"the\"만 불용어 목록에 있음\n",
        "- ❌ `isalpha()` 누락: 구두점(\"!\", \".\")이 결과에 포함됨\n",
        "- ❌ 리스트로 불용어 탐색: 느림 (매번 전체 순회)\n\n",
        "**실무 팁**:\n",
        "'very', 'really' 같은 부사도 분석 목적에 따라 제거할 수 있습니다. 커스텀 불용어를 추가하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q6. 어간 추출 vs 표제어 추출 ⭐⭐⭐\n\n",
        "**문제**: 단어 리스트에 어간 추출과 표제어 추출을 각각 적용하세요.\n\n",
        "**요구사항**:\n",
        "- `PorterStemmer` 사용\n",
        "- `WordNetLemmatizer` 사용 (동사 품사)\n",
        "- 결과를 딕셔너리로 반환: `{원본: (stemmed, lemmatized)}`\n\n",
        "```python\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q6 정답\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n",
        "\n",
        "# Stemmer와 Lemmatizer 초기화\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 결과 딕셔너리\n",
        "result = {}\n",
        "for word in words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # 동사로 처리\n",
        "    result[word] = (stemmed, lemmatized)\n",
        "\n",
        "# 출력\n",
        "print(\"어간 추출 vs 표제어 추출:\")\n",
        "print(\"-\" * 45)\n",
        "for word, (stem, lemma) in result.items():\n",
        "    print(f\"{word:12} → stem: {stem:10} lemma: {lemma}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert result['running'][0] == 'run'      # stem\n",
        "assert result['running'][1] == 'run'      # lemma\n",
        "assert result['studies'][0] == 'studi'    # stem (불완전)\n",
        "assert result['studies'][1] == 'study'    # lemma (완전)\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "어간 추출과 표제어 추출 모두 단어 정규화 기법이지만, 접근 방식이 다릅니다.\n\n",
        "**핵심 개념**:\n",
        "- **어간 추출 (Stemming)**: 규칙 기반으로 어미 제거\n",
        "  - 장점: 빠름\n",
        "  - 단점: 불완전한 결과 (\"studies\" → \"studi\")\n",
        "- **표제어 추출 (Lemmatization)**: 사전 기반으로 기본형 탐색\n",
        "  - 장점: 정확함 (\"studies\" → \"study\")\n",
        "  - 단점: 느림, 품사 정보 필요\n",
        "- `pos='v'`: 동사로 처리 (n=명사, a=형용사, r=부사)\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 딕셔너리 컴프리헨션\n",
        "result = {\n",
        "    word: (stemmer.stem(word), lemmatizer.lemmatize(word, pos='v'))\n",
        "    for word in words\n",
        "}\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ 표제어 추출 시 품사 미지정: 기본값이 명사라서 동사가 제대로 변환 안 됨\n",
        "- ❌ \"better\" → 표제어 추출로도 \"good\"으로 안 바뀜 (형용사 비교급은 WordNet에서 지원 안 함)\n\n",
        "**실무 팁**:\n",
        "검색 엔진에서는 빠른 어간 추출을, 텍스트 분석에서는 정확한 표제어 추출을 선택하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q7. KoNLPy 명사 추출 ⭐⭐⭐\n\n",
        "**문제**: 고객 리뷰에서 명사를 추출하고 빈도를 분석하세요.\n\n",
        "**요구사항**:\n",
        "- `Okt.nouns()` 사용\n",
        "- 1글자 명사 제외\n",
        "- 상위 5개 명사 출력\n\n",
        "```python\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"가격대비 품질이 훌륭합니다\",\n",
        "    \"배송은 빠른데 포장이 아쉬워요\",\n",
        "    \"상품 퀄리티 최고 배송도 빠름\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q7 정답\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"가격대비 품질이 훌륭합니다\",\n",
        "    \"배송은 빠른데 포장이 아쉬워요\",\n",
        "    \"상품 퀄리티 최고 배송도 빠름\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# 1. 모든 리뷰에서 명사 추출\n",
        "all_nouns = []\n",
        "for review in reviews:\n",
        "    nouns = okt.nouns(review)\n",
        "    all_nouns.extend(nouns)\n",
        "\n",
        "# 2. 1글자 명사 제외\n",
        "filtered_nouns = [noun for noun in all_nouns if len(noun) > 1]\n",
        "\n",
        "# 3. 빈도 분석\n",
        "noun_counts = Counter(filtered_nouns)\n",
        "\n",
        "# 4. 상위 5개 출력\n",
        "print(\"상위 5개 명사:\")\n",
        "for noun, count in noun_counts.most_common(5):\n",
        "    print(f\"  {noun}: {count}회\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "top_nouns = [noun for noun, _ in noun_counts.most_common(5)]\n",
        "assert '배송' in top_nouns  # 배송이 상위에 있어야 함\n",
        "assert '품질' in top_nouns  # 품질이 상위에 있어야 함\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "여러 리뷰에서 명사를 추출하고 빈도 분석으로 주요 키워드를 파악합니다.\n\n",
        "**핵심 개념**:\n",
        "- `okt.nouns()`: 문장에서 명사만 추출 (리스트 반환)\n",
        "- `list.extend()`: 리스트를 확장 (append는 요소 하나 추가)\n",
        "- `Counter.most_common(n)`: 빈도 상위 n개 반환\n",
        "- 1글자 명사 제외: 노이즈 감소 (\"것\", \"수\", \"등\")\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 리스트 컴프리헨션 + 중첩\n",
        "all_nouns = [noun for review in reviews \n",
        "             for noun in okt.nouns(review) if len(noun) > 1]\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ `append()` 사용: 리스트 안에 리스트가 생김 (`extend()` 사용)\n",
        "- ❌ 1글자 필터링 안 함: \"것\", \"수\" 등이 상위에 올라옴\n",
        "\n",
        "**실무 팁**:\n",
        "도메인별 불용어 목록을 만들어 추가 필터링하세요. 예: 쇼핑몰 리뷰에서 \"상품\", \"제품\"은 노이즈일 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q8. 품사별 단어 분류 ⭐⭐⭐⭐\n\n",
        "**문제**: 문장에서 품사별로 단어를 분류하세요.\n\n",
        "**요구사항**:\n",
        "- `Okt.pos()` 사용\n",
        "- 명사(Noun), 동사(Verb), 형용사(Adjective) 분류\n",
        "- 결과를 딕셔너리로 반환\n\n",
        "```python\n",
        "text = \"맛있는 음식을 먹고 행복한 하루를 보냈다\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q8 정답\n",
        "text = \"맛있는 음식을 먹고 행복한 하루를 보냈다\"\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# 품사 태깅 (어간 추출 적용)\n",
        "pos_result = okt.pos(text, stem=True)\n",
        "print(f\"품사 태깅: {pos_result}\")\n",
        "\n",
        "# 품사별 분류\n",
        "classified = {\n",
        "    'Noun': [],\n",
        "    'Verb': [],\n",
        "    'Adjective': []\n",
        "}\n",
        "\n",
        "for word, pos in pos_result:\n",
        "    if pos in classified:\n",
        "        classified[pos].append(word)\n",
        "\n",
        "print(f\"\\n분류 결과: {classified}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert '음식' in classified['Noun']\n",
        "assert '먹다' in classified['Verb']\n",
        "assert '맛있다' in classified['Adjective']\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "품사 태깅 결과를 순회하며 원하는 품사의 단어만 수집합니다.\n\n",
        "**핵심 개념**:\n",
        "- `okt.pos()`: (단어, 품사) 튜플의 리스트 반환\n",
        "- `stem=True`: 어간 추출 적용 (\"먹고\" → \"먹다\")\n",
        "- Okt 품사 태그: Noun(명사), Verb(동사), Adjective(형용사), Adverb(부사) 등\n",
        "\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# defaultdict 사용\n",
        "from collections import defaultdict\n",
        "classified = defaultdict(list)\n",
        "for word, pos in pos_result:\n",
        "    classified[pos].append(word)\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ `stem=True` 미적용: \"먹고\", \"보냈다\" 그대로 나옴\n",
        "- ❌ 품사 태그 오타: 'Noun' (O), 'noun' (X)\n",
        "\n",
        "**실무 팁**:\n",
        "감성 분석에서는 형용사가 중요하고, 키워드 추출에서는 명사가 중요합니다. 목적에 맞게 품사를 선택하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q9. 한글 전처리 함수 ⭐⭐⭐⭐\n\n",
        "**문제**: 한글 텍스트 전처리 함수를 작성하세요.\n\n",
        "**요구사항**:\n",
        "- 함수명: `clean_korean_text`\n",
        "- 기능:\n",
        "  1. 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "  2. 중복 공백 제거\n",
        "  3. 형태소 분석 (정규화, 어간 추출)\n",
        "  4. 불용어 제거 (조사, 어미)\n",
        "  5. 2글자 이상 토큰만 유지\n",
        "- 반환: 토큰 리스트\n\n",
        "```python\n",
        "text = \"오늘 배송받은 상품이 정말 좋아요!!! 강력 추천합니다~~\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q9 정답\n",
        "def clean_korean_text(text, min_length=2):\n",
        "    \"\"\"\n",
        "    한글 텍스트 전처리 함수\n",
        "    \n",
        "    Args:\n",
        "        text (str): 원본 텍스트\n",
        "        min_length (int): 최소 토큰 길이\n",
        "    \n",
        "    Returns:\n",
        "        list: 전처리된 토큰 리스트\n",
        "    \"\"\"\n",
        "    okt = Okt()\n",
        "    \n",
        "    # 1. 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # 2. 중복 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 3. 형태소 분석 (정규화 + 어간 추출)\n",
        "    tokens = okt.morphs(text, norm=True, stem=True)\n",
        "    \n",
        "    # 4. 불용어 제거\n",
        "    korean_stopwords = {'은', '는', '이', '가', '을', '를', '의', '에', '에서',\n",
        "                        '로', '으로', '도', '만', '과', '와', '하다', '있다', '되다',\n",
        "                        '것', '수', '등', '더', '좀'}\n",
        "    tokens = [t for t in tokens if t not in korean_stopwords]\n",
        "    \n",
        "    # 5. 최소 길이 필터링\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# 테스트\n",
        "text = \"오늘 배송받은 상품이 정말 좋아요!!! 강력 추천합니다~~\"\n",
        "result = clean_korean_text(text)\n",
        "\n",
        "print(f\"원본: {text}\")\n",
        "print(f\"결과: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "assert '배송' in result or '받다' in result  # 핵심 단어 포함\n",
        "assert '!!!' not in str(result)  # 특수문자 제거됨\n",
        "assert all(len(t) >= 2 for t in result)  # 모두 2글자 이상\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "전처리 파이프라인을 함수로 캡슐화하면 재사용성이 높아집니다. 순서가 중요합니다.\n\n",
        "**핵심 개념**:\n",
        "- 전처리 순서: 특수문자 제거 → 공백 정리 → 토큰화 → 불용어 제거 → 길이 필터\n",
        "- `norm=True`: 정규화 (\"ㅋㅋㅋ\" → \"ㅋㅋ\", \"그래욬\" → \"그래요\")\n",
        "- `stem=True`: 어간 추출 (\"좋아요\" → \"좋다\")\n",
        "- 커스텀 불용어: 조사, 의존명사 등 분석에 불필요한 단어\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# 클래스로 구현 (상태 유지)\n",
        "class KoreanPreprocessor:\n",
        "    def __init__(self, stopwords=None, min_length=2):\n",
        "        self.okt = Okt()\n",
        "        self.stopwords = stopwords or set()\n",
        "        self.min_length = min_length\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        # ... 동일한 로직\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ Okt()를 함수 내에서 매번 생성: 느림 (전역 또는 클래스 멤버로)\n",
        "- ❌ 정규표현식에서 한글 범위 오타: `[가-힣]` (O), `[ㄱ-힣]` (X)\n",
        "\n",
        "**실무 팁**:\n",
        "불용어 목록은 분석 도메인에 따라 계속 업데이트하세요. 파일로 관리하면 편리합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "### Q10. 종합: 뉴스 기사 키워드 추출 ⭐⭐⭐⭐⭐\n\n",
        "**문제**: 뉴스 기사에서 핵심 키워드를 추출하는 파이프라인을 완성하세요.\n\n",
        "**요구사항**:\n",
        "- 함수명: `extract_keywords`\n",
        "- 입력: 뉴스 기사 텍스트\n",
        "- 처리 과정:\n",
        "  1. 특수문자 제거\n",
        "  2. 형태소 분석 (Okt)\n",
        "  3. 명사만 추출\n",
        "  4. 1글자 명사 제외\n",
        "  5. 빈도 분석\n",
        "- 반환: 상위 N개 키워드 리스트 `[(키워드, 빈도), ...]`\n\n",
        "```python\n",
        "news = \"\"\"\n",
        "삼성전자가 새로운 갤럭시 스마트폰을 출시했다.\n",
        "이번 갤럭시는 카메라 성능이 크게 향상되었으며,\n",
        "배터리 용량도 늘어났다. 삼성전자 관계자는\n",
        "\"이번 갤럭시가 시장에서 좋은 반응을 얻을 것\"이라고 밝혔다.\n",
        "경쟁사인 애플의 아이폰과의 경쟁도 치열해질 전망이다.\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q10 정답\n",
        "def extract_keywords(text, top_n=10, min_length=2):\n",
        "    \"\"\"\n",
        "    뉴스 기사에서 핵심 키워드를 추출합니다.\n",
        "    \n",
        "    Args:\n",
        "        text (str): 뉴스 기사 텍스트\n",
        "        top_n (int): 반환할 상위 키워드 수\n",
        "        min_length (int): 최소 키워드 길이\n",
        "    \n",
        "    Returns:\n",
        "        list: [(키워드, 빈도), ...] 형태의 리스트\n",
        "    \"\"\"\n",
        "    okt = Okt()\n",
        "    \n",
        "    # 1. 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "    cleaned = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # 2. 중복 공백 제거\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    \n",
        "    # 3. 명사 추출 (Okt.nouns() 사용)\n",
        "    nouns = okt.nouns(cleaned)\n",
        "    \n",
        "    # 4. 1글자 명사 제외 + 불용어 제거\n",
        "    stopwords = {'것', '수', '등', '이번', '이', '그', '저'}\n",
        "    filtered = [noun for noun in nouns \n",
        "                if len(noun) >= min_length and noun not in stopwords]\n",
        "    \n",
        "    # 5. 빈도 분석\n",
        "    keyword_counts = Counter(filtered)\n",
        "    \n",
        "    # 6. 상위 N개 반환\n",
        "    return keyword_counts.most_common(top_n)\n",
        "\n",
        "# 테스트\n",
        "news = \"\"\"\n",
        "삼성전자가 새로운 갤럭시 스마트폰을 출시했다.\n",
        "이번 갤럭시는 카메라 성능이 크게 향상되었으며,\n",
        "배터리 용량도 늘어났다. 삼성전자 관계자는\n",
        "\"이번 갤럭시가 시장에서 좋은 반응을 얻을 것\"이라고 밝혔다.\n",
        "경쟁사인 애플의 아이폰과의 경쟁도 치열해질 전망이다.\n",
        "\"\"\"\n",
        "\n",
        "keywords = extract_keywords(news, top_n=10)\n",
        "\n",
        "print(\"뉴스 기사 핵심 키워드:\")\n",
        "print(\"=\"*30)\n",
        "for keyword, count in keywords:\n",
        "    print(f\"  {keyword}: {count}회\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트\n",
        "keyword_words = [kw for kw, _ in keywords]\n",
        "\n",
        "# 핵심 키워드가 포함되어야 함\n",
        "assert '갤럭시' in keyword_words, \"'갤럭시'가 키워드에 없습니다\"\n",
        "assert '삼성전자' in keyword_words or '삼성' in keyword_words, \"'삼성전자'가 키워드에 없습니다\"\n",
        "\n",
        "# 반환 형식 검증\n",
        "assert isinstance(keywords, list), \"반환값이 리스트가 아닙니다\"\n",
        "assert isinstance(keywords[0], tuple), \"키워드가 튜플이 아닙니다\"\n",
        "assert len(keywords[0]) == 2, \"(키워드, 빈도) 형식이 아닙니다\"\n",
        "\n",
        "print(\"✅ 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 풀이 설명\n\n",
        "**접근 방법**:\n",
        "전처리 → 명사 추출 → 필터링 → 빈도 분석의 전형적인 키워드 추출 파이프라인입니다.\n\n",
        "**핵심 개념**:\n",
        "- **전처리 파이프라인**: 순차적인 텍스트 정제 과정\n",
        "- **명사 기반 키워드**: 뉴스에서 명사가 핵심 정보를 담음\n",
        "- **빈도 분석**: 자주 등장하는 단어 = 중요 키워드\n",
        "- **불용어 처리**: 도메인별 커스텀 불용어 적용\n\n",
        "**대안 솔루션**:\n",
        "```python\n",
        "# TF-IDF 기반 (여러 문서 비교 시 유용)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TextRank 알고리즘 (문맥 고려)\n",
        "from gensim.summarization import keywords\n",
        "```\n\n",
        "**흔한 실수**:\n",
        "- ❌ 불용어 미처리: \"이번\", \"것\" 등이 상위에 올라옴\n",
        "- ❌ 1글자 필터링 누락: \"외\", \"내\" 등 의미 없는 단어 포함\n",
        "- ❌ 반환 형식 오류: `[(키워드, 빈도)]` 형태여야 함\n\n",
        "**실무 팁**:\n",
        "1. **TF-IDF 활용**: 단순 빈도보다 정교한 중요도 측정\n",
        "2. **n-gram 고려**: \"삼성전자\" 같은 복합명사 추출\n",
        "3. **시각화**: 워드클라우드로 결과 표현\n",
        "4. **도메인 사전**: 전문 용어는 별도 사전으로 관리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 보너스: 여러 뉴스 기사에 적용\n",
        "news_articles = [\n",
        "    \"삼성전자가 새로운 갤럭시 스마트폰을 출시했다. 카메라 성능이 향상되었다.\",\n",
        "    \"애플이 아이폰15를 발표했다. 배터리 수명이 크게 늘었다.\",\n",
        "    \"LG전자가 새로운 TV를 선보였다. OLED 기술을 적용했다.\"\n",
        "]\n",
        "\n",
        "print(\"뉴스별 키워드 분석:\")\n",
        "print(\"=\"*50)\n",
        "for i, article in enumerate(news_articles, 1):\n",
        "    kws = extract_keywords(article, top_n=5)\n",
        "    kw_str = ', '.join([f\"{kw}({cnt})\" for kw, cnt in kws])\n",
        "    print(f\"뉴스 {i}: {kw_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 🎉 수고하셨습니다!\n\n",
        "텍스트 전처리 마스터를 축하합니다! 이제 다음 단계로 넘어갑시다.\n\n",
        "**다음 노트북**: `Day11_1_텍스트_분석_기법_Text_Analysis.ipynb`\n\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
