{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Day2-3] íŒŒì¼ I/Oì™€ ì§ë ¬í™” í€´ì¦ˆ ì •ë‹µ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Day2-3 ì‹¤ì „ í€´ì¦ˆì˜ ì •ë‹µ í’€ì´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. [íŒŒì¼ I/O] ë¡œê·¸ íŒŒì¼ ì½ê¸° (ë‚œì´ë„: â­â­)\n",
    "\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ë¡œê·¸ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. ìœ„ ë‚´ìš©ìœ¼ë¡œ `server.log` íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "2. íŒŒì¼ì„ ì½ì–´ì„œ `\"ERROR\"`ê°€ í¬í•¨ëœ ì¤„ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "3. ì´ ëª‡ ê°œì˜ ì—ëŸ¬ê°€ ìˆëŠ”ì§€ ì„¸ì–´ ì¶œë ¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: ê¸°ë³¸ í’€ì´ (ê¶Œì¥)\n",
    "# 1. ë¡œê·¸ íŒŒì¼ ìƒì„±\n",
    "log_content = \"\"\"2025-01-01 ERROR Connection failed\n",
    "2025-01-01 INFO User login\n",
    "2025-01-01 ERROR Database timeout\n",
    "2025-01-01 WARN Low memory\"\"\"\n",
    "\n",
    "with open('server.log', 'w', encoding='utf-8') as f:\n",
    "    f.write(log_content)\n",
    "\n",
    "# 2. ERROR ë¡œê·¸ë§Œ ì¶œë ¥\n",
    "print(\"=== ERROR ë¡œê·¸ ===\")\n",
    "error_count = 0\n",
    "\n",
    "with open('server.log', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if 'ERROR' in line:\n",
    "            print(line.strip())\n",
    "            error_count += 1\n",
    "\n",
    "# 3. ì—ëŸ¬ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"\\nì´ ì—ëŸ¬ ê°œìˆ˜: {error_count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: readlines() ì‚¬ìš©\n",
    "with open('server.log', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ í•„í„°ë§\n",
    "error_lines = [line.strip() for line in lines if 'ERROR' in line]\n",
    "\n",
    "print(\"=== ERROR ë¡œê·¸ (ë°©ë²•2) ===\")\n",
    "for error in error_lines:\n",
    "    print(error)\n",
    "\n",
    "print(f\"\\nì´ ì—ëŸ¬ ê°œìˆ˜: {len(error_lines)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: ì œë„ˆë ˆì´í„° í‘œí˜„ì‹ í™œìš©\n",
    "with open('server.log', 'r', encoding='utf-8') as f:\n",
    "    error_lines = (line.strip() for line in f if 'ERROR' in line)\n",
    "    \n",
    "    print(\"=== ERROR ë¡œê·¸ (ë°©ë²•3) ===\")\n",
    "    count = 0\n",
    "    for error in error_lines:\n",
    "        print(error)\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"\\nì´ ì—ëŸ¬ ê°œìˆ˜: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **íŒŒì¼ ìƒì„±**: `'w'` ëª¨ë“œë¡œ ìƒˆ íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ íŒŒì¼ì„ ë®ì–´ì”ë‹ˆë‹¤.\n",
    "- **ì¤„ ë‹¨ìœ„ ì½ê¸°**: `for line in f:` íŒ¨í„´ì´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤ (ì œë„ˆë ˆì´í„°).\n",
    "- **`strip()` ë©”ì†Œë“œ**: ì¤„ ëì˜ ì¤„ë°”ê¿ˆ ë¬¸ì(`\\n`)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "- **`in` ì—°ì‚°ì**: ë¬¸ìì—´ì— íŠ¹ì • ë¶€ë¶„ ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "- **ë°©ë²• ë¹„êµ**:\n",
    "  - ë°©ë²• 1: í•œ ì¤„ì”© ì½ìœ¼ë©° ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼ì— ì í•©)\n",
    "  - ë°©ë²• 2: ì „ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì½ì€ í›„ ì²˜ë¦¬ (ì‘ì€ íŒŒì¼ì— ì í•©)\n",
    "  - ë°©ë²• 3: ì œë„ˆë ˆì´í„°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Q2. [Context Manager] ì•ˆì „í•œ íŒŒì¼ ì“°ê¸° (ë‚œì´ë„: â­â­)\n",
    "\n",
    "`í•¨ìˆ˜ ì‘ì„±:`\n",
    "* `safe_write_lines(filename, lines)` í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "* `lines`ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "* `with` ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì— ê° ì¤„ì„ ì•ˆì „í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "* ê° ì¤„ ëì— ì¤„ë°”ê¿ˆ(`\\n`)ì„ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: ê¸°ë³¸ í’€ì´ (ê¶Œì¥)\n",
    "def safe_write_lines(filename, lines):\n",
    "    \"\"\"\n",
    "    with ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ íŒŒì¼ì— ì¤„ë“¤ì„ ì‘ì„±\n",
    "    \n",
    "    Args:\n",
    "        filename (str): ì €ì¥í•  íŒŒì¼ëª…\n",
    "        lines (list): ì‘ì„±í•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "safe_write_lines('test_output.txt', ['Line 1', 'Line 2', 'Line 3'])\n",
    "print(\"íŒŒì¼ ì‘ì„± ì™„ë£Œ!\")\n",
    "\n",
    "# í™•ì¸\n",
    "with open('test_output.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: writelines() í™œìš©\n",
    "def safe_write_lines_v2(filename, lines):\n",
    "    \"\"\"writelines()ë¥¼ ì‚¬ìš©í•œ ë²„ì „\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # ê° ì¤„ì— \\nì„ ì¶”ê°€í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "        lines_with_newline = [line + '\\n' for line in lines]\n",
    "        f.writelines(lines_with_newline)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "safe_write_lines_v2('test_output2.txt', ['Alpha', 'Beta', 'Gamma'])\n",
    "\n",
    "# í™•ì¸\n",
    "with open('test_output2.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: join() í™œìš© (ê°€ì¥ íš¨ìœ¨ì )\n",
    "def safe_write_lines_v3(filename, lines):\n",
    "    \"\"\"join()ìœ¼ë¡œ í•œ ë²ˆì— ì‘ì„±\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(lines) + '\\n')\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "safe_write_lines_v3('test_output3.txt', ['First', 'Second', 'Third'])\n",
    "\n",
    "# í™•ì¸\n",
    "with open('test_output3.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **`with` ë¬¸ì˜ ì¥ì **: ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë‹«í™ë‹ˆë‹¤.\n",
    "- **`write()` vs `writelines()`**:\n",
    "  - `write()`: ë¬¸ìì—´ í•˜ë‚˜ë¥¼ ì‘ì„± (ì¤„ë°”ê¿ˆ ìë™ ì¶”ê°€ ì•ˆ ë¨)\n",
    "  - `writelines()`: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„± (ì¤„ë°”ê¿ˆ ìë™ ì¶”ê°€ ì•ˆ ë¨)\n",
    "- **ì¤„ë°”ê¿ˆ ì¶”ê°€**: `\\n`ì„ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- **ì„±ëŠ¥ ë¹„êµ**:\n",
    "  - ë°©ë²• 1: ê°€ì¥ ëª…í™•í•˜ê³  ì½ê¸° ì‰¬ì›€\n",
    "  - ë°©ë²• 2: PythonìŠ¤ëŸ¬ìš´ ë°©ì‹\n",
    "  - ë°©ë²• 3: ê°€ì¥ íš¨ìœ¨ì  (I/O íšŸìˆ˜ ìµœì†Œí™”)\n",
    "- **ì¸ì½”ë”©**: `encoding='utf-8'`ë¡œ í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. [JSON] ì‚¬ìš©ì ë°ì´í„° ê´€ë¦¬ (ë‚œì´ë„: â­â­â­)\n",
    "\n",
    "1. ì‚¬ìš©ì ë°ì´í„°ë¥¼ `user_profile.json` íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”.\n",
    "2. íŒŒì¼ì„ ì½ì–´ì„œ ì‚¬ìš©ìì˜ ì´ë¦„ê³¼ ì–¸ì–´ ì„¤ì •ì„ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "3. `preferences`ì— `\"theme\": \"dark\"`ë¥¼ ì¶”ê°€í•˜ê³  íŒŒì¼ì„ ë‹¤ì‹œ ì €ì¥í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "user = {\n",
    "    \"user_id\": \"U001\",\n",
    "    \"name\": \"ê¹€ë°ì´í„°\",\n",
    "    \"email\": \"data@example.com\",\n",
    "    \"preferences\": {\n",
    "        \"language\": \"ko\",\n",
    "        \"notifications\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# 1. JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open('user_profile.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(user, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# 2. íŒŒì¼ ì½ê³  ì •ë³´ ì¶œë ¥\n",
    "with open('user_profile.json', 'r', encoding='utf-8') as f:\n",
    "    loaded_user = json.load(f)\n",
    "\n",
    "print(f\"ì´ë¦„: {loaded_user['name']}\")\n",
    "print(f\"ì–¸ì–´ ì„¤ì •: {loaded_user['preferences']['language']}\")\n",
    "\n",
    "# 3. theme ì¶”ê°€ í›„ ì¬ì €ì¥\n",
    "loaded_user['preferences']['theme'] = 'dark'\n",
    "\n",
    "with open('user_profile.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(loaded_user, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\ní…Œë§ˆ ì„¤ì • ì¶”ê°€ ì™„ë£Œ!\")\n",
    "\n",
    "# í™•ì¸\n",
    "with open('user_profile.json', 'r', encoding='utf-8') as f:\n",
    "    print(\"\\nìµœì¢… í”„ë¡œí•„:\")\n",
    "    print(json.dumps(json.load(f), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **`json.dump(obj, file)`**: Python ê°ì²´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "- **`json.load(file)`**: JSON íŒŒì¼ì„ Python ê°ì²´ë¡œ ë³€í™˜\n",
    "- **`ensure_ascii=False`**: í•œê¸€ì„ ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„(`\\uXXXX`) ëŒ€ì‹  ê·¸ëŒ€ë¡œ ì €ì¥\n",
    "- **`indent=2`**: ë“¤ì—¬ì“°ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ (ì‚¬ëŒì´ ì½ê¸° ì‰¬ì›€)\n",
    "- **ì¤‘ì²© ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼**: `user['preferences']['language']`ì²˜ëŸ¼ ì²´ì´ë‹ ê°€ëŠ¥\n",
    "- **ë”•ì…”ë„ˆë¦¬ ìˆ˜ì •**: JSONì—ì„œ ì½ì–´ì˜¨ ê°ì²´ëŠ” ì¼ë°˜ Python ë”•ì…”ë„ˆë¦¬ì´ë¯€ë¡œ ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥\n",
    "- **ì¬ì €ì¥**: ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ `json.dump()`ë¡œ ì €ì¥í•˜ë©´ íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. [JSON] ë°ì´í„° ë³€í™˜ ë° í•„í„°ë§ (ë‚œì´ë„: â­â­â­)\n",
    "\n",
    "`í•¨ìˆ˜ ì‘ì„±:`\n",
    "* `filter_products_by_price(json_file, min_price)` í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "* JSON íŒŒì¼ì—ì„œ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "* `price`ê°€ `min_price` ì´ìƒì¸ ì œí’ˆë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© JSON íŒŒì¼ ìƒì„±\n",
    "products = [\n",
    "    {\"name\": \"Mouse\", \"price\": 15000},\n",
    "    {\"name\": \"Keyboard\", \"price\": 45000},\n",
    "    {\"name\": \"Monitor\", \"price\": 250000}\n",
    "]\n",
    "\n",
    "with open('products.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(products, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ë°©ë²• 1: ê¸°ë³¸ í’€ì´ (ê¶Œì¥)\n",
    "def filter_products_by_price(json_file, min_price):\n",
    "    \"\"\"\n",
    "    JSON íŒŒì¼ì—ì„œ ìµœì†Œ ê°€ê²© ì´ìƒì˜ ì œí’ˆë§Œ í•„í„°ë§\n",
    "    \n",
    "    Args:\n",
    "        json_file (str): JSON íŒŒì¼ ê²½ë¡œ\n",
    "        min_price (int): ìµœì†Œ ê°€ê²©\n",
    "    \n",
    "    Returns:\n",
    "        list: í•„í„°ë§ëœ ì œí’ˆ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # JSON íŒŒì¼ ì½ê¸°\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        products = json.load(f)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ í•„í„°ë§\n",
    "    filtered = [p for p in products if p['price'] >= min_price]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "result = filter_products_by_price('products.json', 40000)\n",
    "print(result)\n",
    "# [{'name': 'Keyboard', 'price': 45000}, {'name': 'Monitor', 'price': 250000}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: filter() í•¨ìˆ˜ ì‚¬ìš©\n",
    "def filter_products_by_price_v2(json_file, min_price):\n",
    "    \"\"\"filter() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ë²„ì „\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        products = json.load(f)\n",
    "    \n",
    "    # filter()ì™€ lambda ì‚¬ìš©\n",
    "    filtered = list(filter(lambda p: p['price'] >= min_price, products))\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "result = filter_products_by_price_v2('products.json', 40000)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€ (ì‹¤ë¬´ ë²„ì „)\n",
    "def filter_products_by_price_v3(json_file, min_price):\n",
    "    \"\"\"ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•œ ì‹¤ë¬´ ë²„ì „\"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            products = json.load(f)\n",
    "        \n",
    "        # ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "        if not isinstance(products, list):\n",
    "            raise ValueError(\"JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        # í•„í„°ë§\n",
    "        filtered = [p for p in products if p.get('price', 0) >= min_price]\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤: {json_file}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        return []\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "result = filter_products_by_price_v3('products.json', 40000)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **JSON íŒŒì¼ ì½ê¸°**: `json.load(f)`ë¡œ íŒŒì¼ì—ì„œ ì§ì ‘ ì½ì–´ì˜µë‹ˆë‹¤.\n",
    "- **ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜**: `[p for p in products if condition]` íŒ¨í„´ìœ¼ë¡œ í•„í„°ë§\n",
    "- **`filter()` í•¨ìˆ˜**: í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ìŠ¤íƒ€ì¼ (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë” PythonìŠ¤ëŸ¬ì›€)\n",
    "- **`.get('price', 0)`**: í‚¤ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)\n",
    "- **ì—ëŸ¬ ì²˜ë¦¬**:\n",
    "  - `FileNotFoundError`: íŒŒì¼ì´ ì—†ì„ ë•Œ\n",
    "  - `JSONDecodeError`: JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆì„ ë•Œ\n",
    "  - ì‹¤ë¬´ì—ì„œëŠ” í•­ìƒ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. [JSON] ì—¬ëŸ¬ íŒŒì¼ ë³‘í•© (ë‚œì´ë„: â­â­â­â­)\n",
    "\n",
    "`í•¨ìˆ˜ ì‘ì„±:`\n",
    "* `merge_json_files(file_list, output_file)` í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "* ëª¨ë“  íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ìƒì„±\n",
    "data1 = [{\"id\": 1, \"name\": \"A\"}, {\"id\": 2, \"name\": \"B\"}]\n",
    "data2 = [{\"id\": 3, \"name\": \"C\"}, {\"id\": 4, \"name\": \"D\"}]\n",
    "\n",
    "with open('data1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data1, f)\n",
    "\n",
    "with open('data2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data2, f)\n",
    "\n",
    "# ë°©ë²• 1: ê¸°ë³¸ í’€ì´ (ê¶Œì¥)\n",
    "def merge_json_files(file_list, output_file):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ JSON íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "    \n",
    "    Args:\n",
    "        file_list (list): JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
    "        output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    \n",
    "    # ê° íŒŒì¼ ì½ì–´ì„œ ë³‘í•©\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            merged_data.extend(data)  # ë¦¬ìŠ¤íŠ¸ í™•ì¥\n",
    "    \n",
    "    # ë³‘í•©ëœ ë°ì´í„° ì €ì¥\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ë³‘í•© ì™„ë£Œ! ì´ {len(merged_data)}ê°œ í•­ëª©\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "merge_json_files(['data1.json', 'data2.json'], 'merged.json')\n",
    "\n",
    "# í™•ì¸\n",
    "with open('merged.json', 'r') as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ í™œìš©\n",
    "def merge_json_files_v2(file_list, output_file):\n",
    "    \"\"\"ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ\"\"\"\n",
    "    # ëª¨ë“  íŒŒì¼ ì½ê¸°\n",
    "    all_data = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            all_data.extend(json.load(f))\n",
    "    \n",
    "    # ì €ì¥\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "merge_json_files_v2(['data1.json', 'data2.json'], 'merged2.json')\n",
    "\n",
    "# í™•ì¸\n",
    "with open('merged2.json', 'r') as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: ì—ëŸ¬ ì²˜ë¦¬ + ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì‹¤ë¬´ ë²„ì „)\n",
    "from datetime import datetime\n",
    "\n",
    "def merge_json_files_v3(file_list, output_file, add_metadata=True):\n",
    "    \"\"\"ì—ëŸ¬ ì²˜ë¦¬ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•œ ì‹¤ë¬´ ë²„ì „\"\"\"\n",
    "    merged_data = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"ê²½ê³ : {file_path}ëŠ” ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                    failed_files.append(file_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ì—ëŸ¬: {file_path} - {e}\")\n",
    "            failed_files.append(file_path)\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì„ íƒì‚¬í•­)\n",
    "    result = {\n",
    "        \"data\": merged_data,\n",
    "        \"metadata\": {\n",
    "            \"total_items\": len(merged_data),\n",
    "            \"source_files\": len(file_list) - len(failed_files),\n",
    "            \"failed_files\": failed_files,\n",
    "            \"merged_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    } if add_metadata else merged_data\n",
    "    \n",
    "    # ì €ì¥\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ë³‘í•© ì™„ë£Œ! ì´ {len(merged_data)}ê°œ í•­ëª©\")\n",
    "    if failed_files:\n",
    "        print(f\"ì‹¤íŒ¨í•œ íŒŒì¼: {failed_files}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "merge_json_files_v3(['data1.json', 'data2.json'], 'merged3.json', add_metadata=True)\n",
    "\n",
    "# í™•ì¸\n",
    "with open('merged3.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "    print(f\"\\në°ì´í„°: {result['data']}\")\n",
    "    print(f\"ë©”íƒ€ë°ì´í„°: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **`extend()` vs `append()`**:\n",
    "  - `extend()`: ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ì¶”ê°€ (ë³‘í•©)\n",
    "  - `append()`: ë¦¬ìŠ¤íŠ¸ ìì²´ë¥¼ í•˜ë‚˜ì˜ ìš”ì†Œë¡œ ì¶”ê°€ (ì¤‘ì²©)\n",
    "- **ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬**: `for` ë£¨í”„ë¡œ ê° íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘\n",
    "- **ì—ëŸ¬ ì²˜ë¦¬**: ê° íŒŒì¼ ì½ê¸° ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ë¥¸ íŒŒì¼ ì²˜ë¦¬ ê³„ì†\n",
    "- **ë©”íƒ€ë°ì´í„°**: ë³‘í•© ì •ë³´ë¥¼ í•¨ê»˜ ì €ì¥í•˜ë©´ ì¶”ì  ê°€ëŠ¥\n",
    "- **ì‹¤ë¬´ íŒ**: ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ë³‘í•©í•  ë•ŒëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. [ì¢…í•©] CSV to JSON ë³€í™˜ê¸° (ë‚œì´ë„: â­â­â­â­)\n",
    "\n",
    "CSV í˜•ì‹ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ JSON íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© CSV íŒŒì¼ ìƒì„±\n",
    "csv_content = \"\"\"product,quantity,price\n",
    "Apple,100,1000\n",
    "Banana,200,500\n",
    "Orange,150,800\"\"\"\n",
    "\n",
    "with open('sales.csv', 'w', encoding='utf-8') as f:  #\n",
    "    f.write(csv_content)\n",
    "\n",
    "# ë°©ë²• 1: ê¸°ë³¸ í’€ì´ (ê¶Œì¥)\n",
    "def csv_to_json(csv_file, json_file):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì„ JSON íŒŒì¼ë¡œ ë³€í™˜\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ\n",
    "        json_file (str): ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # ì²« ì¤„ì€ í—¤ë”\n",
    "    header = lines[0].strip().split(',')\n",
    "    \n",
    "    # ë‚˜ë¨¸ì§€ ì¤„ì€ ë°ì´í„°\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        values = line.strip().split(',')\n",
    "        # zipìœ¼ë¡œ í—¤ë”ì™€ ê°’ì„ ë§¤ì¹­í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "        row_dict = dict(zip(header, values))\n",
    "        data.append(row_dict)\n",
    "    \n",
    "    # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ë³€í™˜ ì™„ë£Œ! {len(data)}ê°œ í–‰\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "csv_to_json('sales.csv', 'sales.json')\n",
    "\n",
    "# í™•ì¸\n",
    "with open('sales.json', 'r', encoding='utf-8') as f:\n",
    "    print(json.dumps(json.load(f), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ í™œìš©\n",
    "def csv_to_json_v2(csv_file, json_file):\n",
    "    \"\"\"ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ\"\"\"\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    \n",
    "    header = lines[0].split(',')\n",
    "    data = [\n",
    "        dict(zip(header, line.split(',')))\n",
    "        for line in lines[1:]\n",
    "    ]\n",
    "    \n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "csv_to_json_v2('sales.csv', 'sales2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: íƒ€ì… ë³€í™˜ ì¶”ê°€ (ì‹¤ë¬´ ë²„ì „)\n",
    "def csv_to_json_v3(csv_file, json_file, numeric_cols=None):\n",
    "    \"\"\"\n",
    "    íƒ€ì… ë³€í™˜ì„ ì¶”ê°€í•œ ì‹¤ë¬´ ë²„ì „\n",
    "    \n",
    "    Args:\n",
    "        csv_file: ì…ë ¥ CSV íŒŒì¼\n",
    "        json_file: ì¶œë ¥ JSON íŒŒì¼\n",
    "        numeric_cols: ìˆ«ìë¡œ ë³€í™˜í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = []\n",
    "    \n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    \n",
    "    header = lines[0].split(',')\n",
    "    data = []\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        values = line.split(',')\n",
    "        row_dict = {}\n",
    "        \n",
    "        for key, value in zip(header, values):\n",
    "            # ìˆ«ì ì»¬ëŸ¼ì´ë©´ intë¡œ ë³€í™˜\n",
    "            if key in numeric_cols:\n",
    "                try:\n",
    "                    row_dict[key] = int(value)\n",
    "                except ValueError:\n",
    "                    row_dict[key] = value\n",
    "            else:\n",
    "                row_dict[key] = value\n",
    "        \n",
    "        data.append(row_dict)\n",
    "    \n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ë³€í™˜ ì™„ë£Œ! {len(data)}ê°œ í–‰\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ (quantityì™€ priceë¥¼ ìˆ«ìë¡œ ë³€í™˜)\n",
    "csv_to_json_v3('sales.csv', 'sales3.json', numeric_cols=['quantity', 'price'])\n",
    "\n",
    "# í™•ì¸\n",
    "with open('sales3.json', 'r', encoding='utf-8') as f:\n",
    "    result = json.load(f)\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    print(f\"\\nquantity íƒ€ì…: {type(result[0]['quantity'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **CSV êµ¬ì¡°**: ì²« ì¤„ì€ í—¤ë”(ì»¬ëŸ¼ëª…), ë‚˜ë¨¸ì§€ëŠ” ë°ì´í„° í–‰\n",
    "- **`split(',')`**: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬\n",
    "- **`zip(header, values)`**: ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ìŒìœ¼ë¡œ ë¬¶ì–´ì¤Œ\n",
    "  - `zip(['a', 'b'], [1, 2])` â†’ `[('a', 1), ('b', 2)]`\n",
    "- **`dict(zip(...))`**: ìŒ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "- **íƒ€ì… ë³€í™˜**: CSVëŠ” ëª¨ë‘ ë¬¸ìì—´ì´ë¯€ë¡œ, í•„ìš”ì‹œ `int()`, `float()` ë“±ìœ¼ë¡œ ë³€í™˜\n",
    "- **ì‹¤ë¬´ íŒ**: \n",
    "  - í° CSV íŒŒì¼ì€ `csv` ëª¨ë“ˆ ì‚¬ìš© ê¶Œì¥\n",
    "  - ë³µì¡í•œ CSVëŠ” `pandas` ì‚¬ìš© ê¶Œì¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. [ì¢…í•©/ì‹¬í™”] ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (ë‚œì´ë„: â­â­â­â­â­)\n",
    "\n",
    "ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. ê° CSV íŒŒì¼ì„ ì½ì–´ íŒŒì‹±\n",
    "2. ë‚ ì§œë³„ë¡œ ëª¨ë“  ë¶€ì„œì˜ ë§¤ì¶œ í•©ì‚°\n",
    "3. JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "4. ì „ì²´ ê¸°ê°„ ì´ ë§¤ì¶œ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© CSV íŒŒì¼ ìƒì„±\n",
    "csv_files = {\n",
    "    'dept_a.csv': \"date,amount\\n2025-01-01,100000\\n2025-01-02,150000\",\n",
    "    'dept_b.csv': \"date,amount\\n2025-01-01,200000\\n2025-01-02,180000\",\n",
    "    'dept_c.csv': \"date,amount\\n2025-01-01,120000\\n2025-01-02,140000\"\n",
    "}\n",
    "\n",
    "for filename, content in csv_files.items():\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# ë°©ë²• 1: ë‹¨ê³„ë³„ í’€ì´ (ê¶Œì¥)\n",
    "# 1. CSV íŒŒì¼ë“¤ ì½ê¸° ë° íŒŒì‹±\n",
    "all_sales = []\n",
    "\n",
    "for filename in ['dept_a.csv', 'dept_b.csv', 'dept_c.csv']:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # í—¤ë” ê±´ë„ˆë›°ê³  ë°ì´í„°ë§Œ íŒŒì‹±\n",
    "    for line in lines[1:]:\n",
    "        date, amount = line.strip().split(',')\n",
    "        all_sales.append({\n",
    "            'date': date,\n",
    "            'amount': int(amount)\n",
    "        })\n",
    "\n",
    "print(f\"ì´ {len(all_sales)}ê°œì˜ ë§¤ì¶œ ë°ì´í„° ë¡œë“œ\")\n",
    "\n",
    "# 2. ë‚ ì§œë³„ë¡œ ë§¤ì¶œ í•©ì‚°\n",
    "daily_summary = {}\n",
    "\n",
    "for sale in all_sales:\n",
    "    date = sale['date']\n",
    "    amount = sale['amount']\n",
    "    \n",
    "    if date not in daily_summary:\n",
    "        daily_summary[date] = {\n",
    "            'total': 0,\n",
    "            'departments': 0\n",
    "        }\n",
    "    \n",
    "    daily_summary[date]['total'] += amount\n",
    "    daily_summary[date]['departments'] += 1\n",
    "\n",
    "# 3. JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open('report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(daily_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== ì¼ë³„ ë§¤ì¶œ ë¦¬í¬íŠ¸ ===\")\n",
    "print(json.dumps(daily_summary, ensure_ascii=False, indent=2))\n",
    "\n",
    "# 4. ì „ì²´ ê¸°ê°„ ì´ ë§¤ì¶œ ê³„ì‚°\n",
    "total_revenue = sum(day['total'] for day in daily_summary.values())\n",
    "print(f\"\\nì „ì²´ ê¸°ê°„ ì´ ë§¤ì¶œ: {total_revenue:,}ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: í•¨ìˆ˜ë¡œ ëª¨ë“ˆí™” (ì‹¤ë¬´ ë²„ì „)\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def read_csv_sales(file_path):\n",
    "    \"\"\"CSV íŒŒì¼ì—ì„œ ë§¤ì¶œ ë°ì´í„° ì½ê¸°\"\"\"\n",
    "    sales = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        date, amount = line.strip().split(',')\n",
    "        sales.append({'date': date, 'amount': int(amount)})\n",
    "    \n",
    "    return sales\n",
    "\n",
    "def aggregate_by_date(all_sales):\n",
    "    \"\"\"ë‚ ì§œë³„ë¡œ ë§¤ì¶œ ì§‘ê³„\"\"\"\n",
    "    # defaultdict ì‚¬ìš©ìœ¼ë¡œ ì½”ë“œ ê°„ê²°í™”\n",
    "    summary = defaultdict(lambda: {'total': 0, 'departments': 0})\n",
    "    \n",
    "    for sale in all_sales:\n",
    "        date = sale['date']\n",
    "        summary[date]['total'] += sale['amount']\n",
    "        summary[date]['departments'] += 1\n",
    "    \n",
    "    return dict(summary)\n",
    "\n",
    "def create_sales_report(csv_files, output_file):\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\"\"\"\n",
    "    # 1. ëª¨ë“  CSV íŒŒì¼ ì½ê¸°\n",
    "    all_sales = []\n",
    "    for file_path in csv_files:\n",
    "        all_sales.extend(read_csv_sales(file_path))\n",
    "    \n",
    "    # 2. ë‚ ì§œë³„ ì§‘ê³„\n",
    "    summary = aggregate_by_date(all_sales)\n",
    "    \n",
    "    # 3. JSON ì €ì¥\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 4. ì´ ë§¤ì¶œ ê³„ì‚°\n",
    "    total = sum(day['total'] for day in summary.values())\n",
    "    \n",
    "    return summary, total\n",
    "\n",
    "# ì‹¤í–‰\n",
    "files = ['dept_a.csv', 'dept_b.csv', 'dept_c.csv']\n",
    "summary, total = create_sales_report(files, 'report_v2.json')\n",
    "\n",
    "print(\"=== ë§¤ì¶œ ë¦¬í¬íŠ¸ ===\")\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "print(f\"\\nì „ì²´ ì´ ë§¤ì¶œ: {total:,}ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 3: ê³ ê¸‰ - ì œë„ˆë ˆì´í„°ì™€ ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€\n",
    "def read_sales_generator(file_paths):\n",
    "    \"\"\"ì—¬ëŸ¬ CSV íŒŒì¼ì„ ì œë„ˆë ˆì´í„°ë¡œ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\"\"\"\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines[1:]:\n",
    "                try:\n",
    "                    date, amount = line.strip().split(',')\n",
    "                    yield {'date': date, 'amount': int(amount), 'source': file_path}\n",
    "                except ValueError as e:\n",
    "                    print(f\"ë°ì´í„° íŒŒì‹± ì—ëŸ¬ ({file_path}): {line.strip()} - {e}\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "summary = defaultdict(lambda: {'total': 0, 'departments': set()})\n",
    "\n",
    "for sale in read_sales_generator(['dept_a.csv', 'dept_b.csv', 'dept_c.csv']):\n",
    "    date = sale['date']\n",
    "    summary[date]['total'] += sale['amount']\n",
    "    summary[date]['departments'].add(sale['source'])\n",
    "\n",
    "# setì„ ê°œìˆ˜ë¡œ ë³€í™˜\n",
    "final_summary = {\n",
    "    date: {'total': info['total'], 'departments': len(info['departments'])}\n",
    "    for date, info in summary.items()\n",
    "}\n",
    "\n",
    "# ì €ì¥\n",
    "with open('report_v3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(json.dumps(final_summary, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ í•´ì„¤\n",
    "\n",
    "- **ë°ì´í„° íŒŒì´í”„ë¼ì¸**: ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ì›ì‹œ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ëŠ” íë¦„\n",
    "  1. ì¶”ì¶œ(Extract): CSV íŒŒì¼ ì½ê¸°\n",
    "  2. ë³€í™˜(Transform): ë‚ ì§œë³„ ì§‘ê³„\n",
    "  3. ì ì¬(Load): JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "- **`defaultdict`**: í‚¤ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ì„ ìë™ ìƒì„± (ì½”ë“œ ê°„ê²°í™”)\n",
    "- **ì§‘ê³„ íŒ¨í„´**: ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹í™” ë° í•©ì‚°\n",
    "- **ëª¨ë“ˆí™”**: ê° ë‹¨ê³„ë¥¼ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ\n",
    "- **ì œë„ˆë ˆì´í„°**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
    "- **ì—ëŸ¬ ì²˜ë¦¬**: íŒŒì¼ ì—†ìŒ, ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ ë“± ë‹¤ì–‘í•œ ì˜ˆì™¸ ìƒí™© ëŒ€ë¹„\n",
    "- **ì‹¤ë¬´ í™œìš©**:\n",
    "  - ETL (Extract, Transform, Load) íŒŒì´í”„ë¼ì¸\n",
    "  - ì¼ì¼ ë°°ì¹˜ ì‘ì—…\n",
    "  - ë¦¬í¬íŠ¸ ìë™í™”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì¢…í•© ì •ë¦¬\n",
    "\n",
    "### íŒŒì¼ I/Oì™€ ì§ë ¬í™” í•µì‹¬\n",
    "\n",
    "| ì‘ì—… | ë„êµ¬/ë°©ë²• | ì‚¬ìš© ì‹œì  |\n",
    "|------|-----------|----------|\n",
    "| **í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°** | `open('r')` + `read()`/`readlines()` | ë¡œê·¸, ì„¤ì • íŒŒì¼ |\n",
    "| **í…ìŠ¤íŠ¸ íŒŒì¼ ì“°ê¸°** | `open('w')` + `write()` | ê²°ê³¼ ì €ì¥ |\n",
    "| **ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬** | `with` ë¬¸ | í•­ìƒ |\n",
    "| **JSON ì €ì¥/ë¡œë“œ** | `json.dump()`/`json.load()` | API, ì„¤ì •, ë²”ìš© ë°ì´í„° |\n",
    "| **Python ê°ì²´ ì €ì¥** | `pickle.dump()`/`pickle.load()` | ëª¨ë¸, ë³µì¡í•œ ê°ì²´ |\n",
    "| **CSV ì²˜ë¦¬** | ìˆ˜ë™ íŒŒì‹± or `csv` ëª¨ë“ˆ | í‘œ í˜•ì‹ ë°ì´í„° |\n",
    "\n",
    "### ì‹¤ë¬´ Best Practices\n",
    "\n",
    "1. **í•­ìƒ `with` ë¬¸ ì‚¬ìš©**: ìë™ ë¦¬ì†ŒìŠ¤ í•´ì œ\n",
    "2. **ì¸ì½”ë”© ëª…ì‹œ**: `encoding='utf-8'` (í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "3. **ì—ëŸ¬ ì²˜ë¦¬**: `try-except`ë¡œ íŒŒì¼ ì—†ìŒ, í˜•ì‹ ì˜¤ë¥˜ ë“± ëŒ€ë¹„\n",
    "4. **íƒ€ì… ì²´í¬**: JSONì—ì„œ ì½ì€ ë°ì´í„°ì˜ íƒ€ì… í™•ì¸\n",
    "5. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì œë„ˆë ˆì´í„° ì‚¬ìš©\n",
    "6. **ëª¨ë“ˆí™”**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬\n",
    "\n",
    "### JSON vs Pickle ë¹„êµ\n",
    "\n",
    "| íŠ¹ì„± | JSON | Pickle |\n",
    "|------|------|--------|\n",
    "| í˜¸í™˜ì„± | ëª¨ë“  ì–¸ì–´ | Pythonë§Œ |\n",
    "| ê°€ë…ì„± | ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆìŒ | ë°”ì´ë„ˆë¦¬ |\n",
    "| ì§€ì› íƒ€ì… | ê¸°ë³¸ íƒ€ì…ë§Œ | ê±°ì˜ ëª¨ë“  ê°ì²´ |\n",
    "| ë³´ì•ˆ | ì•ˆì „ | ìœ„í—˜ (ì‹ ë¢°ëœ ì†ŒìŠ¤ë§Œ) |\n",
    "| ì†ë„ | ë³´í†µ | ë¹ ë¦„ |\n",
    "\n",
    "### ë‹¤ìŒ í•™ìŠµ\n",
    "\n",
    "- `csv` ëª¨ë“ˆ: CSV ì „ìš© ì²˜ë¦¬ (ë³µì¡í•œ ì¼€ì´ìŠ¤)\n",
    "- `pathlib`: í˜„ëŒ€ì ì¸ ê²½ë¡œ ì²˜ë¦¬\n",
    "- `pandas`: ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ê°•ë ¥í•œ I/O\n",
    "- `sqlite3`: ê²½ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
