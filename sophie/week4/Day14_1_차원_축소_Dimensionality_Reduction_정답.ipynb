{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day14_1: 차원 축소 (Dimensionality Reduction) - 정답 노트북\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1. 데이터 표준화하기 (난이도: 1/5)\n",
    "\n",
    "**문제**: Iris 데이터를 StandardScaler로 표준화하고, 표준화 후 평균과 표준편차를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"표준화 전:\")\n",
    "print(f\"  평균: {X.mean(axis=0).round(2)}\")\n",
    "print(f\"  표준편차: {X.std(axis=0).round(2)}\")\n",
    "\n",
    "print(\"\\n표준화 후:\")\n",
    "print(f\"  평균: {X_scaled.mean(axis=0).round(6)}\")\n",
    "print(f\"  표준편차: {X_scaled.std(axis=0).round(6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert X_scaled.shape == X.shape, \"형태가 동일해야 함\"\n",
    "assert np.allclose(X_scaled.mean(axis=0), 0, atol=1e-10), \"평균은 0에 가까워야 함\"\n",
    "assert np.allclose(X_scaled.std(axis=0), 1, atol=1e-10), \"표준편차는 1에 가까워야 함\"\n",
    "print(\"테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- StandardScaler는 각 특성의 평균을 0, 표준편차를 1로 변환\n",
    "- `fit_transform()`은 학습과 변환을 동시에 수행\n",
    "\n",
    "**핵심 개념**:\n",
    "- 표준화 공식: z = (x - mean) / std\n",
    "- PCA 전 표준화는 필수! (스케일이 다르면 분산 계산이 왜곡됨)\n",
    "\n",
    "**대안**:\n",
    "- `MinMaxScaler`: 0~1 범위로 정규화\n",
    "- `RobustScaler`: 이상치에 강건한 스케일링\n",
    "\n",
    "**흔한 실수**:\n",
    "- fit()만 하고 transform() 안 함\n",
    "- 테스트 데이터에 fit_transform() 사용 (정보 누출)\n",
    "\n",
    "**실무 팁**:\n",
    "- 학습 데이터로 fit(), 테스트 데이터는 transform()만 사용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. PCA 2차원 변환 (난이도: 2/5)\n",
    "\n",
    "**문제**: 표준화된 Iris 데이터를 PCA로 2차원으로 축소하고, 설명된 분산 비율을 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA 적용 (2차원)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 설명된 분산 출력\n",
    "print(f\"PCA 결과 형태: {X_pca.shape}\")\n",
    "print(\"\\n설명된 분산 비율:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "cumulative = sum(pca.explained_variance_ratio_)\n",
    "print(f\"\\n누적 설명 분산: {cumulative:.4f} ({cumulative*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert X_pca.shape == (150, 2), \"2차원으로 축소되어야 함\"\n",
    "assert len(pca.explained_variance_ratio_) == 2, \"2개의 주성분\"\n",
    "assert sum(pca.explained_variance_ratio_) > 0.9, \"90% 이상 설명해야 함\"\n",
    "print(\"테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- PCA(n_components=2)로 2개의 주성분만 추출\n",
    "- explained_variance_ratio_로 각 PC의 설명 분산 확인\n",
    "\n",
    "**핵심 개념**:\n",
    "- PC1은 데이터 분산의 약 73%를 설명\n",
    "- PC1 + PC2 = 약 96%, 4차원 데이터를 2차원으로 압축해도 정보 손실 4%\n",
    "\n",
    "**대안**:\n",
    "- `n_components=0.95`: 95% 분산을 설명하는 최소 성분 수 자동 선택\n",
    "- `svd_solver='randomized'`: 대용량 데이터에서 빠른 근사 계산\n",
    "\n",
    "**흔한 실수**:\n",
    "- 표준화 없이 PCA 적용 (스케일 영향)\n",
    "- n_components를 데이터 차원보다 크게 설정\n",
    "\n",
    "**실무 팁**:\n",
    "- 시각화 목적이면 2~3개, 전처리 목적이면 누적 90~95% 기준으로 선택\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. PCA 시각화 (난이도: 2/5)\n",
    "\n",
    "**문제**: PCA 변환된 Iris 데이터를 px.scatter()로 시각화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "target_names = iris.target_names\n",
    "y = iris.target\n",
    "\n",
    "# DataFrame 생성\n",
    "df_pca = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'Species': [target_names[i] for i in y]\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2', color='Species',\n",
    "    title=f'Iris PCA (설명 분산: {sum(pca.explained_variance_ratio_)*100:.1f}%)',\n",
    "    labels={\n",
    "        'PC1': f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "        'PC2': f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)'\n",
    "    },\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- PCA 결과를 DataFrame으로 변환\n",
    "- Species 컬럼 추가로 색상 구분\n",
    "- px.scatter()로 2D 산점도 생성\n",
    "\n",
    "**핵심 개념**:\n",
    "- 축 라벨에 설명 분산 표시 -> 정보량 시각화\n",
    "- Setosa는 명확히 분리, Versicolor/Virginica는 일부 중첩\n",
    "\n",
    "**대안**:\n",
    "- `px.scatter_3d()`: 3차원 시각화\n",
    "- `hover_data`: 추가 정보 표시\n",
    "\n",
    "**흔한 실수**:\n",
    "- target을 그대로 사용 (0, 1, 2 대신 이름 사용 권장)\n",
    "- 축 라벨에 설명 분산 미표시\n",
    "\n",
    "**실무 팁**:\n",
    "- 시각화 시 항상 설명 분산 비율을 함께 표시하여 정보 손실량 명시\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Scree Plot 그리기 (난이도: 3/5)\n",
    "\n",
    "**문제**: Wine 데이터에 대해 모든 주성분의 설명 분산을 막대그래프로, 누적 분산을 선그래프로 그리세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.datasets import load_wine\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "\n",
    "# 표준화\n",
    "scaler_wine = StandardScaler()\n",
    "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "\n",
    "# 전체 PCA\n",
    "pca_wine = PCA()\n",
    "pca_wine.fit(X_wine_scaled)\n",
    "\n",
    "# 데이터 준비\n",
    "explained_var = pca_wine.explained_variance_ratio_ * 100\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "components = [f'PC{i+1}' for i in range(len(explained_var))]\n",
    "\n",
    "# Scree Plot\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# 개별 분산 (막대)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=components,\n",
    "        y=explained_var,\n",
    "        name='개별 설명 분산',\n",
    "        marker_color='steelblue'\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "# 누적 분산 (선)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=components,\n",
    "        y=cumulative_var,\n",
    "        name='누적 설명 분산',\n",
    "        mode='lines+markers',\n",
    "        marker_color='coral',\n",
    "        line=dict(width=3)\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "# 90% 기준선\n",
    "fig.add_hline(y=90, line_dash=\"dash\", line_color=\"green\",\n",
    "              annotation_text=\"90%\", secondary_y=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Wine Scree Plot',\n",
    "    xaxis_title='주성분',\n",
    "    yaxis_title='개별 분산 (%)',\n",
    "    yaxis2_title='누적 분산 (%)'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 90% 달성 지점\n",
    "n_90 = np.argmax(cumulative_var >= 90) + 1\n",
    "print(f\"90% 분산 설명: {n_90}개 주성분 필요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- `PCA()` (n_components 미지정)으로 모든 주성분 계산\n",
    "- `np.cumsum()`으로 누적 분산 계산\n",
    "- `make_subplots(secondary_y=True)`로 이중 축 생성\n",
    "\n",
    "**핵심 개념**:\n",
    "- Scree Plot: 엘보우 지점에서 주성분 수 결정\n",
    "- 누적 90% 기준: 정보 손실 10% 이하\n",
    "\n",
    "**대안**:\n",
    "- Kaiser 규칙: 고유값 > 1인 성분만 (표준화 데이터)\n",
    "- 교차 검증으로 다운스트림 태스크 성능 기준\n",
    "\n",
    "**흔한 실수**:\n",
    "- 백분율 변환 안 함 (0~1 vs 0~100)\n",
    "- 누적 분산 계산 실수\n",
    "\n",
    "**실무 팁**:\n",
    "- 시각화와 수치 기준을 함께 사용하여 최적 성분 수 결정\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. t-SNE 시각화 (난이도: 3/5)\n",
    "\n",
    "**문제**: Iris 데이터를 t-SNE로 2차원 변환 후 시각화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE 적용\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# DataFrame 생성\n",
    "df_tsne = pd.DataFrame({\n",
    "    'TSNE1': X_tsne[:, 0],\n",
    "    'TSNE2': X_tsne[:, 1],\n",
    "    'Species': [target_names[i] for i in y]\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "fig = px.scatter(\n",
    "    df_tsne, x='TSNE1', y='TSNE2', color='Species',\n",
    "    title='Iris t-SNE (perplexity=30)',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- `TSNE(n_components=2, perplexity=30, random_state=42)` 설정\n",
    "- `fit_transform()`으로 변환 (t-SNE는 transform() 단독 불가)\n",
    "\n",
    "**핵심 개념**:\n",
    "- t-SNE는 지역 구조를 잘 보존 -> 군집 분리가 더 뚜렷\n",
    "- perplexity: 각 점이 고려하는 이웃 수 (보통 5~50)\n",
    "\n",
    "**대안**:\n",
    "- UMAP: t-SNE보다 빠르고 전역 구조도 보존\n",
    "- 3D t-SNE: `n_components=3`\n",
    "\n",
    "**흔한 실수**:\n",
    "- random_state 미설정 -> 매번 다른 결과\n",
    "- transform() 단독 호출 시도\n",
    "\n",
    "**실무 팁**:\n",
    "- 대용량 데이터는 샘플링 후 t-SNE 적용 (계산 비용 높음)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. perplexity 비교 (난이도: 3/5)\n",
    "\n",
    "**문제**: perplexity를 [5, 15, 30, 50]으로 변경하며 t-SNE 결과를 2x2 서브플롯으로 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "perplexities = [5, 15, 30, 50]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[f'perplexity={p}' for p in perplexities]\n",
    ")\n",
    "\n",
    "colors = {'setosa': 'blue', 'versicolor': 'green', 'virginica': 'red'}\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # t-SNE 변환\n",
    "    tsne_temp = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=500)\n",
    "    X_temp = tsne_temp.fit_transform(X_scaled)\n",
    "    \n",
    "    # 각 종별 플롯\n",
    "    for species_name in target_names:\n",
    "        mask = [target_names[i] == species_name for i in y]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_temp[mask, 0],\n",
    "                y=X_temp[mask, 1],\n",
    "                mode='markers',\n",
    "                name=species_name,\n",
    "                marker=dict(color=colors[species_name], size=6, opacity=0.7),\n",
    "                showlegend=(idx == 0)\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='t-SNE perplexity 비교',\n",
    "    height=600,\n",
    "    legend=dict(x=1.02, y=0.5)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- 4개의 perplexity 값에 대해 반복\n",
    "- `make_subplots(rows=2, cols=2)`로 2x2 그리드\n",
    "- `row = idx // 2 + 1, col = idx % 2 + 1`로 위치 계산\n",
    "\n",
    "**핵심 개념**:\n",
    "- 낮은 perplexity: 지역 구조 강조, 작은 군집\n",
    "- 높은 perplexity: 전역 구조 강조, 큰 군집\n",
    "\n",
    "**대안**:\n",
    "- n_iter 조정: 반복 횟수 (기본 1000)\n",
    "- learning_rate 조정: 학습률\n",
    "\n",
    "**흔한 실수**:\n",
    "- 서브플롯 위치 계산 오류\n",
    "- showlegend 중복 (첫 번째만 True)\n",
    "\n",
    "**실무 팁**:\n",
    "- 데이터 크기에 따라 perplexity 조정: 작은 데이터(5~15), 큰 데이터(30~50)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. SelectKBest 적용 (난이도: 4/5)\n",
    "\n",
    "**문제**: Wine 데이터에서 SelectKBest로 상위 5개 특성을 선택하고, 각 특성의 F-score를 막대그래프로 시각화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "wine_features = wine.feature_names\n",
    "\n",
    "# SelectKBest 적용\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_kbest = selector.fit_transform(X_wine, y_wine)\n",
    "\n",
    "# 선택된 특성\n",
    "selected = np.array(wine_features)[selector.get_support()]\n",
    "print(f\"선택된 특성: {list(selected)}\")\n",
    "\n",
    "# F-score 시각화\n",
    "df_scores = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'F-Score': selector.scores_\n",
    "}).sort_values('F-Score', ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    df_scores, x='F-Score', y='Feature', orientation='h',\n",
    "    title='SelectKBest: F-Score (ANOVA)',\n",
    "    color='F-Score',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(categoryorder='total ascending'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- `SelectKBest(score_func=f_classif, k=5)`로 ANOVA F-검정 기반 선택\n",
    "- `selector.get_support()`로 선택된 특성 마스크 획득\n",
    "- `selector.scores_`로 각 특성의 점수 획득\n",
    "\n",
    "**핵심 개념**:\n",
    "- f_classif: 분류 문제용 ANOVA F-검정\n",
    "- F-score가 높을수록 그룹 간 분산이 큼 (분류에 유용)\n",
    "\n",
    "**대안**:\n",
    "- `mutual_info_classif`: 상호 정보량 기반 (비선형 관계 포착)\n",
    "- `chi2`: 카이제곱 검정 (비음수 데이터)\n",
    "\n",
    "**흔한 실수**:\n",
    "- 회귀 문제에 f_classif 사용 (f_regression 사용해야 함)\n",
    "- score_func 미지정\n",
    "\n",
    "**실무 팁**:\n",
    "- 빠른 초기 탐색에 적합, 최종 선택은 모델 기반 방법으로\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. RFE 적용 (난이도: 4/5)\n",
    "\n",
    "**문제**: Logistic Regression을 기반으로 RFE를 적용하여 상위 5개 특성을 선택하고, 특성별 순위를 막대그래프로 시각화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# RFE 적용\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=model, n_features_to_select=5, step=1)\n",
    "X_rfe = rfe.fit_transform(X_wine, y_wine)\n",
    "\n",
    "# 선택된 특성\n",
    "rfe_features = np.array(wine_features)[rfe.support_]\n",
    "print(f\"선택된 특성: {list(rfe_features)}\")\n",
    "\n",
    "# 순위 시각화\n",
    "df_rfe = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'Ranking': rfe.ranking_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "fig = px.bar(\n",
    "    df_rfe, x='Ranking', y='Feature', orientation='h',\n",
    "    title='RFE: 특성 순위 (1=선택됨)',\n",
    "    color='Ranking',\n",
    "    color_continuous_scale='Reds_r'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(categoryorder='total descending'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- `RFE(estimator=model, n_features_to_select=5)`로 설정\n",
    "- step=1: 매 반복마다 1개씩 제거 (정밀하지만 느림)\n",
    "- `rfe.ranking_`: 1=선택됨, 숫자 클수록 먼저 제거됨\n",
    "\n",
    "**핵심 개념**:\n",
    "- RFE: 모델을 반복 학습하며 가장 덜 중요한 특성을 제거\n",
    "- 모델의 coef_ 또는 feature_importances_를 기준으로 중요도 평가\n",
    "\n",
    "**대안**:\n",
    "- `RFECV`: 교차 검증으로 최적 특성 수 자동 결정\n",
    "- step > 1: 빠른 실행 (정밀도 감소)\n",
    "\n",
    "**흔한 실수**:\n",
    "- coef_ 없는 모델 사용 (KNN 등)\n",
    "- n_features_to_select > 전체 특성 수\n",
    "\n",
    "**실무 팁**:\n",
    "- 대용량 데이터는 step 값을 높여 속도 향상\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Feature Importance 비교 (난이도: 5/5)\n",
    "\n",
    "**문제**: Random Forest의 feature_importances_를 추출하고, SelectKBest, RFE와 함께 세 가지 방법의 상위 5개 특성을 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_wine, y_wine)\n",
    "\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# 상위 5개\n",
    "rf_top5 = df_importance.head(5)['Feature'].tolist()\n",
    "\n",
    "# SelectKBest 상위 5개\n",
    "kbest_top5 = df_scores.nlargest(5, 'F-Score')['Feature'].tolist()\n",
    "\n",
    "# RFE 상위 5개\n",
    "rfe_top5 = list(rfe_features)\n",
    "\n",
    "# 비교 출력\n",
    "print(\"특성 선택 방법 비교 (상위 5개):\")\n",
    "print(f\"  SelectKBest: {kbest_top5}\")\n",
    "print(f\"  RFE:         {rfe_top5}\")\n",
    "print(f\"  RF Importance: {rf_top5}\")\n",
    "\n",
    "# 공통 특성\n",
    "common = set(kbest_top5) & set(rfe_top5) & set(rf_top5)\n",
    "print(f\"\\n모든 방법에서 공통: {list(common)}\")\n",
    "\n",
    "# RF Importance 시각화\n",
    "fig = px.bar(\n",
    "    df_importance.sort_values('Importance', ascending=True),\n",
    "    x='Importance', y='Feature', orientation='h',\n",
    "    title='Random Forest: Feature Importance',\n",
    "    color='Importance',\n",
    "    color_continuous_scale='Greens'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "- Random Forest 학습 후 `feature_importances_` 추출\n",
    "- 각 방법의 상위 5개를 리스트로 저장\n",
    "- 집합(set)의 교집합(&)으로 공통 특성 찾기\n",
    "\n",
    "**핵심 개념**:\n",
    "- Feature Importance: 각 특성이 분할에 기여한 정도 (불순도 감소량)\n",
    "- 여러 방법의 공통 특성 = 가장 신뢰도 높은 특성\n",
    "\n",
    "**대안**:\n",
    "- Permutation Importance: 더 신뢰할 수 있는 중요도\n",
    "- SHAP: 개별 예측에 대한 기여도\n",
    "\n",
    "**흔한 실수**:\n",
    "- 고카디널리티 특성에 높은 중요도 (편향)\n",
    "- 상관 특성 간 중요도 분산\n",
    "\n",
    "**실무 팁**:\n",
    "- 여러 방법을 앙상블하여 특성 선택의 신뢰도 향상\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. 종합 문제: 차원 축소 파이프라인 (난이도: 5/5)\n",
    "\n",
    "**문제**: MNIST 데이터에 대해 PCA로 90% 분산 설명 주성분 수를 찾고, 원본 vs 축소 데이터의 분류 정확도를 비교하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 코드\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"MNIST 데이터: {X_digits.shape[0]}개 샘플, {X_digits.shape[1]}개 특성\")\n",
    "\n",
    "# Step 1: 표준화\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "# Step 2: 전체 PCA로 설명 분산 분석\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_digits_scaled)\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# 90% 분산 설명 주성분 수\n",
    "n_90 = np.argmax(cumvar >= 0.90) + 1\n",
    "print(f\"\\n90% 분산 설명: {n_90}개 주성분 (64 -> {n_90}, {(1-n_90/64)*100:.1f}% 압축)\")\n",
    "\n",
    "# Step 3: 해당 수로 차원 축소\n",
    "pca_90 = PCA(n_components=n_90)\n",
    "X_digits_pca = pca_90.fit_transform(X_digits_scaled)\n",
    "\n",
    "# Step 4: 분류 정확도 비교\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# 원본 (64차원)\n",
    "score_original = cross_val_score(model, X_digits_scaled, y_digits, cv=5).mean()\n",
    "\n",
    "# PCA 축소 (n_90차원)\n",
    "score_pca = cross_val_score(model, X_digits_pca, y_digits, cv=5).mean()\n",
    "\n",
    "print(f\"\\n로지스틱 회귀 교차검증 정확도:\")\n",
    "print(f\"  원본 (64차원): {score_original:.4f}\")\n",
    "print(f\"  PCA ({n_90}차원): {score_pca:.4f}\")\n",
    "print(f\"  차이: {(score_pca - score_original)*100:.2f}%p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert n_90 < 64, \"차원이 축소되어야 함\"\n",
    "assert X_digits_pca.shape[1] == n_90, \"PCA 결과 차원 확인\"\n",
    "assert score_pca > 0.9, \"정확도 90% 이상이어야 함\"\n",
    "print(\"테스트 통과!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가: 차원별 정확도 변화 시각화\n",
    "n_components_list = [5, 10, 15, 20, 25, 30, 40, 50, 64]\n",
    "scores = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    if n < 64:\n",
    "        pca_temp = PCA(n_components=n)\n",
    "        X_temp = pca_temp.fit_transform(X_digits_scaled)\n",
    "    else:\n",
    "        X_temp = X_digits_scaled\n",
    "    score = cross_val_score(model, X_temp, y_digits, cv=5).mean()\n",
    "    scores.append(score)\n",
    "\n",
    "fig = px.line(\n",
    "    x=n_components_list, y=scores,\n",
    "    markers=True,\n",
    "    title='PCA 차원 수에 따른 분류 정확도',\n",
    "    labels={'x': '주성분 수', 'y': '정확도'}\n",
    ")\n",
    "fig.add_vline(x=n_90, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=f\"90% 분산 ({n_90}개)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**:\n",
    "1. 데이터 표준화\n",
    "2. 전체 PCA로 누적 분산 계산\n",
    "3. `np.argmax(cumvar >= 0.9) + 1`로 90% 달성 지점 찾기\n",
    "4. 해당 차원으로 PCA 변환\n",
    "5. cross_val_score()로 원본/축소 비교\n",
    "\n",
    "**핵심 개념**:\n",
    "- 64차원 -> 21차원 (약 67% 압축)으로도 정확도 유지/향상 가능\n",
    "- 차원 축소로 노이즈 제거, 과적합 방지 효과\n",
    "\n",
    "**대안**:\n",
    "- `PCA(n_components=0.9)`: 90% 분산 설명 자동 선택\n",
    "- GridSearchCV로 최적 차원 수 탐색\n",
    "\n",
    "**흔한 실수**:\n",
    "- argmax 후 +1 누락 (0-indexed)\n",
    "- 테스트 데이터에 fit_transform() 사용\n",
    "\n",
    "**실무 팁**:\n",
    "- PCA 전처리는 학습 속도 향상 + 과적합 방지에 효과적\n",
    "- 차원-성능 그래프로 최적점 시각화\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "\n",
    "### Part 1: PCA 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 코드 |\n",
    "|------|----------|------|\n",
    "| 차원의 저주 | 고차원에서 거리 의미 상실, 과적합 위험 | - |\n",
    "| PCA 원리 | 분산 최대화 축 찾기, 직교 변환 | `PCA(n_components=k)` |\n",
    "| 설명 분산 | 각 PC가 원본 정보를 얼마나 보존 | `pca.explained_variance_ratio_` |\n",
    "| 주성분 선택 | 누적 90%+ 또는 엘보우 규칙 | `np.cumsum()` |\n",
    "| PCA 로딩 | 원본 특성과 PC의 관계 | `pca.components_` |\n",
    "\n",
    "### Part 2: t-SNE와 특성 선택 핵심 요약\n",
    "\n",
    "| 방법 | 특징 | 코드 |\n",
    "|------|------|------|\n",
    "| t-SNE | 비선형, 시각화 전용, 지역 구조 보존 | `TSNE(perplexity=30)` |\n",
    "| SelectKBest | 통계적 필터링 (F-test) | `SelectKBest(f_classif, k=5)` |\n",
    "| RFE | 모델 기반 재귀적 제거 | `RFE(estimator, n_features_to_select)` |\n",
    "| Feature Importance | 트리 기반 중요도 | `rf.feature_importances_` |\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **PCA 전 반드시 표준화**: 스케일 차이가 크면 결과 왜곡\n",
    "2. **t-SNE는 시각화 전용**: 새 데이터 변환 불가, 재현성 위해 random_state 설정\n",
    "3. **특성 선택 앙상블**: 여러 방법의 공통 특성이 가장 신뢰도 높음\n",
    "4. **perplexity 튜닝**: 데이터 크기에 따라 조정 (작은 데이터는 작은 값)\n",
    "5. **교차 검증 필수**: 특성 수에 따른 성능 변화 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
