{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day18_0: Transformer ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "**Part 1: ê¸°ì´ˆ**\n",
        "1. Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í•„ìš”ì„± ì´í•´í•˜ê¸°\n",
        "2. Self-Attention ê°œë… ì´í•´í•˜ê¸°\n",
        "3. Query, Key, Value ì´í•´í•˜ê¸°\n",
        "4. Scaled Dot-Product Attention êµ¬í˜„í•˜ê¸°\n",
        "5. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”í•˜ê¸°\n",
        "\n",
        "**Part 2: ì‹¬í™”**\n",
        "1. Multi-Head Attention ì´í•´í•˜ê¸°\n",
        "2. Positional Encoding ì´í•´í•˜ê¸°\n",
        "3. Transformer Encoder êµ¬ì¡° ì´í•´í•˜ê¸°\n",
        "4. PyTorch nn.Transformer í™œìš©í•˜ê¸°\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ì™œ ì´ê²ƒì„ ë°°ìš°ë‚˜ìš”?\n",
        "\n",
        "| ê°œë… | ì‹¤ë¬´ í™œìš© | ì˜ˆì‹œ |\n",
        "|------|----------|------|\n",
        "| Attention | ì‹œí€€ìŠ¤ ë‚´ ê´€ê³„ íŒŒì•… | ê¸°ê³„ ë²ˆì—­, ìš”ì•½, QA |\n",
        "| Self-Attention | ë¬¸ë§¥ ì´í•´ | \"it\"ì´ ë¬´ì—‡ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ íŒŒì•… |\n",
        "| Multi-Head | ë‹¤ì–‘í•œ ê´€ì  ë¶„ì„ | ë¬¸ë²•ì  ê´€ê³„, ì˜ë¯¸ì  ê´€ê³„ ë™ì‹œ íŒŒì•… |\n",
        "| Transformer | í˜„ëŒ€ NLPì˜ ê¸°ë°˜ | GPT, BERT, LLMì˜ í•µì‹¬ |\n",
        "\n",
        "**ë¶„ì„ê°€ ê´€ì **: TransformerëŠ” í˜„ëŒ€ AIì˜ í•µì‹¬ì…ë‹ˆë‹¤. ChatGPT, Claude, Gemini ëª¨ë‘ Transformer ê¸°ë°˜ì´ë©°, NLPë¶€í„° ì´ë¯¸ì§€, ë¹„ë””ì˜¤ê¹Œì§€ ê±°ì˜ ëª¨ë“  ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: ê¸°ì´ˆ\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 ì™œ Attentionì¸ê°€? - RNNì˜ í•œê³„\n",
        "\n",
        "### RNN/LSTMì˜ í•œê³„\n",
        "\n",
        "Day 17ì—ì„œ ë°°ìš´ RNN, LSTM, GRUëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì˜ ì²˜ë¦¬í•˜ì§€ë§Œ, ì¹˜ëª…ì ì¸ ì•½ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "| ë¬¸ì œ | ì„¤ëª… | ê²°ê³¼ |\n",
        "|-----|------|------|\n",
        "| ìˆœì°¨ ì²˜ë¦¬ | t ì‹œì ì´ ëë‚˜ì•¼ t+1 ì²˜ë¦¬ ê°€ëŠ¥ | GPU ë³‘ë ¬í™” ë¶ˆê°€, í•™ìŠµ ëŠë¦¼ |\n",
        "| ì¥ê¸° ì˜ì¡´ì„± | ì •ë³´ê°€ í•œ ë‹¨ê³„ì”© ì „ë‹¬ | ê¸´ ë¬¸ì¥ì—ì„œ ì• ì •ë³´ ì†ì‹¤ |\n",
        "| ê³ ì • Context | ë§ˆì§€ë§‰ hidden stateë§Œ ì‚¬ìš© | ì •ë³´ ë³‘ëª© í˜„ìƒ |\n",
        "\n",
        "### ì˜ˆì‹œ: \"it\"ì€ ë¬´ì—‡ì„ ê°€ë¦¬í‚¬ê¹Œ?\n",
        "\n",
        "```\n",
        "\"The animal didn't cross the street because it was too tired.\"\n",
        "```\n",
        "\n",
        "- \"it\"ì´ \"animal\"ì„ ê°€ë¦¬í‚¨ë‹¤ëŠ” ê²ƒì„ RNNì´ ì–´ë–»ê²Œ ì•Œê¹Œìš”?\n",
        "- ë‘ ë‹¨ì–´ê°€ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìœ¼ë©´, ì—°ê²° ê³ ë¦¬ê°€ í¬ë¯¸í•´ì§‘ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RNNì˜ ìˆœì°¨ ì²˜ë¦¬ ë¬¸ì œ ì‹œê°í™”\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# ë¬¸ì¥ ì˜ˆì‹œ\n",
        "sentence = \"The animal didn't cross the street because it was too tired\".split()\n",
        "print(f\"ë¬¸ì¥: {sentence}\")\n",
        "print(f\"ë‹¨ì–´ ìˆ˜: {len(sentence)}\")\n",
        "print(f\"\\n'it' ìœ„ì¹˜: {sentence.index('it')}\")\n",
        "print(f\"'animal' ìœ„ì¹˜: {sentence.index('animal')}\")\n",
        "print(f\"\\nê±°ë¦¬: {sentence.index('it') - sentence.index('animal')} ë‹¨ì–´ ë–¨ì–´ì ¸ ìˆìŒ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformerì˜ í•´ê²°ì±…: Attention\n",
        "\n",
        "**\"Attention Is All You Need\"** (2017, Google)\n",
        "\n",
        "TransformerëŠ” RNNì˜ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë²„ë¦¬ê³ , **Attention**ë§Œìœ¼ë¡œ ëª¨ë“  ë‹¨ì–´ ê´€ê³„ë¥¼ í•œ ë²ˆì— íŒŒì•…í•©ë‹ˆë‹¤.\n",
        "\n",
        "| íŠ¹ì§• | RNN | Transformer |\n",
        "|-----|-----|-------------|\n",
        "| ì²˜ë¦¬ ë°©ì‹ | ìˆœì°¨ì  | ë³‘ë ¬ |\n",
        "| ì¥ê±°ë¦¬ ì˜ì¡´ì„± | ì–´ë ¤ì›€ | ì§ì ‘ ì—°ê²° |\n",
        "| í•™ìŠµ ì†ë„ | ëŠë¦¼ | ë¹ ë¦„ |\n",
        "| ë©”ëª¨ë¦¬ | O(n) | O(n^2) |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RNN vs Transformer ë¹„êµ\n",
        "print(\"RNN ì²˜ë¦¬ ë°©ì‹:\")\n",
        "print(\"ë‹¨ì–´1 -> ë‹¨ì–´2 -> ë‹¨ì–´3 -> ... -> ë‹¨ì–´n\")\n",
        "print(\"(ìˆœì°¨ì ìœ¼ë¡œ í•˜ë‚˜ì”© ì²˜ë¦¬)\\n\")\n",
        "\n",
        "print(\"Transformer ì²˜ë¦¬ ë°©ì‹:\")\n",
        "print(\"ë‹¨ì–´1 â†â†’ ë‹¨ì–´2 â†â†’ ë‹¨ì–´3 â†â†’ ... â†â†’ ë‹¨ì–´n\")\n",
        "print(\"(ëª¨ë“  ë‹¨ì–´ê°€ ë™ì‹œì— ì„œë¡œ ì°¸ì¡°)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.2 Self-Attention ê°œë…\n",
        "\n",
        "### Self-Attentionì´ë€?\n",
        "\n",
        "**Self-Attention**: ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ê° ìš”ì†Œê°€ ë‹¤ë¥¸ ëª¨ë“  ìš”ì†Œì™€ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜\n",
        "\n",
        "```\n",
        "\"The cat sat on the mat\"\n",
        "\n",
        "\"cat\"ì˜ ê´€ì ì—ì„œ:\n",
        "- \"The\"ì™€ì˜ ê´€ê³„: 0.1 (ê´€ì‚¬)\n",
        "- \"sat\"ê³¼ì˜ ê´€ê³„: 0.4 (ì£¼ì–´-ë™ì‚¬)\n",
        "- \"on\"ê³¼ì˜ ê´€ê³„: 0.1\n",
        "- \"the\"ì™€ì˜ ê´€ê³„: 0.1\n",
        "- \"mat\"ê³¼ì˜ ê´€ê³„: 0.3 (ì˜ë¯¸ì  ì—°ê´€)\n",
        "```\n",
        "\n",
        "### ë¹„ìœ : íšŒì˜ì—ì„œ ë°œí‘œí•˜ê¸°\n",
        "\n",
        "| ì—­í•  | ì„¤ëª… |\n",
        "|-----|------|\n",
        "| ë°œí‘œì (Query) | \"ë‚˜ëŠ” ì´ëŸ° ì •ë³´ê°€ í•„ìš”í•´\" |\n",
        "| ì²­ì¤‘ (Key) | \"ë‚˜ëŠ” ì´ëŸ° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´\" |\n",
        "| ë‚´ìš© (Value) | \"ê·¸ ì •ë³´ì˜ ì‹¤ì œ ë‚´ìš©ì€ ì´ê±°ì•¼\" |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Self-Attention ê°œë… ì‹œë®¬ë ˆì´ì…˜\n",
        "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "print(\"ë¬¸ì¥:\", \" \".join(words))\n",
        "print(\"\\nê° ë‹¨ì–´ëŠ” ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ì˜ 'ê´€ë ¨ë„'ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\")\n",
        "print(\"\\nì˜ˆ: 'cat'ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì–¼ë§ˆë‚˜ 'ì£¼ëª©'í•´ì•¼ í•˜ëŠ”ì§€\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.3 Query, Key, Value ì´í•´í•˜ê¸°\n",
        "\n",
        "### Q, K, Vë€?\n",
        "\n",
        "ê° ë‹¨ì–´ëŠ” 3ê°€ì§€ ë²¡í„°ë¡œ ë³€í™˜ë©ë‹ˆë‹¤:\n",
        "\n",
        "| ë²¡í„° | ì—­í•  | ë¹„ìœ  |\n",
        "|-----|------|------|\n",
        "| **Query (Q)** | ë‚´ê°€ ì°¾ëŠ” ì •ë³´ | ê²€ìƒ‰ì–´ |\n",
        "| **Key (K)** | ë‚´ê°€ ê°€ì§„ ì •ë³´ì˜ ì œëª© | ë¬¸ì„œ ì œëª© |\n",
        "| **Value (V)** | ë‚´ê°€ ê°€ì§„ ì‹¤ì œ ì •ë³´ | ë¬¸ì„œ ë‚´ìš© |\n",
        "\n",
        "### ê²€ìƒ‰ ì—”ì§„ ë¹„ìœ \n",
        "\n",
        "```\n",
        "ê²€ìƒ‰ì–´ (Query): \"ê³ ì–‘ì´ ì‚¬ì§„\"\n",
        "   â†“\n",
        "ë¬¸ì„œë“¤ì˜ Keyì™€ ë¹„êµ\n",
        "   â†“\n",
        "ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œì˜ Value ê°€ì ¸ì˜¤ê¸°\n",
        "```\n",
        "\n",
        "### ìˆ˜ì‹\n",
        "\n",
        "$$Q = X \\cdot W_Q$$\n",
        "$$K = X \\cdot W_K$$\n",
        "$$V = X \\cdot W_V$$\n",
        "\n",
        "- $X$: ì…ë ¥ ì„ë² ë”© (ì‹œí€€ìŠ¤ ê¸¸ì´ x ì„ë² ë”© ì°¨ì›)\n",
        "- $W_Q, W_K, W_V$: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q, K, V ìƒì„± ê³¼ì • ì‹œê°í™”\n",
        "embed_dim = 64\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# ì…ë ¥ ì„ë² ë”© (ë‹¨ì–´ë“¤ì˜ ë²¡í„° í‘œí˜„)\n",
        "X = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Q, K, V ë³€í™˜ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´\n",
        "W_Q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "W_K = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "W_V = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "# Q, K, V ìƒì„±\n",
        "Q = W_Q(X)\n",
        "K = W_K(X)\n",
        "V = W_V(X)\n",
        "\n",
        "print(f\"ì…ë ¥ X shape: {X.shape}\")\n",
        "print(f\"Query Q shape: {Q.shape}\")\n",
        "print(f\"Key K shape: {K.shape}\")\n",
        "print(f\"Value V shape: {V.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.4 Scaled Dot-Product Attention êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "### ì–´í…ì…˜ ê³„ì‚° 4ë‹¨ê³„\n",
        "\n",
        "```\n",
        "1. Score ê³„ì‚°: Q @ K^T\n",
        "2. ìŠ¤ì¼€ì¼ë§: Score / sqrt(d_k)\n",
        "3. ê°€ì¤‘ì¹˜ ê³„ì‚°: softmax(Score)\n",
        "4. ê°€ì¤‘í•©: weights @ V\n",
        "```\n",
        "\n",
        "### ìˆ˜ì‹\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "### ì™œ ìŠ¤ì¼€ì¼ë§?\n",
        "\n",
        "- Qì™€ Kì˜ ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì§€ë©´ softmaxê°€ ê·¹ë‹¨ì ì¸ ê°’ (0 ë˜ëŠ” 1)ì„ ì¶œë ¥\n",
        "- $\\sqrt{d_k}$ë¡œ ë‚˜ëˆ ì„œ ê°’ì˜ ë¶„ì‚°ì„ ì•ˆì •í™”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention êµ¬í˜„\n",
        "    \n",
        "    Args:\n",
        "        Q: Query (batch, seq_len, d_k)\n",
        "        K: Key (batch, seq_len, d_k)\n",
        "        V: Value (batch, seq_len, d_v)\n",
        "    \n",
        "    Returns:\n",
        "        output: ì–´í…ì…˜ ì ìš© ê²°ê³¼\n",
        "        attention_weights: ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)  # Key ì°¨ì›\n",
        "    \n",
        "    # 1. Score ê³„ì‚°: Q @ K^T\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    print(f\"1. Score (Q @ K^T) shape: {scores.shape}\")\n",
        "    \n",
        "    # 2. ìŠ¤ì¼€ì¼ë§: Score / sqrt(d_k)\n",
        "    scaled_scores = scores / math.sqrt(d_k)\n",
        "    print(f\"2. Scaled Score shape: {scaled_scores.shape}\")\n",
        "    \n",
        "    # 3. ê°€ì¤‘ì¹˜ ê³„ì‚°: softmax\n",
        "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
        "    print(f\"3. Attention Weights shape: {attention_weights.shape}\")\n",
        "    \n",
        "    # 4. ê°€ì¤‘í•©: weights @ V\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    print(f\"4. Output shape: {output.shape}\")\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "print(\"=\" * 50)\n",
        "print(\"Scaled Dot-Product Attention ë‹¨ê³„ë³„ ì‹¤í–‰\")\n",
        "print(\"=\" * 50)\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self-Attention ì•„í‚¤í…ì²˜\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len, embed_dim)\"] --> Q[\"Q Linear<br/>embed_dim â†’ embed_dim\"]\n",
        "    Input --> K[\"K Linear<br/>embed_dim â†’ embed_dim\"]\n",
        "    Input --> V[\"V Linear<br/>embed_dim â†’ embed_dim\"]\n",
        "    Q --> Scores[\"Scores<br/>Q @ K^T / sqrt(d_k)\"]\n",
        "    K --> Scores\n",
        "    Scores --> Softmax[\"Softmax\"]\n",
        "    Softmax --> Attn[\"Attention Weights\"]\n",
        "    V --> Output[\"Output<br/>Attn @ V\"]\n",
        "    Attn --> Output\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style Q fill:#ffffff,color:#000000\n",
        "    style K fill:#ffffff,color:#000000\n",
        "    style V fill:#ffffff,color:#000000\n",
        "    style Scores fill:#ffffff,color:#000000\n",
        "    style Softmax fill:#ffffff,color:#000000\n",
        "    style Attn fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Self-Attention í´ë˜ìŠ¤ë¡œ ì •ë¦¬\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Q, K, V ë³€í™˜ ë ˆì´ì–´\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, embed_dim)\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        \n",
        "        # Scaled Dot-Product Attention\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "attention = SelfAttention(embed_dim=64)\n",
        "x = torch.randn(1, 6, 64)  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
        "print(f\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ shape: {weights.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.5 ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™”í•˜ê¸°\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ì–´í…ì…˜ ì‹œê°í™”ë¡œ ëª¨ë¸ í•´ì„\n",
        "\n",
        "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ë©´ ëª¨ë¸ì´ ì–´ë–¤ ë‹¨ì–´ì— ì£¼ëª©í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# ì˜ˆì‹œ ë¬¸ì¥\n",
        "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "# Self-Attention ê³„ì‚°\n",
        "attention = SelfAttention(embed_dim=64)\n",
        "x = torch.randn(1, len(words), 64)\n",
        "_, attention_weights = attention(x)\n",
        "\n",
        "# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\n",
        "attn_matrix = attention_weights.squeeze().detach().numpy()\n",
        "\n",
        "# Plotly íˆíŠ¸ë§µ ì‹œê°í™”\n",
        "fig = px.imshow(\n",
        "    attn_matrix,\n",
        "    labels=dict(x=\"Key (ì°¸ì¡°ë˜ëŠ” ë‹¨ì–´)\", y=\"Query (ì£¼ëª©í•˜ëŠ” ë‹¨ì–´)\", color=\"ê°€ì¤‘ì¹˜\"),\n",
        "    x=words,\n",
        "    y=words,\n",
        "    color_continuous_scale=\"Blues\",\n",
        "    aspect=\"equal\"\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Self-Attention ê°€ì¤‘ì¹˜ íˆíŠ¸ë§µ\",\n",
        "    xaxis_title=\"Key (ì°¸ì¡°ë˜ëŠ” ë‹¨ì–´)\",\n",
        "    yaxis_title=\"Query (ì£¼ëª©í•˜ëŠ” ë‹¨ì–´)\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# íŠ¹ì • ë‹¨ì–´ì˜ ì–´í…ì…˜ ë¶„í¬ ë§‰ëŒ€ ê·¸ë˜í”„\n",
        "target_word = \"cat\"\n",
        "target_idx = words.index(target_word)\n",
        "\n",
        "# target_wordê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì£¼ëª©í•˜ëŠ” ì •ë„\n",
        "attn_for_target = attn_matrix[target_idx]\n",
        "\n",
        "fig = px.bar(\n",
        "    x=words,\n",
        "    y=attn_for_target,\n",
        "    labels={\"x\": \"ë‹¨ì–´\", \"y\": \"ì–´í…ì…˜ ê°€ì¤‘ì¹˜\"},\n",
        "    title=f\"'{target_word}'ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì£¼ëª©í•˜ëŠ” ì •ë„\",\n",
        "    color=attn_for_target,\n",
        "    color_continuous_scale=\"Blues\"\n",
        ")\n",
        "\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: ì‹¬í™”\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Multi-Head Attention ì´í•´í•˜ê¸°\n",
        "\n",
        "### ì™œ Multi-Head?\n",
        "\n",
        "í•˜ë‚˜ì˜ ì–´í…ì…˜ë§Œìœ¼ë¡œëŠ” ë¬¸ì¥ì˜ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ëª¨ë‘ í¬ì°©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
        "\n",
        "| Head | í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê´€ê³„ |\n",
        "|------|-------------------|\n",
        "| Head 1 | ì£¼ì–´-ë™ì‚¬ ê´€ê³„ |\n",
        "| Head 2 | ìˆ˜ì‹ì–´-í”¼ìˆ˜ì‹ì–´ |\n",
        "| Head 3 | ëŒ€ëª…ì‚¬-ì„ í–‰ì‚¬ |\n",
        "| Head 4 | ì˜ë¯¸ì  ìœ ì‚¬ì„± |\n",
        "\n",
        "### Multi-Head Attention êµ¬ì¡°\n",
        "\n",
        "```\n",
        "1. ì…ë ¥ì„ ì—¬ëŸ¬ Headë¡œ ë¶„í• \n",
        "2. ê° Headê°€ ë…ë¦½ì ìœ¼ë¡œ Attention ìˆ˜í–‰\n",
        "3. ëª¨ë“  Head ê²°ê³¼ë¥¼ Concat\n",
        "4. ì„ í˜• ë³€í™˜ìœ¼ë¡œ ì›ë˜ ì°¨ì› ë³µì›\n",
        "```\n",
        "\n",
        "### ìˆ˜ì‹\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$$\n",
        "$$\\text{where } head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Head Attention ì•„í‚¤í…ì²˜\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len, embed_dim)\"] --> QKV[\"Q/K/V Linear<br/>embed_dim â†’ embed_dim\"]\n",
        "    QKV --> Split[\"Split into<br/>num_heads\"]\n",
        "    Split --> Head1[\"Head 1<br/>Attention\"]\n",
        "    Split --> Head2[\"Head 2<br/>Attention\"]\n",
        "    Split --> HeadN[\"Head N<br/>Attention\"]\n",
        "    Head1 --> Concat[\"Concat<br/>All Heads\"]\n",
        "    Head2 --> Concat\n",
        "    HeadN --> Concat\n",
        "    Concat --> FC[\"Linear<br/>embed_dim â†’ embed_dim\"]\n",
        "    FC --> Output[\"Output<br/>(batch, seq_len, embed_dim)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style QKV fill:#ffffff,color:#000000\n",
        "    style Split fill:#ffffff,color:#000000\n",
        "    style Head1 fill:#ffffff,color:#000000\n",
        "    style Head2 fill:#ffffff,color:#000000\n",
        "    style HeadN fill:#ffffff,color:#000000\n",
        "    style Concat fill:#ffffff,color:#000000\n",
        "    style FC fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dimì€ num_headsë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads  # ê° í—¤ë“œì˜ ì°¨ì›\n",
        "        \n",
        "        # Q, K, Vë¥¼ í•œ ë²ˆì— ìƒì„± (íš¨ìœ¨ì )\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        # ìµœì¢… ì¶œë ¥ ë³€í™˜\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # 1. Q, K, V ìƒì„±\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        \n",
        "        # 2. í—¤ë“œ ìˆ˜ì— ë§ê²Œ ë¶„í• : (B, N, D) -> (B, h, N, d_k)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # 3. ê° í—¤ë“œë³„ Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attention_weights, V)  # (B, h, N, d_k)\n",
        "        \n",
        "        # 4. í—¤ë“œ í•©ì¹˜ê¸°: (B, h, N, d_k) -> (B, N, D)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        \n",
        "        # 5. ìµœì¢… ì„ í˜• ë³€í™˜\n",
        "        output = self.fc_out(context)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "mha = MultiHeadAttention(embed_dim=256, num_heads=8)\n",
        "x = torch.randn(2, 10, 256)  # (batch, seq_len, embed_dim)\n",
        "output, weights = mha(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
        "print(f\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ shape: {weights.shape}\")\n",
        "print(f\"(batch, num_heads, seq_len, seq_len)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Multi-Head Attention ê°€ì¤‘ì¹˜ ì‹œê°í™” (ì—¬ëŸ¬ í—¤ë“œ)\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "mha = MultiHeadAttention(embed_dim=64, num_heads=4)\n",
        "x = torch.randn(1, len(words), 64)\n",
        "_, weights = mha(x)\n",
        "\n",
        "# 4ê°œ í—¤ë“œì˜ ì–´í…ì…˜ ì‹œê°í™”\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=[f\"Head {i+1}\" for i in range(4)]\n",
        ")\n",
        "\n",
        "for i in range(4):\n",
        "    row = i // 2 + 1\n",
        "    col = i % 2 + 1\n",
        "    \n",
        "    head_weights = weights[0, i].detach().numpy()\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=head_weights,\n",
        "            x=words,\n",
        "            y=words,\n",
        "            colorscale=\"Blues\",\n",
        "            showscale=(i == 1)\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Multi-Head Attention: ê° í—¤ë“œë³„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜\",\n",
        "    height=600\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.2 Positional Encoding ì´í•´í•˜ê¸°\n",
        "\n",
        "### ì™œ ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•œê°€?\n",
        "\n",
        "TransformerëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ë¯€ë¡œ **ìˆœì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤**.\n",
        "\n",
        "```\n",
        "\"ê³ ì–‘ì´ê°€ ì¥ë¥¼ ì«“ëŠ”ë‹¤\" vs \"ì¥ê°€ ê³ ì–‘ì´ë¥¼ ì«“ëŠ”ë‹¤\"\n",
        "â†’ Attentionë§Œìœ¼ë¡œëŠ” êµ¬ë¶„ ë¶ˆê°€!\n",
        "```\n",
        "\n",
        "### Positional Encoding\n",
        "\n",
        "ê° ìœ„ì¹˜ì— ê³ ìœ í•œ ë²¡í„°ë¥¼ ë”í•´ì„œ ìˆœì„œ ì •ë³´ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì› ë…¼ë¬¸ì˜ sin/cos ë°©ì‹:**\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "- ì§ìˆ˜ ì°¨ì›: sin í•¨ìˆ˜\n",
        "- í™€ìˆ˜ ì°¨ì›: cos í•¨ìˆ˜\n",
        "- ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ íŒ¨í„´ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Positional Encoding í–‰ë ¬ ìƒì„±\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        # ì§ìˆ˜ ì¸ë±ìŠ¤: sin, í™€ìˆ˜ ì¸ë±ìŠ¤: cos\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)  # í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "pe = PositionalEncoding(d_model=64)\n",
        "x = torch.randn(2, 10, 64)\n",
        "output = pe(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
        "print(f\"ì…ë ¥ê³¼ ì¶œë ¥ì´ ë‹¤ë¥¸ê°€? {not torch.allclose(x, output)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Positional Encoding ì‹œê°í™”\n",
        "d_model = 64\n",
        "max_len = 100\n",
        "\n",
        "pe = torch.zeros(max_len, d_model)\n",
        "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\n",
        "pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "# íˆíŠ¸ë§µ\n",
        "fig = px.imshow(\n",
        "    pe.numpy()[:50, :32],  # ì²˜ìŒ 50ê°œ ìœ„ì¹˜, 32ê°œ ì°¨ì›\n",
        "    labels=dict(x=\"ì°¨ì› (Dimension)\", y=\"ìœ„ì¹˜ (Position)\", color=\"ê°’\"),\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    aspect=\"auto\"\n",
        ")\n",
        "fig.update_layout(title=\"Positional Encoding íŒ¨í„´\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder Layer ì•„í‚¤í…ì²˜\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len, embed_dim)\"] --> MHA[\"Multi-Head<br/>Attention\"]\n",
        "    MHA --> Add1[\"Add<br/>+\"]\n",
        "    Input --> Add1\n",
        "    Add1 --> Norm1[\"LayerNorm\"]\n",
        "    Norm1 --> FFN[\"Feed-Forward<br/>Linear â†’ ReLU â†’ Linear\"]\n",
        "    FFN --> Add2[\"Add<br/>+\"]\n",
        "    Norm1 --> Add2\n",
        "    Add2 --> Norm2[\"LayerNorm\"]\n",
        "    Norm2 --> Output[\"Output<br/>(batch, seq_len, embed_dim)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style MHA fill:#ffffff,color:#000000\n",
        "    style Add1 fill:#ffffff,color:#000000\n",
        "    style Norm1 fill:#ffffff,color:#000000\n",
        "    style FFN fill:#ffffff,color:#000000\n",
        "    style Add2 fill:#ffffff,color:#000000\n",
        "    style Norm2 fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder ì•„í‚¤í…ì²˜\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len)<br/>token indices\"] --> Embed[\"Embedding<br/>vocab_size â†’ embed_dim\"]\n",
        "    Embed --> PosEnc[\"Positional<br/>Encoding\"]\n",
        "    PosEnc --> Layer1[\"Encoder Layer 1\"]\n",
        "    Layer1 --> Layer2[\"Encoder Layer 2\"]\n",
        "    Layer2 --> LayerN[\"Encoder Layer N\"]\n",
        "    LayerN --> Output[\"Output<br/>(batch, seq_len, embed_dim)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style Embed fill:#ffffff,color:#000000\n",
        "    style PosEnc fill:#ffffff,color:#000000\n",
        "    style Layer1 fill:#ffffff,color:#000000\n",
        "    style Layer2 fill:#ffffff,color:#000000\n",
        "    style LayerN fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.3 Transformer Encoder êµ¬ì¡° ì´í•´í•˜ê¸°\n",
        "\n",
        "### Encoder Layer êµ¬ì¡°\n",
        "\n",
        "```\n",
        "ì…ë ¥\n",
        " â†“\n",
        "[Multi-Head Attention]\n",
        " â†“\n",
        "[Add & Norm] â† ì”ì°¨ ì—°ê²° + Layer Normalization\n",
        " â†“\n",
        "[Feed-Forward Network]\n",
        " â†“\n",
        "[Add & Norm]\n",
        " â†“\n",
        "ì¶œë ¥\n",
        "```\n",
        "\n",
        "### í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
        "\n",
        "| êµ¬ì„±ìš”ì†Œ | ì—­í•  |\n",
        "|---------|------|\n",
        "| Multi-Head Attention | ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ |\n",
        "| Add & Norm | í•™ìŠµ ì•ˆì •í™” (ì”ì°¨ ì—°ê²° + ì •ê·œí™”) |\n",
        "| Feed-Forward Network | ë¹„ì„ í˜• ë³€í™˜ (ê° ìœ„ì¹˜ ë…ë¦½ì ) |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Multi-Head Attention\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        \n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Feed-Forward Network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 1. Multi-Head Attention\n",
        "        attn_output, _ = self.attention(x)\n",
        "        \n",
        "        # 2. Add & Norm (ì”ì°¨ ì—°ê²° + LayerNorm)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # 3. Feed-Forward Network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        \n",
        "        # 4. Add & Norm\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "encoder_layer = TransformerEncoderLayer(\n",
        "    embed_dim=256,\n",
        "    num_heads=8,\n",
        "    ff_dim=1024\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 10, 256)\n",
        "output = encoder_layer(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì „ì²´ Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ì„ë² ë”©\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, max_len, dropout)\n",
        "        \n",
        "        # Encoder ë ˆì´ì–´ë“¤\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len) - í† í° ì¸ë±ìŠ¤\n",
        "        \n",
        "        # ì„ë² ë”© + Positional Encoding\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        # Encoder ë ˆì´ì–´ í†µê³¼\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "encoder = TransformerEncoder(\n",
        "    vocab_size=10000,\n",
        "    embed_dim=256,\n",
        "    num_heads=8,\n",
        "    ff_dim=1024,\n",
        "    num_layers=6\n",
        ")\n",
        "\n",
        "x = torch.randint(0, 10000, (2, 50))  # (batch, seq_len)\n",
        "output = encoder(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape} (í† í° ì¸ë±ìŠ¤)\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape} (ë¬¸ë§¥ ë²¡í„°)\")\n",
        "print(f\"\\nì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in encoder.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.4 PyTorch nn.Transformer í™œìš©í•˜ê¸°\n",
        "\n",
        "### PyTorch ë‚´ì¥ Transformer\n",
        "\n",
        "PyTorchëŠ” Transformer ê´€ë ¨ ëª¨ë“ˆì„ ì œê³µí•©ë‹ˆë‹¤:\n",
        "\n",
        "| ëª¨ë“ˆ | ì„¤ëª… |\n",
        "|-----|------|\n",
        "| `nn.MultiheadAttention` | Multi-Head Attention |\n",
        "| `nn.TransformerEncoderLayer` | Encoder ë ˆì´ì–´ 1ê°œ |\n",
        "| `nn.TransformerEncoder` | Encoder ìŠ¤íƒ |\n",
        "| `nn.Transformer` | ì „ì²´ Encoder-Decoder |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer ë¶„ë¥˜ ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len)<br/>token indices\"] --> Embed[\"Embedding<br/>vocab_size â†’ embed_dim\"]\n",
        "    Embed --> PosEnc[\"Positional<br/>Encoding\"]\n",
        "    PosEnc --> Encoder[\"Transformer<br/>Encoder<br/>(N layers)\"]\n",
        "    Encoder --> EncOut[\"(batch, seq_len, embed_dim)\"]\n",
        "    EncOut --> MeanPool[\"Mean Pooling<br/>mean(dim=1)\"]\n",
        "    MeanPool --> PoolOut[\"(batch, embed_dim)\"]\n",
        "    PoolOut --> FC1[\"Linear<br/>embed_dim â†’ embed_dim//2\"]\n",
        "    FC1 --> ReLU[\"ReLU\"]\n",
        "    ReLU --> Dropout[\"Dropout\"]\n",
        "    Dropout --> FC2[\"Linear<br/>embed_dim//2 â†’ num_classes\"]\n",
        "    FC2 --> Output[\"Output<br/>(batch, num_classes)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style Embed fill:#ffffff,color:#000000\n",
        "    style PosEnc fill:#ffffff,color:#000000\n",
        "    style Encoder fill:#ffffff,color:#000000\n",
        "    style EncOut fill:#ffffff,color:#000000\n",
        "    style MeanPool fill:#ffffff,color:#000000\n",
        "    style PoolOut fill:#ffffff,color:#000000\n",
        "    style FC1 fill:#ffffff,color:#000000\n",
        "    style ReLU fill:#ffffff,color:#000000\n",
        "    style Dropout fill:#ffffff,color:#000000\n",
        "    style FC2 fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PyTorch ë‚´ì¥ nn.MultiheadAttention\n",
        "mha = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)\n",
        "\n",
        "x = torch.randn(2, 10, 256)  # (batch, seq_len, embed_dim)\n",
        "output, weights = mha(x, x, x)  # Self-Attention: Q=K=V=x\n",
        "\n",
        "print(f\"nn.MultiheadAttention:\")\n",
        "print(f\"  ì…ë ¥: {x.shape}\")\n",
        "print(f\"  ì¶œë ¥: {output.shape}\")\n",
        "print(f\"  ê°€ì¤‘ì¹˜: {weights.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PyTorch ë‚´ì¥ nn.TransformerEncoderLayer\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=256,\n",
        "    nhead=8,\n",
        "    dim_feedforward=1024,\n",
        "    dropout=0.1,\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 10, 256)\n",
        "output = encoder_layer(x)\n",
        "\n",
        "print(f\"nn.TransformerEncoderLayer:\")\n",
        "print(f\"  ì…ë ¥: {x.shape}\")\n",
        "print(f\"  ì¶œë ¥: {output.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PyTorch ë‚´ì¥ nn.TransformerEncoder\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=256, nhead=8, dim_feedforward=1024, batch_first=True\n",
        ")\n",
        "encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "x = torch.randn(2, 10, 256)\n",
        "output = encoder(x)\n",
        "\n",
        "print(f\"nn.TransformerEncoder (6 layers):\")\n",
        "print(f\"  ì…ë ¥: {x.shape}\")\n",
        "print(f\"  ì¶œë ¥: {output.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ Transformer ëª¨ë¸\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, num_classes, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, max_len, dropout)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.encoder(x)\n",
        "        \n",
        "        # í‰ê·  í’€ë§ (ë˜ëŠ” [CLS] í† í° ì‚¬ìš© ê°€ëŠ¥)\n",
        "        x = x.mean(dim=1)  # (batch, embed_dim)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=10000,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    ff_dim=512,\n",
        "    num_layers=2,\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "x = torch.randint(0, 10000, (4, 100))  # (batch, seq_len)\n",
        "output = model(x)\n",
        "\n",
        "print(f\"Transformer í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸:\")\n",
        "print(f\"  ì…ë ¥: {x.shape}\")\n",
        "print(f\"  ì¶œë ¥: {output.shape}\")\n",
        "print(f\"\\nì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: IMDB ê°ì„± ë¶„ì„ (ê°„ë‹¨ ë²„ì „)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# IMDB ë°ì´í„°ì…‹ ë¡œë“œ (torchtext ì‚¬ìš©)\n",
        "# ì°¸ê³ : ì‹¤ì œ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ êµ¬ì¡°ë§Œ í™•ì¸í•©ë‹ˆë‹¤\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ê°€ìƒì˜ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” IMDB ë°ì´í„° ì‚¬ìš©)\n",
        "vocab_size = 10000\n",
        "seq_len = 200\n",
        "num_samples = 1000\n",
        "\n",
        "# ëœë¤ ë°ì´í„° (ì‹¤ì œë¡œëŠ” í† í°í™”ëœ ë¦¬ë·° í…ìŠ¤íŠ¸)\n",
        "X_train = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
        "y_train = torch.randint(0, 2, (num_samples,))  # 0: ë¶€ì •, 1: ê¸ì •\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ\")\n",
        "print(f\"ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    num_layers=2,\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (1 ì—í¬í¬)\n",
        "model.train()\n",
        "total_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for batch_idx, (texts, labels) in enumerate(train_loader):\n",
        "    texts, labels = texts.to(device), labels.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(texts)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    if batch_idx % 10 == 0:\n",
        "        print(f\"Batch {batch_idx}/{len(train_loader)}: Loss={loss.item():.4f}\")\n",
        "\n",
        "print(f\"\\nì—í¬í¬ ì™„ë£Œ: Loss={total_loss/len(train_loader):.4f}, Acc={100.*correct/total:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ ì‹¤ìŠµ í€´ì¦ˆ\n",
        "\n",
        "**ë‚œì´ë„**: â­ (ì‰¬ì›€) ~ â­â­â­â­â­ (ì–´ë ¤ì›€)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. Attention Score ê³„ì‚° â­\n",
        "\n",
        "**ë¬¸ì œ**: Queryì™€ Keyê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Attention Scoreë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n",
        "\n",
        "```python\n",
        "Q = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # (2, 2)\n",
        "K = torch.tensor([[1.0, 1.0], [0.0, 1.0]])  # (2, 2)\n",
        "```\n",
        "\n",
        "íŒíŠ¸: Score = Q @ K^T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Scaled Dot-Product Attention â­\n",
        "\n",
        "**ë¬¸ì œ**: Scoreë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  softmaxë¥¼ ì ìš©í•˜ì—¬ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n",
        "\n",
        "d_k = 2 (Key ì°¨ì›)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. Query, Key, Value ìƒì„± â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ì…ë ¥ í…ì„œ Xë¡œë¶€í„° Q, K, Vë¥¼ ìƒì„±í•˜ëŠ” ì„ í˜• ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ê³  ì ìš©í•˜ì„¸ìš”.\n",
        "\n",
        "- ì…ë ¥: X (batch=1, seq_len=4, embed_dim=8)\n",
        "- ì¶œë ¥: Q, K, V ê°ê° (1, 4, 8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. Self-Attention ì „ì²´ ê³„ì‚° â­â­\n",
        "\n",
        "**ë¬¸ì œ**: Q, K, Vê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Self-Attentionì˜ ì „ì²´ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
        "\n",
        "1. Score ê³„ì‚°\n",
        "2. ìŠ¤ì¼€ì¼ë§\n",
        "3. Softmax\n",
        "4. Valueì™€ ê°€ì¤‘í•©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œê°í™” â­â­\n",
        "\n",
        "**ë¬¸ì œ**: 5ê°œ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ì˜ Self-Attention ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ì„¸ìš”.\n",
        "\n",
        "ë‹¨ì–´: [\"I\", \"love\", \"deep\", \"learning\", \"!\"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. Multi-Head Attention í—¤ë“œ ë¶„í•  â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: embed_dim=64, num_heads=4ì¼ ë•Œ, í…ì„œë¥¼ í—¤ë“œë³„ë¡œ ë¶„í• í•˜ì„¸ìš”.\n",
        "\n",
        "- ì…ë ¥: (batch=2, seq_len=10, embed_dim=64)\n",
        "- ì¶œë ¥: (batch=2, num_heads=4, seq_len=10, head_dim=16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Positional Encoding ìƒì„± â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: d_model=16, max_len=10ì¸ Positional Encodingì„ ìƒì„±í•˜ê³  ì‹œê°í™”í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. Transformer Encoder Layer êµ¬í˜„ â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: Multi-Head Attention, Add & Norm, Feed-Forwardë¥¼ í¬í•¨í•˜ëŠ” Encoder Layerë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "ì„¤ì •:\n",
        "- embed_dim = 64\n",
        "- num_heads = 4\n",
        "- ff_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. nn.TransformerEncoder í™œìš© â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: PyTorchì˜ nn.TransformerEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ 6ê°œ ë ˆì´ì–´ì˜ Encoderë¥¼ ë§Œë“¤ê³  ìˆœì „íŒŒë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. Transformer í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ â­â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: Transformer ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , ê°€ìƒì˜ ë°ì´í„°ë¡œ 1 ì—í¬í¬ í•™ìŠµì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
        "\n",
        "ìš”êµ¬ì‚¬í•­:\n",
        "1. Embedding + Positional Encoding\n",
        "2. TransformerEncoder (2 layers)\n",
        "3. í‰ê·  í’€ë§ + ë¶„ë¥˜ í—¤ë“œ\n",
        "4. í•™ìŠµ ë£¨í”„ (1 ì—í¬í¬)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š í•™ìŠµ ì •ë¦¬\n",
        "\n",
        "### Part 1: ê¸°ì´ˆ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê°œë… | í•µì‹¬ ìˆ˜ì‹/ì½”ë“œ | ì—­í•  |\n",
        "|-----|---------------|------|\n",
        "| Self-Attention | Q @ K^T @ V | ì‹œí€€ìŠ¤ ë‚´ ê´€ê³„ íŒŒì•… |\n",
        "| Q, K, V | X @ W_Q, X @ W_K, X @ W_V | ì§ˆì˜, í‚¤, ê°’ ë³€í™˜ |\n",
        "| Scaling | / sqrt(d_k) | ê°’ ì•ˆì •í™” |\n",
        "| Softmax | F.softmax(scores, dim=-1) | ê°€ì¤‘ì¹˜ ì •ê·œí™” |\n",
        "\n",
        "### Part 2: ì‹¬í™” í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê°œë… | í•µì‹¬ | íš¨ê³¼ |\n",
        "|-----|------|------|\n",
        "| Multi-Head | ì—¬ëŸ¬ ê´€ì  ë³‘ë ¬ ì²˜ë¦¬ | ë‹¤ì–‘í•œ ê´€ê³„ í¬ì°© |\n",
        "| Positional Encoding | sin/cos íŒ¨í„´ | ìˆœì„œ ì •ë³´ ì£¼ì… |\n",
        "| Add & Norm | ì”ì°¨ ì—°ê²° + LayerNorm | í•™ìŠµ ì•ˆì •í™” |\n",
        "| Feed-Forward | 2ì¸µ MLP | ë¹„ì„ í˜• ë³€í™˜ |\n",
        "\n",
        "### Attention ê³µì‹\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ íŒ\n",
        "\n",
        "1. **Multi-Head ìˆ˜**: embed_dimì„ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ê²Œ ì„¤ì • (ë³´í†µ 8, 16)\n",
        "2. **ff_dim**: ì¼ë°˜ì ìœ¼ë¡œ embed_dimì˜ 4ë°° (ì˜ˆ: 512 -> 2048)\n",
        "3. **Layer ìˆ˜**: ì‘ì€ ëª¨ë¸ 2-4, í° ëª¨ë¸ 6-12+\n",
        "4. **Dropout**: 0.1ì´ ê¸°ë³¸, ê³¼ì í•© ì‹œ 0.2-0.3\n",
        "5. **ë©”ëª¨ë¦¬**: Attentionì€ O(n^2), ê¸´ ì‹œí€€ìŠ¤ ì£¼ì˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}