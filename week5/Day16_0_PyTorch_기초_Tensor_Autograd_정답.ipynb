{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_0: PyTorch 기초 (Tensor & Autograd)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. PyTorch 텐서 생성 및 기본 연산 이해하기\n",
    "2. 텐서의 shape, dtype, device 속성 다루기\n",
    "3. NumPy와 PyTorch 텐서 변환하기\n",
    "4. GPU 가속(CUDA) 개념 이해하기\n",
    "5. 텐서 인덱싱과 슬라이싱 활용하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. Autograd(자동 미분) 원리 이해하기\n",
    "2. requires_grad와 backward() 활용하기\n",
    "3. nn.Module로 신경망 정의하기\n",
    "4. Dataset과 DataLoader로 데이터 공급하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| Tensor | 딥러닝 데이터 표현 | 이미지, 텍스트, 시계열 데이터 |\n",
    "| Autograd | 자동 그래디언트 계산 | 역전파 학습 자동화 |\n",
    "| nn.Module | 신경망 구조 정의 | MLP, CNN, RNN 모델 설계 |\n",
    "| DataLoader | 효율적 데이터 공급 | 배치 처리, 셔플링 |\n",
    "\n",
    "**분석가 관점**: PyTorch는 연구와 프로덕션 모두에서 가장 인기 있는 딥러닝 프레임워크입니다. 직관적인 Python 스타일과 동적 계산 그래프로 디버깅이 쉽습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 PyTorch 소개\n",
    "\n",
    "### 딥러닝 프레임워크 비교\n",
    "\n",
    "| 구분 | PyTorch | TensorFlow |\n",
    "|------|---------|------------|\n",
    "| 개발사 | Meta (Facebook) | Google |\n",
    "| 계산 그래프 | 동적 (Define-by-Run) | 정적 (TF1) / 동적 (TF2) |\n",
    "| 디버깅 | Python 스타일 (쉬움) | 상대적으로 복잡 |\n",
    "| 연구 | 학계에서 선호 | 산업에서 선호 |\n",
    "| 장점 | 직관적, 빠른 프로토타입 | 대규모 배포 용이 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 설치 확인\n",
    "import torch\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "\n",
    "# CUDA (GPU) 사용 가능 여부\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CPU 모드로 실행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 텐서 생성\n",
    "\n",
    "### 텐서(Tensor)란?\n",
    "\n",
    "- 다차원 배열 (NumPy의 ndarray와 유사)\n",
    "- GPU에서 연산 가능\n",
    "- 자동 미분 지원\n",
    "\n",
    "```\n",
    "스칼라(0D) -> 벡터(1D) -> 행렬(2D) -> 텐서(3D+)\n",
    "    5         [1,2,3]     [[1,2],[3,4]]   [[[1,2],[3,4]],[[5,6],[7,8]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. torch.tensor() - 직접 값 지정\n",
    "scalar = torch.tensor(5)               # 스칼라 (0차원)\n",
    "vector = torch.tensor([1, 2, 3])       # 벡터 (1차원)\n",
    "matrix = torch.tensor([[1, 2], [3, 4]])  # 행렬 (2차원)\n",
    "\n",
    "print(f\"스칼라: {scalar}, shape: {scalar.shape}\")\n",
    "print(f\"벡터: {vector}, shape: {vector.shape}\")\n",
    "print(f\"행렬:\\n{matrix}, shape: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 특수 텐서 생성 함수\n",
    "zeros = torch.zeros(3, 4)          # 0으로 채운 3x4 텐서\n",
    "ones = torch.ones(2, 3)            # 1로 채운 2x3 텐서\n",
    "rand = torch.rand(2, 2)            # 0~1 균등 분포\n",
    "randn = torch.randn(2, 2)          # 표준 정규 분포\n",
    "\n",
    "print(f\"zeros (3x4):\\n{zeros}\")\n",
    "print(f\"\\nones (2x3):\\n{ones}\")\n",
    "print(f\"\\nrand (0~1 균등):\\n{rand}\")\n",
    "print(f\"\\nrandn (정규 분포):\\n{randn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 범위/간격 텐서\n",
    "arange = torch.arange(0, 10, 2)    # 0부터 10 미만, 간격 2\n",
    "linspace = torch.linspace(0, 1, 5) # 0~1을 5등분\n",
    "\n",
    "print(f\"arange(0, 10, 2): {arange}\")\n",
    "print(f\"linspace(0, 1, 5): {linspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기존 텐서와 같은 shape/dtype으로 생성\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "\n",
    "zeros_like = torch.zeros_like(x)   # x와 같은 shape, 0으로 채움\n",
    "ones_like = torch.ones_like(x)     # x와 같은 shape, 1로 채움\n",
    "rand_like = torch.rand_like(x)     # x와 같은 shape, 랜덤\n",
    "\n",
    "print(f\"원본 x:\\n{x}\")\n",
    "print(f\"zeros_like:\\n{zeros_like}\")\n",
    "print(f\"ones_like:\\n{ones_like}\")\n",
    "print(f\"rand_like:\\n{rand_like}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 딥러닝 입력 데이터 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 시뮬레이션 (batch_size=32, features=10)\n",
    "batch_size = 32\n",
    "num_features = 10\n",
    "\n",
    "# 입력 데이터 (정규 분포)\n",
    "X_batch = torch.randn(batch_size, num_features)\n",
    "\n",
    "# 레이블 (0 또는 1)\n",
    "y_batch = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "print(f\"입력 데이터 shape: {X_batch.shape}\")\n",
    "print(f\"레이블 shape: {y_batch.shape}\")\n",
    "print(f\"\\n첫 5개 샘플의 처음 3개 특성:\\n{X_batch[:5, :3]}\")\n",
    "print(f\"\\n첫 10개 레이블: {y_batch[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 텐서 속성\n",
    "\n",
    "### shape, dtype, device\n",
    "\n",
    "텐서의 3가지 핵심 속성:\n",
    "- **shape**: 각 차원의 크기\n",
    "- **dtype**: 데이터 타입\n",
    "- **device**: CPU 또는 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성\n",
    "t = torch.randn(3, 4, 5)\n",
    "\n",
    "# 속성 확인\n",
    "print(f\"Shape: {t.shape}\")       # torch.Size([3, 4, 5])\n",
    "print(f\"Size: {t.size()}\")       # shape와 동일\n",
    "print(f\"Dtype: {t.dtype}\")       # torch.float32 (기본값)\n",
    "print(f\"Device: {t.device}\")     # cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 지정\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "bool_tensor = torch.tensor([True, False, True], dtype=torch.bool)\n",
    "\n",
    "print(f\"int64: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "print(f\"float32: {float_tensor}, dtype: {float_tensor.dtype}\")\n",
    "print(f\"bool: {bool_tensor}, dtype: {bool_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 변환\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"원본: {x}, dtype: {x.dtype}\")\n",
    "\n",
    "# float으로 변환\n",
    "x_float = x.float()  # 또는 x.to(torch.float32)\n",
    "print(f\"float: {x_float}, dtype: {x_float.dtype}\")\n",
    "\n",
    "# double로 변환\n",
    "x_double = x.double()  # 또는 x.to(torch.float64)\n",
    "print(f\"double: {x_double}, dtype: {x_double.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 딥러닝에서 자주 사용되는 dtype\n",
    "\n",
    "| dtype | 용도 | 메모리 |\n",
    "|-------|-----|--------|\n",
    "| float32 | 기본 학습 | 4 bytes |\n",
    "| float16 | 혼합 정밀도 학습 | 2 bytes |\n",
    "| int64 | 인덱스, 레이블 | 8 bytes |\n",
    "| bool | 마스크 | 1 byte |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 GPU 가속 (CUDA)\n",
    "\n",
    "### to() 메서드로 device 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정 (GPU 있으면 cuda, 없으면 cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서를 device로 이동\n",
    "x = torch.randn(3, 3)\n",
    "print(f\"원본 device: {x.device}\")\n",
    "\n",
    "# GPU로 이동 (GPU 있는 경우)\n",
    "x_device = x.to(device)\n",
    "print(f\"이동 후 device: {x_device.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성 시 바로 device 지정\n",
    "y = torch.randn(3, 3, device=device)\n",
    "print(f\"직접 생성: {y.device}\")\n",
    "\n",
    "# CPU로 다시 이동 (결과 확인용)\n",
    "y_cpu = y.cpu()\n",
    "print(f\"CPU로 이동: {y_cpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의: device가 다른 텐서끼리 연산 불가!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 불일치 오류 예시\n",
    "a = torch.randn(3, 3)  # CPU\n",
    "b = torch.randn(3, 3, device=device)  # GPU (있는 경우)\n",
    "\n",
    "if a.device != b.device:\n",
    "    print(f\"주의: a({a.device})와 b({b.device})는 device가 다릅니다!\")\n",
    "    print(\"연산 전에 같은 device로 이동해야 합니다.\")\n",
    "    \n",
    "    # 같은 device로 이동 후 연산\n",
    "    a_device = a.to(device)\n",
    "    result = a_device + b\n",
    "    print(f\"연산 성공! 결과 shape: {result.shape}\")\n",
    "else:\n",
    "    result = a + b\n",
    "    print(f\"연산 성공! 결과 shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 NumPy와 PyTorch 변환\n",
    "\n",
    "### 양방향 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NumPy -> PyTorch\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"NumPy 배열:\\n{np_array}\")\n",
    "print(f\"PyTorch 텐서:\\n{tensor_from_np}\")\n",
    "print(f\"dtype: {tensor_from_np.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch -> NumPy\n",
    "tensor = torch.randn(2, 3)\n",
    "np_from_tensor = tensor.numpy()\n",
    "\n",
    "print(f\"PyTorch 텐서:\\n{tensor}\")\n",
    "print(f\"NumPy 배열:\\n{np_from_tensor}\")\n",
    "print(f\"type: {type(np_from_tensor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의: 메모리 공유!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_numpy는 메모리를 공유합니다!\n",
    "np_arr = np.array([1, 2, 3], dtype=np.float32)\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "\n",
    "print(f\"변환 전 NumPy: {np_arr}\")\n",
    "print(f\"변환 전 Tensor: {tensor}\")\n",
    "\n",
    "# NumPy 배열 수정\n",
    "np_arr[0] = 100\n",
    "\n",
    "print(f\"\\n수정 후 NumPy: {np_arr}\")\n",
    "print(f\"수정 후 Tensor: {tensor}  # 같이 변경됨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복사본 만들기 (메모리 공유 방지)\n",
    "np_arr = np.array([1, 2, 3], dtype=np.float32)\n",
    "tensor = torch.tensor(np_arr)  # tensor()는 복사본 생성\n",
    "\n",
    "np_arr[0] = 100\n",
    "\n",
    "print(f\"NumPy: {np_arr}\")\n",
    "print(f\"Tensor: {tensor}  # 독립적\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 텐서 연산\n",
    "\n",
    "### 기본 산술 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# 요소별 연산\n",
    "print(f\"a + b:\\n{a + b}\")\n",
    "print(f\"\\na - b:\\n{a - b}\")\n",
    "print(f\"\\na * b (요소별):\\n{a * b}\")\n",
    "print(f\"\\na / b:\\n{a / b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 곱 (Matrix Multiplication)\n",
    "# 방법 1: torch.matmul()\n",
    "matmul_result = torch.matmul(a, b)\n",
    "print(f\"matmul(a, b):\\n{matmul_result}\")\n",
    "\n",
    "# 방법 2: @ 연산자 (Python 3.5+)\n",
    "at_result = a @ b\n",
    "print(f\"\\na @ b:\\n{at_result}\")\n",
    "\n",
    "# 방법 3: tensor.mm() (2D only)\n",
    "mm_result = a.mm(b)\n",
    "print(f\"\\na.mm(b):\\n{mm_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집계 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "print(f\"텐서 x:\\n{x}\")\n",
    "\n",
    "# 전체 집계\n",
    "print(f\"\\nsum: {x.sum()}\")\n",
    "print(f\"mean: {x.mean()}\")\n",
    "print(f\"max: {x.max()}\")\n",
    "print(f\"min: {x.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 축(dim) 기준 집계\n",
    "print(f\"행 방향 합 (dim=0): {x.sum(dim=0)}\")\n",
    "print(f\"열 방향 합 (dim=1): {x.sum(dim=1)}\")\n",
    "\n",
    "# argmax: 최댓값의 인덱스\n",
    "print(f\"\\n전체 argmax: {x.argmax()}\")\n",
    "print(f\"열 방향 argmax (dim=1): {x.argmax(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 형태 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)\n",
    "print(f\"원본: {x}, shape: {x.shape}\")\n",
    "\n",
    "# reshape: 형태 변경\n",
    "x_3x4 = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{x_3x4}\")\n",
    "\n",
    "# view: reshape와 유사 (연속 메모리 필요)\n",
    "x_2x6 = x.view(2, 6)\n",
    "print(f\"\\nview(2, 6):\\n{x_2x6}\")\n",
    "\n",
    "# -1: 자동 계산\n",
    "x_auto = x.reshape(4, -1)  # 4 x ? -> 4 x 3\n",
    "print(f\"\\nreshape(4, -1):\\n{x_auto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 추가/제거\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(f\"원본 shape: {x.shape}\")\n",
    "\n",
    "# unsqueeze: 차원 추가\n",
    "x_unsq0 = x.unsqueeze(0)  # (3,) -> (1, 3)\n",
    "x_unsq1 = x.unsqueeze(1)  # (3,) -> (3, 1)\n",
    "print(f\"unsqueeze(0): {x_unsq0.shape}\")\n",
    "print(f\"unsqueeze(1): {x_unsq1.shape}\")\n",
    "\n",
    "# squeeze: 크기 1인 차원 제거\n",
    "y = torch.zeros(1, 3, 1)\n",
    "y_sq = y.squeeze()\n",
    "print(f\"\\nsqueeze 전: {y.shape}\")\n",
    "print(f\"squeeze 후: {y_sq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.7 텐서 인덱싱과 슬라이싱\n",
    "\n",
    "### NumPy와 동일한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12]])\n",
    "print(f\"텐서 x:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 인덱싱\n",
    "print(f\"x[0]: {x[0]}\")\n",
    "print(f\"x[0, 0]: {x[0, 0]}\")\n",
    "print(f\"x[-1]: {x[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이싱\n",
    "print(f\"x[:2]: {x[:2]}\")\n",
    "print(f\"x[:, 1:3]:\\n{x[:, 1:3]}\")\n",
    "print(f\"x[1:, 2:]:\\n{x[1:, 2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건 인덱싱 (Boolean Indexing)\n",
    "mask = x > 5\n",
    "print(f\"마스크 (x > 5):\\n{mask}\")\n",
    "print(f\"\\n5보다 큰 값: {x[mask]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 팬시 인덱싱\n",
    "indices = torch.tensor([0, 2])\n",
    "print(f\"x[indices] (0, 2번 행):\\n{x[indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 배치에서 특정 샘플 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 (batch_size=8, features=5)\n",
    "batch = torch.randn(8, 5)\n",
    "labels = torch.tensor([0, 1, 0, 1, 0, 1, 1, 0])\n",
    "\n",
    "print(f\"배치 shape: {batch.shape}\")\n",
    "print(f\"레이블: {labels}\")\n",
    "\n",
    "# 클래스 1인 샘플만 추출\n",
    "class1_samples = batch[labels == 1]\n",
    "print(f\"\\n클래스 1 샘플 shape: {class1_samples.shape}\")\n",
    "print(f\"클래스 1 샘플:\\n{class1_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Autograd (자동 미분)\n",
    "\n",
    "### 역전파의 핵심\n",
    "\n",
    "**Autograd**: 텐서 연산의 그래디언트(미분값)를 자동으로 계산\n",
    "\n",
    "```\n",
    "순전파 (Forward):  x -> f(x) -> y -> g(y) -> z\n",
    "역전파 (Backward): dz/dx <- dz/dy <- dz/dz=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad=True: 그래디언트 추적 활성화\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연산 수행 (계산 그래프 생성)\n",
    "y = x ** 2 + 3 * x  # y = x^2 + 3x\n",
    "print(f\"y = x^2 + 3x = {y}\")\n",
    "\n",
    "# 스칼라로 변환 (backward는 스칼라에 대해 호출)\n",
    "z = y.sum()  # z = y[0] + y[1]\n",
    "print(f\"z = sum(y) = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward(): 그래디언트 계산\n",
    "z.backward()\n",
    "\n",
    "# x.grad: dz/dx\n",
    "# y = x^2 + 3x -> dy/dx = 2x + 3\n",
    "# x = [2, 3] -> dy/dx = [7, 9]\n",
    "print(f\"dz/dx = {x.grad}\")\n",
    "print(f\"수동 계산: 2*{x.data} + 3 = {2*x.data + 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 누적 주의!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래디언트는 누적됩니다!\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# 첫 번째 backward\n",
    "y1 = x * 2\n",
    "y1.backward()\n",
    "print(f\"첫 번째 backward 후 grad: {x.grad}\")\n",
    "\n",
    "# 두 번째 backward (누적됨!)\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"두 번째 backward 후 grad: {x.grad}  # 2 + 3 = 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해결: 매 iteration마다 grad 초기화\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    # 그래디언트 초기화\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    y = x * (i + 1)\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}: grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 간단한 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표: y = 2x + 1의 기울기(w)와 절편(b) 찾기\n",
    "# 데이터 생성\n",
    "torch.manual_seed(42)\n",
    "X = torch.linspace(-1, 1, 20).reshape(-1, 1)  # (20, 1)\n",
    "y_true = 2 * X + 1 + 0.1 * torch.randn_like(X)  # y = 2x + 1 + noise\n",
    "\n",
    "# 학습할 파라미터 (초기값)\n",
    "w = torch.tensor([0.0], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"경사 하강법으로 w=2, b=1 찾기\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 순전파: 예측\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # 손실 계산: MSE\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "    \n",
    "    # 역전파: 그래디언트 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 파라미터 업데이트 (no_grad 안에서!)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 그래디언트 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")\n",
    "\n",
    "print(f\"\\n최종 결과: w={w.item():.4f} (목표: 2.0), b={b.item():.4f} (목표: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 nn.Module로 신경망 정의\n",
    "\n",
    "### nn.Module 기초\n",
    "\n",
    "PyTorch의 모든 신경망은 `nn.Module`을 상속받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 간단한 선형 모델\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 레이어 정의\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 순전파 정의\n",
    "        return self.linear(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = LinearModel(input_dim=10, output_dim=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 확인\n",
    "print(\"모델 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파 테스트\n",
    "x = torch.randn(5, 10)  # 배치 5개, 특성 10개\n",
    "output = model(x)       # forward() 자동 호출\n",
    "print(f\"입력 shape: {x.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"출력:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론 (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)    # 입력 -> 은닉\n",
    "        self.relu = nn.ReLU()                          # 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)   # 은닉 -> 출력\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)      # 선형 변환\n",
    "        x = self.relu(x)     # 활성화\n",
    "        x = self.fc2(x)      # 선형 변환\n",
    "        return x\n",
    "\n",
    "# MLP 생성\n",
    "mlp = MLP(input_dim=10, hidden_dim=32, output_dim=2)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential로 더 간단하게\n",
    "mlp_sequential = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "print(mlp_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "x = torch.randn(4, 10)\n",
    "output = mlp_sequential(x)\n",
    "print(f\"입력: {x.shape}\")\n",
    "print(f\"출력: {output.shape}\")\n",
    "print(f\"출력 값:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 Dataset과 DataLoader\n",
    "\n",
    "### Dataset 클래스\n",
    "\n",
    "데이터를 관리하는 추상 클래스. `__len__`과 `__getitem__` 구현 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 데이터 생성\n",
    "X_data = torch.randn(100, 5)  # 100개 샘플, 5개 특성\n",
    "y_data = torch.randint(0, 2, (100,))  # 이진 레이블\n",
    "\n",
    "# Dataset 생성\n",
    "dataset = SimpleDataset(X_data, y_data)\n",
    "print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "print(f\"첫 번째 샘플: X={dataset[0][0].shape}, y={dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader: 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,   # 배치 크기\n",
    "    shuffle=True,    # 셔플 여부\n",
    "    drop_last=True   # 마지막 불완전한 배치 버림\n",
    ")\n",
    "\n",
    "print(f\"배치 수: {len(dataloader)}\")\n",
    "\n",
    "# 배치 순회\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "    if batch_idx < 3:  # 처음 3개 배치만\n",
    "        print(f\"Batch {batch_idx}: X={X_batch.shape}, y={y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 학습 루프 (Training Loop)\n",
    "\n",
    "### 전체 흐름\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # 1. 순전파\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        # 2. 역전파\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 그래디언트 계산\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 예제: y = 2x + 1 학습\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 데이터 생성\n",
    "X_train = torch.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y_train = 2 * X_train + 1 + 0.1 * torch.randn_like(X_train)\n",
    "\n",
    "# Dataset, DataLoader\n",
    "train_dataset = SimpleDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model = nn.Linear(1, 1)  # 입력 1, 출력 1\n",
    "criterion = nn.MSELoss()  # 평균 제곱 오차\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"학습 전 파라미터:\")\n",
    "print(f\"  weight: {model.weight.data.item():.4f}\")\n",
    "print(f\"  bias: {model.bias.data.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # 1. 순전파\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        # 2. 역전파\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 그래디언트 계산\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n학습 후 파라미터:\")\n",
    "print(f\"  weight: {model.weight.data.item():.4f} (목표: 2.0)\")\n",
    "print(f\"  bias: {model.bias.data.item():.4f} (목표: 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화 (Plotly)\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(epochs),\n",
    "    'Loss': losses\n",
    "})\n",
    "\n",
    "fig = px.line(loss_df, x='Epoch', y='Loss', title='학습 손실 곡선')\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='MSE Loss',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 이진 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터로 고가/저가 분류\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = (housing.target > housing.target.median()).astype(int)  # 고가: 1, 저가: 0\n",
    "\n",
    "# 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"훈련 데이터: {X_train_t.shape}\")\n",
    "print(f\"테스트 데이터: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 모델\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # 확률 출력\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = BinaryClassifier(input_dim=8)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataset = SimpleDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 학습\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 학습 모드\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "model.eval()  # 평가 모드\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    y_pred_proba = model(X_test_t)\n",
    "    y_pred = (y_pred_proba > 0.5).float()\n",
    "    \n",
    "    accuracy = (y_pred == y_test_t).float().mean()\n",
    "    print(f\"\\n테스트 정확도: {accuracy.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈 정답\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 텐서 생성하기 (기본)\n",
    "\n",
    "**문제**: 다음 조건에 맞는 텐서를 생성하세요.\n",
    "\n",
    "1. 3x4 크기의 모든 요소가 1인 텐서\n",
    "2. 0부터 9까지의 정수 텐서\n",
    "3. 2x3 크기의 표준 정규 분포 랜덤 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 정답\n",
    "import torch\n",
    "\n",
    "# 1. 3x4 크기의 모든 요소가 1인 텐서\n",
    "tensor1 = torch.ones(3, 4)\n",
    "print(f\"1. ones 텐서 (3x4):\\n{tensor1}\")\n",
    "print(f\"   shape: {tensor1.shape}\\n\")\n",
    "\n",
    "# 2. 0부터 9까지의 정수 텐서\n",
    "tensor2 = torch.arange(0, 10)\n",
    "print(f\"2. arange 텐서 (0~9): {tensor2}\")\n",
    "print(f\"   shape: {tensor2.shape}\\n\")\n",
    "\n",
    "# 3. 2x3 크기의 표준 정규 분포 랜덤 텐서\n",
    "tensor3 = torch.randn(2, 3)\n",
    "print(f\"3. randn 텐서 (2x3):\\n{tensor3}\")\n",
    "print(f\"   shape: {tensor3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert tensor1.shape == torch.Size([3, 4]), \"tensor1의 shape이 (3, 4)가 아닙니다\"\n",
    "assert torch.all(tensor1 == 1), \"tensor1의 모든 요소가 1이 아닙니다\"\n",
    "assert tensor2.shape == torch.Size([10]), \"tensor2의 shape이 (10,)이 아닙니다\"\n",
    "assert tensor2.tolist() == list(range(10)), \"tensor2가 0~9가 아닙니다\"\n",
    "assert tensor3.shape == torch.Size([2, 3]), \"tensor3의 shape이 (2, 3)이 아닙니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: PyTorch의 텐서 생성 함수를 사용합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- `torch.ones(shape)`: 1로 채워진 텐서 생성\n",
    "- `torch.arange(start, end)`: 범위 내 정수 텐서 생성\n",
    "- `torch.randn(shape)`: 표준 정규 분포(평균 0, 표준편차 1) 랜덤 텐서\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# ones 대안\n",
    "tensor1 = torch.full((3, 4), 1.0)\n",
    "\n",
    "# arange 대안\n",
    "tensor2 = torch.tensor(list(range(10)))\n",
    "\n",
    "# randn 대안 (균등 분포)\n",
    "tensor3 = torch.rand(2, 3)  # 0~1 균등 분포\n",
    "```\n",
    "\n",
    "**흔한 실수**: `torch.ones([3, 4])`처럼 리스트로 전달해도 동작하지만, `torch.ones(3, 4)`가 권장됩니다.\n",
    "\n",
    "**실무 팁**: `randn`은 가중치 초기화, `ones`와 `zeros`는 마스크나 편향 초기화에 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q2. 텐서 속성 확인 (기본)\n",
    "\n",
    "**문제**: 아래 텐서의 shape, dtype, device를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 정답\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "# 속성 확인\n",
    "print(f\"텐서:\\n{tensor}\")\n",
    "print(f\"\\nShape: {tensor.shape}\")\n",
    "print(f\"Dtype: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert tensor.shape == torch.Size([2, 3]), \"shape이 (2, 3)이 아닙니다\"\n",
    "assert tensor.dtype == torch.float32, \"dtype이 float32가 아닙니다\"\n",
    "assert str(tensor.device) == 'cpu', \"device가 cpu가 아닙니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 텐서의 속성에 직접 접근합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- `tensor.shape`: 텐서의 차원 크기 (torch.Size 객체)\n",
    "- `tensor.dtype`: 데이터 타입 (float32, int64 등)\n",
    "- `tensor.device`: 텐서가 위치한 장치 (cpu 또는 cuda:0 등)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# size() 메서드 사용 (shape와 동일)\n",
    "print(f\"Size: {tensor.size()}\")\n",
    "\n",
    "# 개별 차원 크기\n",
    "print(f\"행 수: {tensor.shape[0]}\")\n",
    "print(f\"열 수: {tensor.shape[1]}\")\n",
    "```\n",
    "\n",
    "**흔한 실수**: `shape`는 속성이고 `size()`는 메서드입니다. 둘 다 동일한 결과를 반환합니다.\n",
    "\n",
    "**실무 팁**: 디버깅 시 항상 shape, dtype, device를 확인하세요. 연산 오류의 대부분은 이 세 가지 불일치에서 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q3. 텐서 연산 (기본)\n",
    "\n",
    "**문제**: 두 행렬 A와 B의 행렬 곱을 계산하세요.\n",
    "\n",
    "```python\n",
    "A = [[1, 2], [3, 4]]\n",
    "B = [[5, 6], [7, 8]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 정답\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(f\"A:\\n{A}\")\n",
    "print(f\"\\nB:\\n{B}\")\n",
    "\n",
    "# 방법 1: @ 연산자 (권장)\n",
    "result1 = A @ B\n",
    "print(f\"\\nA @ B:\\n{result1}\")\n",
    "\n",
    "# 방법 2: torch.matmul()\n",
    "result2 = torch.matmul(A, B)\n",
    "print(f\"\\ntorch.matmul(A, B):\\n{result2}\")\n",
    "\n",
    "# 방법 3: tensor.mm() (2D 전용)\n",
    "result3 = A.mm(B)\n",
    "print(f\"\\nA.mm(B):\\n{result3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "expected = torch.tensor([[19, 22], [43, 50]], dtype=torch.float32)\n",
    "assert torch.allclose(result1, expected), \"행렬 곱 결과가 올바르지 않습니다\"\n",
    "assert torch.allclose(result2, expected), \"행렬 곱 결과가 올바르지 않습니다\"\n",
    "assert torch.allclose(result3, expected), \"행렬 곱 결과가 올바르지 않습니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 행렬 곱은 `@` 연산자, `torch.matmul()`, 또는 `mm()` 메서드를 사용합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- 행렬 곱: (m x k) @ (k x n) = (m x n)\n",
    "- 수동 계산: `C[i][j] = sum(A[i][k] * B[k][j] for k)`\n",
    "  - C[0][0] = 1*5 + 2*7 = 19\n",
    "  - C[0][1] = 1*6 + 2*8 = 22\n",
    "  - C[1][0] = 3*5 + 4*7 = 43\n",
    "  - C[1][1] = 3*6 + 4*8 = 50\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 요소별 곱 (element-wise)과 혼동 주의!\n",
    "elementwise = A * B  # [[5, 12], [21, 32]] - 다른 결과!\n",
    "```\n",
    "\n",
    "**흔한 실수**: `A * B`는 요소별 곱이지 행렬 곱이 아닙니다. 행렬 곱은 반드시 `@` 또는 `matmul`을 사용하세요.\n",
    "\n",
    "**실무 팁**: `@` 연산자가 가장 간결하고 가독성이 좋습니다. 3D 이상의 배치 행렬 곱에는 `matmul`을 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q4. NumPy 변환 (응용)\n",
    "\n",
    "**문제**: NumPy 배열을 PyTorch 텐서로 변환하고, 다시 NumPy로 변환하세요. 메모리 공유 여부를 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 정답\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"원본 NumPy 배열: {np_array}\")\n",
    "\n",
    "# 1. NumPy -> PyTorch (from_numpy: 메모리 공유)\n",
    "tensor_shared = torch.from_numpy(np_array)\n",
    "print(f\"\\nfrom_numpy 변환: {tensor_shared}\")\n",
    "\n",
    "# 2. PyTorch -> NumPy (numpy(): 메모리 공유)\n",
    "np_back = tensor_shared.numpy()\n",
    "print(f\"numpy() 변환: {np_back}\")\n",
    "\n",
    "# 메모리 공유 확인\n",
    "print(\"\\n===== 메모리 공유 확인 =====\")\n",
    "np_array[0] = 100.0\n",
    "print(f\"NumPy 배열 수정 후: {np_array}\")\n",
    "print(f\"PyTorch 텐서 (공유): {tensor_shared}\")\n",
    "print(f\"NumPy 변환 결과: {np_back}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert tensor_shared[0].item() == 100.0, \"메모리 공유가 동작하지 않습니다\"\n",
    "assert np_back[0] == 100.0, \"메모리 공유가 동작하지 않습니다\"\n",
    "\n",
    "# 독립 복사 테스트\n",
    "np_array2 = np.array([1.0, 2.0, 3.0])\n",
    "tensor_copy = torch.tensor(np_array2)  # 복사본 생성\n",
    "np_array2[0] = 999.0\n",
    "assert tensor_copy[0].item() == 1.0, \"torch.tensor()는 복사본을 생성해야 합니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: `from_numpy()`와 `numpy()` 메서드를 사용하여 변환합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- `torch.from_numpy()`: NumPy -> PyTorch (메모리 공유)\n",
    "- `tensor.numpy()`: PyTorch -> NumPy (메모리 공유)\n",
    "- 메모리 공유: 한쪽을 수정하면 다른 쪽도 변경됨\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 독립적인 복사본 생성\n",
    "tensor_copy = torch.tensor(np_array)  # 메모리 복사\n",
    "np_copy = tensor.clone().numpy()      # clone 후 변환\n",
    "```\n",
    "\n",
    "**흔한 실수**: `from_numpy()`로 변환 후 원본 NumPy 배열을 수정하면 텐서도 변경됩니다. 의도치 않은 데이터 변경에 주의하세요.\n",
    "\n",
    "**실무 팁**: 데이터 전처리 후 학습에 사용할 때는 `torch.tensor()`로 복사본을 생성하는 것이 안전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q5. GPU 이동 (응용)\n",
    "\n",
    "**문제**: 텐서를 GPU로 이동하고 (GPU가 없으면 CPU 유지), device를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 정답\n",
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "print(f\"원본 텐서:\\n{x}\")\n",
    "print(f\"원본 device: {x.device}\")\n",
    "\n",
    "# 1. device 설정 (GPU 있으면 cuda, 없으면 cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n사용할 device: {device}\")\n",
    "\n",
    "# 2. 텐서를 device로 이동\n",
    "x_device = x.to(device)\n",
    "print(f\"이동 후 device: {x_device.device}\")\n",
    "\n",
    "# 3. CPU로 다시 이동 (필요한 경우)\n",
    "x_cpu = x_device.cpu()\n",
    "print(f\"CPU로 이동: {x_cpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert str(x_cpu.device) == 'cpu', \"CPU로 이동 실패\"\n",
    "if torch.cuda.is_available():\n",
    "    assert 'cuda' in str(x_device.device), \"GPU로 이동 실패\"\n",
    "else:\n",
    "    assert str(x_device.device) == 'cpu', \"CPU device 확인 실패\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: `torch.device`로 대상 장치를 지정하고, `to()` 메서드로 이동합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- `torch.cuda.is_available()`: GPU 사용 가능 여부 확인\n",
    "- `tensor.to(device)`: 텐서를 지정된 장치로 이동\n",
    "- `tensor.cpu()`: CPU로 이동 (축약형)\n",
    "- `tensor.cuda()`: GPU로 이동 (축약형)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 축약형\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.cuda()\n",
    "    x_back = x_gpu.cpu()\n",
    "\n",
    "# 생성 시 바로 device 지정\n",
    "y = torch.randn(3, 3, device=device)\n",
    "```\n",
    "\n",
    "**흔한 실수**: GPU 텐서와 CPU 텐서 간의 연산은 오류가 발생합니다. 연산 전에 반드시 같은 device로 이동하세요.\n",
    "\n",
    "**실무 팁**: 학습 코드에서는 항상 `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')` 패턴을 사용하여 GPU 유무에 관계없이 동작하도록 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q6. 텐서 인덱싱 (응용)\n",
    "\n",
    "**문제**: 아래 텐서에서 5보다 큰 값만 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 정답\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 8, 3], [4, 2, 9], [7, 6, 5]])\n",
    "print(f\"원본 텐서:\\n{x}\")\n",
    "\n",
    "# 1. 조건 마스크 생성\n",
    "mask = x > 5\n",
    "print(f\"\\n마스크 (x > 5):\\n{mask}\")\n",
    "\n",
    "# 2. Boolean 인덱싱으로 값 추출\n",
    "result = x[mask]\n",
    "print(f\"\\n5보다 큰 값: {result}\")\n",
    "\n",
    "# 한 줄로 표현\n",
    "result_oneline = x[x > 5]\n",
    "print(f\"한 줄 표현: {result_oneline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "expected = torch.tensor([8, 9, 7, 6])\n",
    "assert torch.all(result == expected), \"추출 결과가 올바르지 않습니다\"\n",
    "assert torch.all(result > 5), \"모든 값이 5보다 커야 합니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 조건식으로 Boolean 마스크를 생성하고, 마스크로 인덱싱합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- Boolean 인덱싱: `tensor[조건]`으로 조건을 만족하는 요소만 추출\n",
    "- 결과는 1D 텐서 (평탄화됨)\n",
    "- 원본 텐서의 순서대로 추출 (행 우선)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# torch.where로 인덱스 얻기\n",
    "indices = torch.where(x > 5)\n",
    "print(f\"인덱스: {indices}\")\n",
    "\n",
    "# torch.masked_select\n",
    "result = torch.masked_select(x, x > 5)\n",
    "```\n",
    "\n",
    "**흔한 실수**: Boolean 인덱싱 결과는 항상 1D 텐서입니다. 원본 shape을 유지하려면 `torch.where(condition, x, default)`를 사용하세요.\n",
    "\n",
    "**실무 팁**: 이상치 필터링, 특정 클래스 샘플 추출 등에 Boolean 인덱싱이 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q7. Autograd 이해 (복합)\n",
    "\n",
    "**문제**: y = x^3 + 2x^2 + 1에서 x=2일 때 dy/dx를 Autograd로 계산하세요.\n",
    "\n",
    "힌트: dy/dx = 3x^2 + 4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 정답\n",
    "import torch\n",
    "\n",
    "# 1. 그래디언트 추적 활성화\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x.item()}\")\n",
    "\n",
    "# 2. 함수 계산 (순전파)\n",
    "y = x**3 + 2*x**2 + 1\n",
    "print(f\"y = x^3 + 2x^2 + 1 = {y.item()}\")\n",
    "\n",
    "# 3. 역전파로 그래디언트 계산\n",
    "y.backward()\n",
    "\n",
    "# 4. 그래디언트 확인\n",
    "print(f\"\\ndy/dx (Autograd): {x.grad.item()}\")\n",
    "\n",
    "# 수동 검증: dy/dx = 3x^2 + 4x\n",
    "manual_grad = 3 * (2.0)**2 + 4 * (2.0)\n",
    "print(f\"dy/dx (수동 계산): 3*2^2 + 4*2 = {manual_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert abs(x.grad.item() - 20.0) < 1e-6, \"그래디언트가 20이 아닙니다\"\n",
    "assert abs(x.grad.item() - manual_grad) < 1e-6, \"Autograd와 수동 계산이 다릅니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: `requires_grad=True`로 텐서를 생성하고, 연산 후 `backward()`를 호출합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- 미분: y = x^3 + 2x^2 + 1 -> dy/dx = 3x^2 + 4x\n",
    "- x=2 대입: 3*(2)^2 + 4*(2) = 12 + 8 = 20\n",
    "- Autograd: 계산 그래프를 따라 역전파로 미분 계산\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# torch.autograd.grad 직접 사용\n",
    "grad = torch.autograd.grad(y, x)\n",
    "print(grad[0])  # 20.0\n",
    "```\n",
    "\n",
    "**흔한 실수**: `backward()`는 스칼라에 대해서만 호출 가능합니다. 벡터 출력의 경우 `backward(gradient)`로 gradient 인자를 전달해야 합니다.\n",
    "\n",
    "**실무 팁**: 손실 함수는 항상 스칼라를 반환하도록 설계해야 `backward()`를 직접 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q8. nn.Module 정의 (복합)\n",
    "\n",
    "**문제**: 3개의 은닉층을 가진 MLP를 정의하세요.\n",
    "\n",
    "- 입력: 10\n",
    "- 은닉층: 64 -> 32 -> 16\n",
    "- 출력: 2\n",
    "- 활성화: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 방법 1: 클래스로 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(10, 64)   # 입력 -> 첫 번째 은닉층\n",
    "        self.fc2 = nn.Linear(64, 32)   # 첫 번째 -> 두 번째 은닉층\n",
    "        self.fc3 = nn.Linear(32, 16)   # 두 번째 -> 세 번째 은닉층\n",
    "        self.fc4 = nn.Linear(16, 2)    # 세 번째 -> 출력층\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # 출력층은 활성화 없음\n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "model = MLP()\n",
    "print(\"방법 1 (클래스):\")\n",
    "print(model)\n",
    "\n",
    "# 테스트\n",
    "x = torch.randn(4, 10)\n",
    "output = model(x)\n",
    "print(f\"\\n입력: {x.shape}\")\n",
    "print(f\"출력: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2: nn.Sequential로 간단하게\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 2)\n",
    ")\n",
    "\n",
    "print(\"방법 2 (Sequential):\")\n",
    "print(model_seq)\n",
    "\n",
    "# 테스트\n",
    "output_seq = model_seq(x)\n",
    "print(f\"\\n출력: {output_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert output.shape == torch.Size([4, 2]), \"출력 shape이 (4, 2)가 아닙니다\"\n",
    "assert output_seq.shape == torch.Size([4, 2]), \"Sequential 출력 shape이 (4, 2)가 아닙니다\"\n",
    "\n",
    "# 파라미터 수 확인\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"총 파라미터 수: {total_params}\")\n",
    "# 10*64+64 + 64*32+32 + 32*16+16 + 16*2+2 = 640+64 + 2048+32 + 512+16 + 32+2 = 3346\n",
    "assert total_params == 3346, f\"파라미터 수가 올바르지 않습니다: {total_params}\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: `nn.Module`을 상속받아 `__init__`과 `forward` 메서드를 구현합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- MLP 구조: 입력 -> (Linear -> ReLU) x 3 -> Linear -> 출력\n",
    "- 은닉층 크기: 64, 32, 16\n",
    "- 출력층에는 보통 활성화 함수를 적용하지 않음 (분류 시 CrossEntropyLoss에 Softmax 포함)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# nn.ModuleList 사용\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims=[10, 64, 32, 16, 2]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dims[i], dims[i+1]) \n",
    "            for i in range(len(dims)-1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "```\n",
    "\n",
    "**흔한 실수**: `nn.Sequential` 내부에서 `torch.relu()` 함수 대신 `nn.ReLU()` 모듈을 사용해야 합니다.\n",
    "\n",
    "**실무 팁**: 복잡한 분기 로직이 필요하면 클래스, 단순한 순차 연결은 `nn.Sequential`을 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q9. Dataset과 DataLoader (종합)\n",
    "\n",
    "**문제**: 1000개의 랜덤 샘플(특성 5개)과 레이블로 Dataset을 만들고, batch_size=32인 DataLoader를 생성하세요. 첫 번째 배치를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 정답\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. 커스텀 Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_features):\n",
    "        # 랜덤 데이터 생성\n",
    "        self.X = torch.randn(num_samples, num_features)\n",
    "        self.y = torch.randint(0, 2, (num_samples,))  # 이진 레이블\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 2. Dataset 생성\n",
    "dataset = CustomDataset(num_samples=1000, num_features=5)\n",
    "print(f\"Dataset 크기: {len(dataset)}\")\n",
    "print(f\"샘플 예시: X shape={dataset[0][0].shape}, y={dataset[0][1]}\")\n",
    "\n",
    "# 3. DataLoader 생성\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "print(f\"\\n총 배치 수: {len(dataloader)}\")\n",
    "\n",
    "# 4. 첫 번째 배치 출력\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "    print(f\"\\n첫 번째 배치:\")\n",
    "    print(f\"  X_batch shape: {X_batch.shape}\")\n",
    "    print(f\"  y_batch shape: {y_batch.shape}\")\n",
    "    print(f\"  X_batch[:3]:\\n{X_batch[:3]}\")\n",
    "    print(f\"  y_batch[:10]: {y_batch[:10]}\")\n",
    "    break  # 첫 번째 배치만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "assert len(dataset) == 1000, \"Dataset 크기가 1000이 아닙니다\"\n",
    "assert dataset[0][0].shape == torch.Size([5]), \"특성 수가 5가 아닙니다\"\n",
    "assert X_batch.shape == torch.Size([32, 5]), \"배치 shape이 올바르지 않습니다\"\n",
    "assert y_batch.shape == torch.Size([32]), \"레이블 shape이 올바르지 않습니다\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: `Dataset` 클래스를 상속받아 `__len__`과 `__getitem__`을 구현하고, `DataLoader`로 감쌉니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- `Dataset`: 데이터 접근 인터페이스\n",
    "  - `__len__()`: 데이터셋 크기 반환\n",
    "  - `__getitem__(idx)`: 인덱스로 샘플 반환\n",
    "- `DataLoader`: 배치 처리, 셔플링, 병렬 로딩\n",
    "  - `batch_size`: 배치 크기\n",
    "  - `shuffle`: 에포크마다 셔플\n",
    "  - `num_workers`: 병렬 로딩 (>0이면 멀티프로세스)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# TensorDataset 사용 (간단한 경우)\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "X = torch.randn(1000, 5)\n",
    "y = torch.randint(0, 2, (1000,))\n",
    "dataset = TensorDataset(X, y)\n",
    "```\n",
    "\n",
    "**흔한 실수**: `__getitem__`에서 인덱스 범위를 벗어나면 오류가 발생합니다. 항상 `len`과 일관되게 구현하세요.\n",
    "\n",
    "**실무 팁**: 대용량 데이터는 `__getitem__`에서 그때그때 로딩하여 메모리 효율을 높이세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q10. 학습 루프 작성 (종합)\n",
    "\n",
    "**문제**: 아래 데이터로 선형 회귀 모델을 학습하세요.\n",
    "\n",
    "요구사항:\n",
    "1. y = 3x - 2의 관계를 학습\n",
    "2. nn.Linear 모델 사용\n",
    "3. MSE 손실, SGD 옵티마이저\n",
    "4. 100 에포크 학습 후 weight와 bias 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 데이터 생성\n",
    "torch.manual_seed(42)\n",
    "X = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "y = 3 * X - 2 + 0.1 * torch.randn_like(X)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"목표: y = 3x - 2\")\n",
    "\n",
    "# 1. 모델 정의\n",
    "model = nn.Linear(1, 1)  # 입력 1, 출력 1\n",
    "print(f\"\\n학습 전 파라미터:\")\n",
    "print(f\"  weight: {model.weight.data.item():.4f}\")\n",
    "print(f\"  bias: {model.bias.data.item():.4f}\")\n",
    "\n",
    "# 2. 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 학습 루프\n",
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 순전파: 예측\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 역전파\n",
    "    optimizer.zero_grad()  # 그래디언트 초기화\n",
    "    loss.backward()        # 그래디언트 계산\n",
    "    optimizer.step()       # 파라미터 업데이트\n",
    "    \n",
    "    # 진행 상황 출력\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={loss.item():.4f}, w={model.weight.item():.4f}, b={model.bias.item():.4f}\")\n",
    "\n",
    "# 4. 최종 결과\n",
    "print(f\"\\n===== 학습 완료 =====\")\n",
    "print(f\"최종 weight: {model.weight.data.item():.4f} (목표: 3.0)\")\n",
    "print(f\"최종 bias: {model.bias.data.item():.4f} (목표: -2.0)\")\n",
    "print(f\"최종 loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "final_weight = model.weight.data.item()\n",
    "final_bias = model.bias.data.item()\n",
    "\n",
    "assert abs(final_weight - 3.0) < 0.1, f\"weight가 3.0에 가깝지 않습니다: {final_weight}\"\n",
    "assert abs(final_bias - (-2.0)) < 0.1, f\"bias가 -2.0에 가깝지 않습니다: {final_bias}\"\n",
    "assert losses[-1] < 0.02, f\"최종 손실이 너무 높습니다: {losses[-1]}\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# 손실 곡선\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(epochs),\n",
    "    'Loss': losses\n",
    "})\n",
    "\n",
    "fig1 = px.line(loss_df, x='Epoch', y='Loss', title='학습 손실 곡선')\n",
    "fig1.update_layout(template='plotly_white')\n",
    "fig1.show()\n",
    "\n",
    "# 예측 vs 실제\n",
    "with torch.no_grad():\n",
    "    y_pred_final = model(X)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    'x': X.squeeze().numpy(),\n",
    "    'y_true': y.squeeze().numpy(),\n",
    "    'y_pred': y_pred_final.squeeze().numpy()\n",
    "})\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(x=pred_df['x'], y=pred_df['y_true'], mode='markers', name='실제', opacity=0.6))\n",
    "fig2.add_trace(go.Scatter(x=pred_df['x'], y=pred_df['y_pred'], mode='lines', name='예측', line=dict(color='red')))\n",
    "fig2.update_layout(title='예측 vs 실제', xaxis_title='x', yaxis_title='y', template='plotly_white')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 표준 PyTorch 학습 루프를 작성합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "학습 루프의 3단계:\n",
    "1. **순전파**: `output = model(input)` - 예측값 계산\n",
    "2. **손실 계산**: `loss = criterion(output, target)` - 오차 측정\n",
    "3. **역전파 + 업데이트**:\n",
    "   - `optimizer.zero_grad()` - 이전 그래디언트 초기화\n",
    "   - `loss.backward()` - 그래디언트 계산\n",
    "   - `optimizer.step()` - 파라미터 업데이트\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# DataLoader 사용 (미니배치 학습)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**흔한 실수**: \n",
    "- `optimizer.zero_grad()`를 빠뜨리면 그래디언트가 누적됩니다.\n",
    "- `loss.backward()` 전에 `zero_grad()`를 호출해야 합니다.\n",
    "\n",
    "**실무 팁**: \n",
    "- 학습률(lr)은 모델 수렴 속도에 큰 영향을 미칩니다. 너무 크면 발산, 너무 작으면 느림.\n",
    "- 실제 프로젝트에서는 학습률 스케줄러와 조기 종료를 함께 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 함수 | 실무 활용 |\n",
    "|-----|----------|----------|\n",
    "| 텐서 생성 | torch.tensor(), zeros(), ones(), rand() | 데이터 초기화 |\n",
    "| 속성 | shape, dtype, device | 디버깅, 호환성 확인 |\n",
    "| NumPy 변환 | from_numpy(), .numpy() | 데이터 전처리 연동 |\n",
    "| GPU 이동 | .to(device), .cuda(), .cpu() | 학습 가속 |\n",
    "| 인덱싱 | 슬라이싱, Boolean indexing | 배치 추출, 필터링 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 메서드 | 언제 사용? |\n",
    "|-----|-----------|----------|\n",
    "| Autograd | requires_grad=True, backward() | 그래디언트 자동 계산 |\n",
    "| nn.Module | __init__(), forward() | 신경망 정의 |\n",
    "| Dataset | __len__(), __getitem__() | 데이터 관리 |\n",
    "| DataLoader | batch_size, shuffle | 배치 처리 |\n",
    "\n",
    "### 학습 루프 핵심 패턴\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        output = model(X_batch)           # 순전파\n",
    "        loss = criterion(output, y_batch) # 손실 계산\n",
    "        \n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        loss.backward()        # 역전파\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **device 일관성**: 모든 텐서와 모델이 같은 device에 있는지 확인\n",
    "2. **grad 초기화**: 매 iteration마다 `optimizer.zero_grad()` 필수\n",
    "3. **eval() 모드**: 평가 시 `model.eval()` + `torch.no_grad()`\n",
    "4. **dtype 주의**: 연산 전 dtype 일치 확인\n",
    "5. **메모리 관리**: 불필요한 텐서는 `del`로 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
