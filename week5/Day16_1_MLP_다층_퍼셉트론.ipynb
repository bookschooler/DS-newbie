{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_1: MLP (다층 퍼셉트론)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 퍼셉트론의 구조와 동작 원리 이해하기\n",
    "2. 활성화 함수(Sigmoid, Tanh, ReLU) 비교하기\n",
    "3. 손실 함수(MSE, Cross-Entropy) 이해하기\n",
    "4. 경사 하강법 개념 이해하기\n",
    "5. XOR 문제와 MLP의 필요성 이해하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. 역전파 알고리즘 개념적으로 이해하기\n",
    "2. PyTorch로 MLP 구현하기\n",
    "3. 옵티마이저(SGD, Adam) 비교하기\n",
    "4. MNIST 손글씨 분류 실습하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| 퍼셉트론 | 신경망의 기본 단위 | 모든 딥러닝 모델의 구성 요소 |\n",
    "| 활성화 함수 | 비선형성 추가 | ReLU로 깊은 네트워크 학습 가능 |\n",
    "| 손실 함수 | 학습 목표 정의 | 분류/회귀에 맞는 손실 선택 |\n",
    "| MLP | 정형 데이터 분류/회귀 | 고객 이탈 예측, 가격 예측 |\n",
    "\n",
    "**분석가 관점**: MLP는 딥러닝의 기본 구조입니다. CNN, RNN, Transformer 모두 MLP를 기반으로 확장됩니다. 이 개념을 확실히 이해하면 모든 딥러닝 아키텍처의 동작 원리를 파악할 수 있습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 퍼셉트론 (Perceptron)\n",
    "\n",
    "### 생물학적 뉴런 비유\n",
    "\n",
    "```\n",
    "생물학적 뉴런:           인공 뉴런 (퍼셉트론):\n",
    "  수상돌기 (입력)    ->    입력 (x1, x2, ...)\n",
    "  세포체 (처리)      ->    가중합 + 편향\n",
    "  축삭돌기 (출력)    ->    활성화 함수 -> 출력\n",
    "```\n",
    "\n",
    "### 퍼셉트론 수식\n",
    "\n",
    "```\n",
    "y = f(w1*x1 + w2*x2 + ... + wn*xn + b)\n",
    "y = f(sum(wi*xi) + b)\n",
    "y = f(W @ X + b)\n",
    "\n",
    "- X: 입력 벡터\n",
    "- W: 가중치 벡터 (학습되는 파라미터)\n",
    "- b: 편향 (학습되는 파라미터)\n",
    "- f: 활성화 함수\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 퍼셉트론 구현 (NumPy)\n",
    "def perceptron(x, w, b, activation='step'):\n",
    "    \"\"\"\n",
    "    단일 퍼셉트론\n",
    "    x: 입력 벡터\n",
    "    w: 가중치 벡터\n",
    "    b: 편향\n",
    "    activation: 활성화 함수 종류\n",
    "    \"\"\"\n",
    "    # 가중합 계산\n",
    "    z = np.dot(w, x) + b\n",
    "    \n",
    "    # 활성화 함수 적용\n",
    "    if activation == 'step':  # 계단 함수 (원래 퍼셉트론)\n",
    "        return 1 if z >= 0 else 0\n",
    "    elif activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        return z  # 선형 (활성화 없음)\n",
    "\n",
    "# AND 게이트 구현\n",
    "print(\"AND 게이트 (퍼셉트론)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 가중치와 편향 (수동 설정)\n",
    "w_and = np.array([0.5, 0.5])\n",
    "b_and = -0.7\n",
    "\n",
    "# 테스트\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "for x in inputs:\n",
    "    output = perceptron(np.array(x), w_and, b_and, 'step')\n",
    "    print(f\"AND({x[0]}, {x[1]}) = {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR 게이트 구현\n",
    "print(\"OR 게이트 (퍼셉트론)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "w_or = np.array([0.5, 0.5])\n",
    "b_or = -0.2\n",
    "\n",
    "for x in inputs:\n",
    "    output = perceptron(np.array(x), w_or, b_or, 'step')\n",
    "    print(f\"OR({x[0]}, {x[1]}) = {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 퍼셉트론의 결정 경계\n",
    "\n",
    "퍼셉트론은 **선형 결정 경계**를 학습합니다:\n",
    "- w1*x1 + w2*x2 + b = 0\n",
    "- 이 직선이 두 클래스를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND 게이트의 결정 경계 시각화\n",
    "x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "\n",
    "# 결정 경계: w1*x1 + w2*x2 + b = 0\n",
    "# x2 = -(w1*x1 + b) / w2\n",
    "x2_boundary_and = -(w_and[0] * x1_range + b_and) / w_and[1]\n",
    "x2_boundary_or = -(w_or[0] * x1_range + b_or) / w_or[1]\n",
    "\n",
    "# 데이터 포인트\n",
    "points = pd.DataFrame({\n",
    "    'x1': [0, 0, 1, 1],\n",
    "    'x2': [0, 1, 0, 1],\n",
    "    'AND': [0, 0, 0, 1],\n",
    "    'OR': [0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['AND 게이트', 'OR 게이트'])\n",
    "\n",
    "# AND 게이트\n",
    "colors_and = ['blue' if y == 0 else 'red' for y in points['AND']]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=points['x1'], y=points['x2'], mode='markers', \n",
    "               marker=dict(size=15, color=colors_and), name='AND 데이터'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x1_range, y=x2_boundary_and, mode='lines',\n",
    "               line=dict(color='green', dash='dash'), name='결정 경계'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# OR 게이트\n",
    "colors_or = ['blue' if y == 0 else 'red' for y in points['OR']]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=points['x1'], y=points['x2'], mode='markers',\n",
    "               marker=dict(size=15, color=colors_or), name='OR 데이터'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x1_range, y=x2_boundary_or, mode='lines',\n",
    "               line=dict(color='green', dash='dash'), name='결정 경계'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[-0.5, 1.5], title_text='x1')\n",
    "fig.update_yaxes(range=[-0.5, 1.5], title_text='x2')\n",
    "fig.update_layout(title='퍼셉트론의 선형 결정 경계', height=400, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 활성화 함수 (Activation Functions)\n",
    "\n",
    "### 왜 활성화 함수가 필요한가?\n",
    "\n",
    "활성화 함수 없이 여러 층을 쌓으면:\n",
    "```\n",
    "y = W2 @ (W1 @ x) = (W2 @ W1) @ x = W' @ x\n",
    "```\n",
    "결국 하나의 선형 변환과 동일! **비선형 활성화 함수**가 있어야 깊은 네트워크의 의미가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 정의\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# 시각화용 데이터\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# 활성화 함수 비교 시각화\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=['Sigmoid', 'Tanh', 'ReLU', 'Leaky ReLU'])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=sigmoid(x), mode='lines', name='Sigmoid'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=tanh(x), mode='lines', name='Tanh'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=relu(x), mode='lines', name='ReLU'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=leaky_relu(x), mode='lines', name='Leaky ReLU'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(title='활성화 함수 비교', height=600, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수 특성 비교표\n",
    "activation_comparison = {\n",
    "    \"활성화 함수\": [\"Sigmoid\", \"Tanh\", \"ReLU\", \"Leaky ReLU\"],\n",
    "    \"출력 범위\": [\"(0, 1)\", \"(-1, 1)\", \"[0, inf)\", \"(-inf, inf)\"],\n",
    "    \"장점\": [\n",
    "        \"확률 해석 가능\",\n",
    "        \"zero-centered\",\n",
    "        \"계산 효율적, 기울기 소실 완화\",\n",
    "        \"ReLU의 dying neuron 문제 해결\"\n",
    "    ],\n",
    "    \"단점\": [\n",
    "        \"기울기 소실, 느린 학습\",\n",
    "        \"기울기 소실 가능\",\n",
    "        \"음수 입력에서 뉴런 사망\",\n",
    "        \"alpha 하이퍼파라미터 필요\"\n",
    "    ],\n",
    "    \"주 용도\": [\n",
    "        \"이진 분류 출력층\",\n",
    "        \"RNN 은닉층\",\n",
    "        \"대부분의 은닉층 (기본값)\",\n",
    "        \"ReLU 대안\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(activation_comparison)\n",
    "print(\"활성화 함수 비교표\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax: 다중 클래스 출력\n",
    "\n",
    "여러 클래스 중 하나를 선택할 때 사용합니다. 모든 출력의 합이 1이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 함수\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # 수치 안정성을 위해 max 빼기\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# 예시: 3-클래스 분류\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(\"Softmax 예시 (3-클래스 분류)\")\n",
    "print(f\"모델 출력 (logits): {logits}\")\n",
    "print(f\"Softmax 후 (확률): {probs}\")\n",
    "print(f\"확률 합계: {probs.sum():.4f}\")\n",
    "print(f\"예측 클래스: {np.argmax(probs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 손실 함수 (Loss Functions)\n",
    "\n",
    "### 손실 함수의 역할\n",
    "\n",
    "**손실 함수**: 모델의 예측이 얼마나 틀렸는지 측정\n",
    "- 작을수록 좋음\n",
    "- 학습 = 손실 함수 최소화\n",
    "\n",
    "| 문제 유형 | 손실 함수 | PyTorch |\n",
    "|----------|----------|----------|\n",
    "| 회귀 | MSE (Mean Squared Error) | nn.MSELoss() |\n",
    "| 이진 분류 | BCE (Binary Cross-Entropy) | nn.BCELoss() |\n",
    "| 다중 분류 | CE (Cross-Entropy) | nn.CrossEntropyLoss() |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MSE (Mean Squared Error) - 회귀\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# 예시\n",
    "y_true = np.array([3.0, 5.0, 2.5])\n",
    "y_pred = np.array([2.5, 5.2, 2.0])\n",
    "\n",
    "print(\"MSE (회귀)\")\n",
    "print(f\"실제값: {y_true}\")\n",
    "print(f\"예측값: {y_pred}\")\n",
    "print(f\"MSE: {mse_loss(y_pred, y_true):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Binary Cross-Entropy - 이진 분류\n",
    "def bce_loss(y_pred, y_true):\n",
    "    epsilon = 1e-15  # 수치 안정성\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# 예시\n",
    "y_true_bin = np.array([1, 0, 1, 1])\n",
    "y_pred_prob = np.array([0.9, 0.1, 0.8, 0.7])  # Sigmoid 출력\n",
    "\n",
    "print(\"Binary Cross-Entropy (이진 분류)\")\n",
    "print(f\"실제 레이블: {y_true_bin}\")\n",
    "print(f\"예측 확률: {y_pred_prob}\")\n",
    "print(f\"BCE Loss: {bce_loss(y_pred_prob, y_true_bin):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Cross-Entropy - 다중 분류\n",
    "def cross_entropy_loss(y_pred_probs, y_true_idx):\n",
    "    \"\"\"\n",
    "    y_pred_probs: Softmax 출력 (확률 분포)\n",
    "    y_true_idx: 정답 클래스 인덱스\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    # 정답 클래스의 확률에 -log 적용\n",
    "    return -np.log(y_pred_probs[y_true_idx] + epsilon)\n",
    "\n",
    "# 예시: 3-클래스 분류\n",
    "y_pred_softmax = np.array([0.7, 0.2, 0.1])  # Softmax 출력\n",
    "y_true_class = 0  # 정답 클래스\n",
    "\n",
    "print(\"Cross-Entropy (다중 분류)\")\n",
    "print(f\"예측 확률: {y_pred_softmax}\")\n",
    "print(f\"정답 클래스: {y_true_class}\")\n",
    "print(f\"CE Loss: {cross_entropy_loss(y_pred_softmax, y_true_class):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 손실 함수와 예측 확률의 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 클래스에 대한 예측 확률 vs 손실\n",
    "probs = np.linspace(0.01, 0.99, 100)\n",
    "ce_losses = -np.log(probs)\n",
    "\n",
    "fig = px.line(x=probs, y=ce_losses, labels={'x': '정답 클래스의 예측 확률', 'y': 'Cross-Entropy Loss'})\n",
    "fig.update_layout(\n",
    "    title='예측 확률과 Cross-Entropy Loss의 관계',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.add_annotation(x=0.9, y=-np.log(0.9), text=\"좋은 예측\\n(낮은 손실)\", showarrow=True, arrowhead=1)\n",
    "fig.add_annotation(x=0.1, y=-np.log(0.1), text=\"나쁜 예측\\n(높은 손실)\", showarrow=True, arrowhead=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 경사 하강법 (Gradient Descent)\n",
    "\n",
    "### 직관적 이해: 언덕 내려가기\n",
    "\n",
    "```\n",
    "목표: 손실 함수의 최솟값 찾기\n",
    "방법: 현재 위치에서 가장 가파르게 내려가는 방향으로 이동\n",
    "\n",
    "새 위치 = 현재 위치 - 학습률 * 기울기\n",
    "w_new = w_old - lr * dL/dw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 시각화: y = x^2의 최솟값 찾기\n",
    "def loss_function(x):\n",
    "    return x ** 2\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * x  # d(x^2)/dx = 2x\n",
    "\n",
    "# 경사 하강법 실행\n",
    "x_init = 4.0  # 시작점\n",
    "lr = 0.1      # 학습률\n",
    "iterations = 20\n",
    "\n",
    "x_history = [x_init]\n",
    "loss_history = [loss_function(x_init)]\n",
    "\n",
    "x = x_init\n",
    "for i in range(iterations):\n",
    "    grad = gradient(x)\n",
    "    x = x - lr * grad  # 경사 하강\n",
    "    x_history.append(x)\n",
    "    loss_history.append(loss_function(x))\n",
    "\n",
    "print(f\"시작점: x = {x_init}\")\n",
    "print(f\"최종점: x = {x_history[-1]:.6f}\")\n",
    "print(f\"최솟값: f(x) = {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 경로 시각화\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "y_range = loss_function(x_range)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# 손실 함수 곡선\n",
    "fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='L(x) = x^2'))\n",
    "\n",
    "# 경사 하강 경로\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_history, y=loss_history, mode='markers+lines',\n",
    "    marker=dict(size=10, color='red'),\n",
    "    name='경사 하강 경로'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='경사 하강법: 손실 함수의 최솟값 찾기',\n",
    "    xaxis_title='파라미터 x',\n",
    "    yaxis_title='손실 L(x)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률(Learning Rate)의 중요성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 학습률 비교\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.99]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='L(x) = x^2', line=dict(color='gray')))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    x_hist = [4.0]\n",
    "    x = 4.0\n",
    "    for _ in range(20):\n",
    "        x = x - lr * 2 * x\n",
    "        x_hist.append(x)\n",
    "    \n",
    "    y_hist = [h**2 for h in x_hist]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_hist, y=y_hist, mode='markers+lines',\n",
    "        name=f'lr={lr}', marker=dict(size=6),\n",
    "        line=dict(color=color)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='학습률(Learning Rate)에 따른 수렴 비교',\n",
    "    xaxis_title='파라미터 x',\n",
    "    yaxis_title='손실 L(x)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"학습률 가이드:\")\n",
    "print(\"- 너무 작으면: 수렴이 매우 느림\")\n",
    "print(\"- 적당하면: 안정적으로 최솟값에 수렴\")\n",
    "print(\"- 너무 크면: 발산하거나 진동\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 XOR 문제와 MLP의 필요성\n",
    "\n",
    "### 단일 퍼셉트론의 한계\n",
    "\n",
    "XOR 게이트는 **선형으로 분리 불가능**(linearly non-separable)합니다.\n",
    "\n",
    "```\n",
    "XOR 진리표:\n",
    "  (0, 0) -> 0\n",
    "  (0, 1) -> 1\n",
    "  (1, 0) -> 1\n",
    "  (1, 1) -> 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터\n",
    "xor_data = pd.DataFrame({\n",
    "    'x1': [0, 0, 1, 1],\n",
    "    'x2': [0, 1, 0, 1],\n",
    "    'XOR': [0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "fig = px.scatter(\n",
    "    xor_data, x='x1', y='x2', color='XOR',\n",
    "    title='XOR 문제: 선형 결정 경계로 분리 불가능',\n",
    "    color_continuous_scale='RdBu'\n",
    ")\n",
    "fig.update_traces(marker=dict(size=20))\n",
    "fig.update_layout(template='plotly_white')\n",
    "fig.add_annotation(text=\"어떤 직선도 이 데이터를 분리할 수 없습니다!\", \n",
    "                   x=0.5, y=-0.3, showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 퍼셉트론으로 XOR 시도 -> 실패\n",
    "print(\"단일 퍼셉트론으로 XOR 시도\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 어떤 가중치를 써도 XOR를 학습할 수 없음\n",
    "# w1, w2, b 조합을 시도해봐도 4개 케이스 모두 맞출 수 없음\n",
    "\n",
    "w_attempts = [\n",
    "    ([1, 1], -0.5),   # OR처럼\n",
    "    ([1, 1], -1.5),   # AND처럼\n",
    "    ([-1, -1], 0.5),  # NOR처럼\n",
    "]\n",
    "\n",
    "for w, b in w_attempts:\n",
    "    print(f\"\\nw={w}, b={b}\")\n",
    "    correct = 0\n",
    "    for i, row in xor_data.iterrows():\n",
    "        x = np.array([row['x1'], row['x2']])\n",
    "        pred = perceptron(x, np.array(w), b, 'step')\n",
    "        actual = row['XOR']\n",
    "        match = \"O\" if pred == actual else \"X\"\n",
    "        print(f\"  ({row['x1']}, {row['x2']}) -> 예측: {pred}, 실제: {actual} [{match}]\")\n",
    "        if pred == actual:\n",
    "            correct += 1\n",
    "    print(f\"  정확도: {correct}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP로 XOR 해결\n",
    "\n",
    "**은닉층(Hidden Layer)**을 추가하면 비선형 결정 경계를 학습할 수 있습니다.\n",
    "\n",
    "```\n",
    "단일 퍼셉트론:       MLP (은닉층 1개):\n",
    "  x1 ─┬─ y            x1 ─┬─ h1 ─┬─ y\n",
    "      │                   │      │\n",
    "  x2 ─┘              x2 ─┴─ h2 ─┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch로 XOR 문제 해결\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR 데이터 준비\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 4),    # 입력 2 -> 은닉 4\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),    # 은닉 4 -> 출력 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model_xor = XOR_MLP()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_xor.parameters(), lr=0.1)\n",
    "\n",
    "print(\"XOR MLP 모델 구조:\")\n",
    "print(model_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 순전파\n",
    "    y_pred = model_xor(X_xor)\n",
    "    loss = criterion(y_pred, y_xor)\n",
    "    \n",
    "    # 역전파\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# 최종 예측\n",
    "print(\"\\n===== XOR 학습 완료 =====\")\n",
    "with torch.no_grad():\n",
    "    predictions = model_xor(X_xor)\n",
    "    for i in range(4):\n",
    "        x = X_xor[i].numpy()\n",
    "        pred = predictions[i].item()\n",
    "        actual = y_xor[i].item()\n",
    "        print(f\"XOR({int(x[0])}, {int(x[1])}) = {pred:.3f} (실제: {int(actual)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 역전파 알고리즘 (Backpropagation)\n",
    "\n",
    "### 개념적 이해\n",
    "\n",
    "**역전파**: 손실 함수의 그래디언트를 출력층에서 입력층 방향으로 계산하는 알고리즘\n",
    "\n",
    "```\n",
    "순전파 (Forward):\n",
    "  x -> h1 -> h2 -> ... -> y_pred -> Loss\n",
    "\n",
    "역전파 (Backward):\n",
    "  dL/dx <- dL/dh1 <- dL/dh2 <- ... <- dL/dy_pred <- dL/dL = 1\n",
    "```\n",
    "\n",
    "### 체인 룰 (Chain Rule)\n",
    "\n",
    "복합 함수의 미분: 각 함수의 미분을 곱합니다.\n",
    "\n",
    "```\n",
    "y = f(g(x))\n",
    "dy/dx = dy/dg * dg/dx = f'(g(x)) * g'(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 룰 예시: y = (2x + 1)^2\n",
    "# g(x) = 2x + 1\n",
    "# f(g) = g^2\n",
    "# dy/dx = dy/dg * dg/dx = 2g * 2 = 4(2x + 1)\n",
    "\n",
    "x_val = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# 순전파\n",
    "g = 2 * x_val + 1  # g(x) = 2x + 1\n",
    "y = g ** 2         # f(g) = g^2\n",
    "\n",
    "# 역전파\n",
    "y.backward()\n",
    "\n",
    "print(\"체인 룰 검증: y = (2x + 1)^2\")\n",
    "print(f\"x = {x_val.item()}\")\n",
    "print(f\"g = 2x + 1 = {g.item()}\")\n",
    "print(f\"y = g^2 = {y.item()}\")\n",
    "print(f\"\\nAutograd로 계산한 dy/dx: {x_val.grad.item()}\")\n",
    "print(f\"수동 계산: 4(2x + 1) = 4 * {g.item()} = {4 * g.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직관적 설명: 오차가 어디서 왔는가?\n",
    "\n",
    "역전파는 \"최종 오차가 각 파라미터에 얼마나 기인했는가?\"를 계산합니다.\n",
    "\n",
    "- 기여도가 큰 파라미터 -> 많이 수정\n",
    "- 기여도가 작은 파라미터 -> 조금 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 2층 네트워크에서 역전파 흐름 확인\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 단순 네트워크: 입력(1) -> 은닉(2) -> 출력(1)\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(1, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "\n",
    "# 데이터\n",
    "x = torch.tensor([[1.0]])\n",
    "y_true = torch.tensor([[3.0]])\n",
    "\n",
    "# 순전파\n",
    "y_pred = simple_net(x)\n",
    "loss = nn.MSELoss()(y_pred, y_true)\n",
    "\n",
    "print(\"순전파:\")\n",
    "print(f\"  입력: {x.item()}\")\n",
    "print(f\"  예측: {y_pred.item():.4f}\")\n",
    "print(f\"  실제: {y_true.item()}\")\n",
    "print(f\"  손실: {loss.item():.4f}\")\n",
    "\n",
    "# 역전파\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\n역전파 (각 레이어의 그래디언트):\")\n",
    "for name, param in simple_net.named_parameters():\n",
    "    print(f\"  {name}: grad shape = {param.grad.shape}\")\n",
    "    print(f\"         grad = {param.grad.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 PyTorch로 MLP 구현하기\n",
    "\n",
    "### MLP 아키텍처 설계\n",
    "\n",
    "```\n",
    "입력층 (784) -> 은닉층1 (128) -> 은닉층2 (64) -> 출력층 (10)\n",
    "         ↓           ↓            ↓           ↓\n",
    "      [ReLU]      [ReLU]       [ReLU]     [Softmax]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 클래스 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.2):\n",
    "        \"\"\"\n",
    "        MLP (Multi-Layer Perceptron)\n",
    "        \n",
    "        Args:\n",
    "            input_dim: 입력 차원\n",
    "            hidden_dims: 은닉층 차원 리스트 [128, 64]\n",
    "            output_dim: 출력 차원\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # 은닉층 추가\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # 출력층\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# MNIST용 MLP 생성\n",
    "mlp = MLP(\n",
    "    input_dim=784,        # 28x28 이미지\n",
    "    hidden_dims=[128, 64],  # 은닉층\n",
    "    output_dim=10,        # 0~9 숫자\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(\"MNIST 분류용 MLP:\")\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 수 계산\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"총 파라미터 수: {count_parameters(mlp):,}\")\n",
    "\n",
    "# 각 레이어별 파라미터\n",
    "print(\"\\n레이어별 파라미터:\")\n",
    "for name, param in mlp.named_parameters():\n",
    "    print(f\"  {name}: {param.numel():,} ({param.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 옵티마이저 비교 (SGD vs Adam)\n",
    "\n",
    "### 주요 옵티마이저\n",
    "\n",
    "| 옵티마이저 | 특징 | 장점 | 단점 |\n",
    "|-----------|------|-----|------|\n",
    "| SGD | 기본 경사 하강법 | 단순, 일반화 좋음 | 느린 수렴, lr 민감 |\n",
    "| SGD+Momentum | 관성 추가 | 진동 감소, 빠른 수렴 | 하이퍼파라미터 추가 |\n",
    "| Adam | 적응적 학습률 | 빠른 수렴, lr 덜 민감 | 일반화 약할 수 있음 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 회귀 문제로 옵티마이저 비교\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 데이터 생성\n",
    "X = torch.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = torch.sin(X) + 0.1 * torch.randn_like(X)\n",
    "\n",
    "def train_with_optimizer(optimizer_name, lr=0.01, epochs=500):\n",
    "    \"\"\"다양한 옵티마이저로 학습\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 간단한 MLP\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 1)\n",
    "    )\n",
    "    \n",
    "    # 옵티마이저 선택\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD+Momentum':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses, model\n",
    "\n",
    "# 각 옵티마이저로 학습\n",
    "sgd_losses, sgd_model = train_with_optimizer('SGD', lr=0.1)\n",
    "momentum_losses, momentum_model = train_with_optimizer('SGD+Momentum', lr=0.1)\n",
    "adam_losses, adam_model = train_with_optimizer('Adam', lr=0.01)\n",
    "\n",
    "print(\"최종 손실 비교:\")\n",
    "print(f\"  SGD:          {sgd_losses[-1]:.4f}\")\n",
    "print(f\"  SGD+Momentum: {momentum_losses[-1]:.4f}\")\n",
    "print(f\"  Adam:         {adam_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 비교\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(y=sgd_losses, mode='lines', name='SGD'))\n",
    "fig.add_trace(go.Scatter(y=momentum_losses, mode='lines', name='SGD+Momentum'))\n",
    "fig.add_trace(go.Scatter(y=adam_losses, mode='lines', name='Adam'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='옵티마이저별 학습 곡선 비교',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss (MSE)',\n",
    "    template='plotly_white',\n",
    "    yaxis_type='log'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 MNIST 손글씨 분류 실습\n",
    "\n",
    "### 데이터셋 소개\n",
    "\n",
    "- **MNIST**: 손글씨 숫자(0~9) 이미지 데이터셋\n",
    "- 훈련 데이터: 60,000개\n",
    "- 테스트 데이터: 10,000개\n",
    "- 이미지 크기: 28x28 픽셀 (흑백)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # PIL Image -> Tensor, 0~1 정규화\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 평균, 표준편차\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"훈련 데이터: {len(train_dataset)}개\")\n",
    "print(f\"테스트 데이터: {len(test_dataset)}개\")\n",
    "print(f\"이미지 shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"클래스 수: 10 (0~9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 시각화\n",
    "fig = make_subplots(rows=2, cols=5, subplot_titles=[f\"Label: {train_dataset[i][1]}\" for i in range(10)])\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    img_np = img.squeeze().numpy()\n",
    "    \n",
    "    row = i // 5 + 1\n",
    "    col = i % 5 + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=img_np, colorscale='gray', showscale=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(title='MNIST 샘플 이미지', height=400)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False, autorange='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "print(f\"훈련 배치 수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 분류 MLP 모델\n",
    "class MNIST_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),   # 28*28 = 784\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 10)      # 10개 클래스\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # (batch, 1, 28, 28) -> (batch, 784)\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MNIST_MLP().to(device)\n",
    "\n",
    "# 손실 함수, 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()  # Softmax + NLL Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"파라미터 수: {count_parameters(model):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 통계\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "epochs = 10\n",
    "\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "print(\"MNIST MLP 학습 시작\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['손실 (Loss)', '정확도 (Accuracy)'])\n",
    "\n",
    "# 손실\n",
    "fig.add_trace(go.Scatter(y=train_losses, mode='lines+markers', name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=test_losses, mode='lines+markers', name='Test Loss'), row=1, col=1)\n",
    "\n",
    "# 정확도\n",
    "fig.add_trace(go.Scatter(y=train_accs, mode='lines+markers', name='Train Acc'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=test_accs, mode='lines+markers', name='Test Acc'), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Epoch')\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Accuracy (%)', row=1, col=2)\n",
    "fig.update_layout(title='MNIST MLP 학습 곡선', height=400, template='plotly_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 테스트 데이터에서 샘플 추출\n",
    "    sample_images, sample_labels = next(iter(test_loader))\n",
    "    sample_images = sample_images[:10].to(device)\n",
    "    sample_labels = sample_labels[:10]\n",
    "    \n",
    "    outputs = model(sample_images)\n",
    "    _, predictions = outputs.max(1)\n",
    "\n",
    "# 시각화\n",
    "fig = make_subplots(rows=2, cols=5)\n",
    "\n",
    "for i in range(10):\n",
    "    img = sample_images[i].cpu().squeeze().numpy()\n",
    "    pred = predictions[i].item()\n",
    "    actual = sample_labels[i].item()\n",
    "    color = 'green' if pred == actual else 'red'\n",
    "    \n",
    "    row = i // 5 + 1\n",
    "    col = i % 5 + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=img, colorscale='gray', showscale=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        text=f\"Pred: {pred} (Actual: {actual})\",\n",
    "        x=0.5, y=-0.15, xref=f'x{i+1} domain', yref=f'y{i+1} domain',\n",
    "        showarrow=False, font=dict(color=color)\n",
    "    )\n",
    "\n",
    "fig.update_layout(title='MNIST 예측 결과', height=400)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False, autorange='reversed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 퍼셉트론 출력 계산하기 \n",
    "\n",
    "**문제**: 아래 퍼셉트론의 출력을 계산하세요 (Sigmoid 활성화 사용).\n",
    "\n",
    "- 입력: x = [1.0, 2.0]\n",
    "- 가중치: w = [0.5, -0.5]\n",
    "- 편향: b = 0.1\n",
    "\n",
    "**기대 결과**: Sigmoid 출력 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([0.5, -0.5])\n",
    "b = 0.1\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. 활성화 함수 선택하기 \n",
    "\n",
    "**문제**: 다음 상황에 적합한 활성화 함수를 선택하고, 이유를 설명하세요.\n",
    "\n",
    "1. 은닉층 (일반적인 경우)\n",
    "2. 이진 분류의 출력층 (0~1 확률)\n",
    "3. 다중 분류의 출력층 (여러 클래스 중 하나 선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 답과 이유를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 손실 함수 계산하기 \n",
    "\n",
    "**문제**: 다음 예측과 실제 값으로 BCE Loss를 수동 계산하세요.\n",
    "\n",
    "- 예측 확률: y_pred = [0.9, 0.3, 0.8]\n",
    "- 실제 레이블: y_true = [1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array([0.9, 0.3, 0.8])\n",
    "y_true = np.array([1, 0, 1])\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. 경사 하강법 구현하기 \n",
    "\n",
    "**문제**: 함수 f(x) = (x - 3)^2의 최솟값을 경사 하강법으로 찾으세요.\n",
    "\n",
    "- 시작점: x = 10\n",
    "- 학습률: 0.1\n",
    "- 반복 횟수: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. XOR MLP 수정하기 \n",
    "\n",
    "**문제**: XOR 문제를 해결하는 MLP를 은닉층 2개로 수정하세요.\n",
    "\n",
    "- 구조: 입력(2) -> 은닉(8) -> 은닉(4) -> 출력(1)\n",
    "- 활성화: ReLU (은닉), Sigmoid (출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. 역전파 그래디언트 확인하기 \n",
    "\n",
    "**문제**: y = 2x^3 - 3x^2 + x 함수에서 x=2일 때 dy/dx를 Autograd로 계산하고, 수동 계산과 비교하세요.\n",
    "\n",
    "힌트: dy/dx = 6x^2 - 6x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. 옵티마이저 선택하기 \n",
    "\n",
    "**문제**: 아래 상황에 적합한 옵티마이저를 선택하고 이유를 설명하세요.\n",
    "\n",
    "1. 빠른 프로토타이핑이 필요한 경우\n",
    "2. 일반화 성능이 중요한 최종 모델\n",
    "3. 학습률 튜닝 시간이 부족한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 답과 이유를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. MLP 구조 설계하기 \n",
    "\n",
    "**문제**: Fashion MNIST (28x28 이미지, 10개 클래스) 분류를 위한 MLP를 설계하세요.\n",
    "\n",
    "요구사항:\n",
    "- 은닉층 3개 (256, 128, 64)\n",
    "- 드롭아웃 0.3\n",
    "- BatchNorm 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. MNIST 정확도 향상하기 \n",
    "\n",
    "**문제**: MNIST MLP 모델을 수정하여 테스트 정확도 98% 이상을 달성하세요.\n",
    "\n",
    "힌트:\n",
    "- 은닉층 크기 증가\n",
    "- 에포크 증가\n",
    "- 학습률 스케줄러 사용\n",
    "- 배치 정규화 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 종합: 이진 분류 MLP 파이프라인 \n",
    "\n",
    "**문제**: sklearn의 breast cancer 데이터셋으로 이진 분류 MLP를 구축하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 데이터 표준화 (StandardScaler)\n",
    "2. MLP 모델 (은닉층 2개)\n",
    "3. Adam 옵티마이저, BCE Loss\n",
    "4. 학습 곡선 시각화\n",
    "5. 테스트 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 실무 활용 |\n",
    "|-----|----------|----------|\n",
    "| 퍼셉트론 | y = f(W @ X + b) | 신경망의 기본 단위 |\n",
    "| 활성화 함수 | 비선형성 추가 | ReLU (은닉), Sigmoid/Softmax (출력) |\n",
    "| 손실 함수 | 예측 오차 측정 | MSE (회귀), CE (분류) |\n",
    "| 경사 하강법 | w = w - lr * gradient | 손실 최소화 |\n",
    "| XOR 문제 | 단일 퍼셉트론의 한계 | MLP로 해결 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 언제 사용? |\n",
    "|-----|----------|----------|\n",
    "| 역전파 | 체인 룰로 그래디언트 계산 | 모든 신경망 학습 |\n",
    "| MLP | 입력 -> 은닉층들 -> 출력 | 정형 데이터 분류/회귀 |\n",
    "| SGD vs Adam | 기본 vs 적응적 학습률 | Adam이 기본, SGD는 일반화 |\n",
    "| MNIST | 손글씨 분류 벤치마크 | 딥러닝 입문 |\n",
    "\n",
    "### MLP 학습 핵심 패턴\n",
    "\n",
    "```python\n",
    "model = MLP(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # 분류\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        outputs = model(X_batch)          # 순전파\n",
    "        loss = criterion(outputs, y_batch) # 손실 계산\n",
    "        \n",
    "        optimizer.zero_grad()   # 그래디언트 초기화\n",
    "        loss.backward()         # 역전파\n",
    "        optimizer.step()        # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **활성화 함수**: 은닉층에는 ReLU, 출력층은 문제에 맞게 (Sigmoid, Softmax, 없음)\n",
    "2. **학습률**: Adam은 0.001, SGD는 0.01~0.1로 시작\n",
    "3. **과적합 방지**: Dropout, 조기 종료, 데이터 증강\n",
    "4. **배치 크기**: 32~128이 일반적, 메모리와 속도 트레이드오프\n",
    "5. **디버깅**: 먼저 작은 데이터로 과적합 확인 (모델이 학습 가능한지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
