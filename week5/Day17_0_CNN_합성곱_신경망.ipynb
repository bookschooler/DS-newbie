{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day17_0: CNN (합성곱 신경망)\n",
    "\n",
    "## 📚 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 이미지 데이터의 특성 이해하기\n",
    "2. 합성곱(Convolution) 연산 이해하기\n",
    "3. Pooling과 Padding 이해하기\n",
    "4. CNN 아키텍처 구조 파악하기\n",
    "5. torchvision으로 이미지 데이터 다루기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. Data Augmentation 적용하기\n",
    "2. 유명 CNN 아키텍처 (LeNet, VGG, ResNet) 소개\n",
    "3. Transfer Learning 활용하기\n",
    "4. CIFAR-10 이미지 분류 실습\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| CNN | 이미지 분류/탐지 | 제품 결함 검사, 의료 영상 분석 |\n",
    "| Conv Layer | 특징 추출 | 엣지, 질감, 패턴 인식 |\n",
    "| Pooling | 차원 축소 | 계산 효율화, 위치 불변성 |\n",
    "| Transfer Learning | 빠른 모델 개발 | 적은 데이터로 고성능 달성 |\n",
    "\n",
    "**분석가 관점**: CNN은 컴퓨터 비전의 기반 기술입니다. 이미지 분류, 객체 탐지, 세그멘테이션 등 거의 모든 시각 AI 과제에서 CNN 또는 CNN 기반 아키텍처가 사용됩니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 이미지 데이터의 특성\n",
    "\n",
    "### 이미지는 3D 텐서\n",
    "\n",
    "이미지 데이터는 (채널, 높이, 너비) 또는 (C, H, W) 형태의 3차원 텐서입니다.\n",
    "\n",
    "| 이미지 유형 | 채널 수 | 설명 |\n",
    "|------------|--------|------|\n",
    "| 흑백 (Grayscale) | 1 | 밝기 정보만 |\n",
    "| 컬러 (RGB) | 3 | Red, Green, Blue |\n",
    "| RGBA | 4 | RGB + 투명도 |\n",
    "\n",
    "```\n",
    "흑백 이미지: (1, H, W)\n",
    "컬러 이미지: (3, H, W)\n",
    "배치 데이터: (N, C, H, W)  # N = 배치 크기\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 이미지 텐서 예시\n",
    "# (배치, 채널, 높이, 너비)\n",
    "grayscale_image = torch.randn(1, 1, 28, 28)  # 흑백 28x28\n",
    "color_image = torch.randn(1, 3, 224, 224)    # RGB 224x224\n",
    "\n",
    "print(f\"흑백 이미지 shape: {grayscale_image.shape}\")\n",
    "print(f\"컬러 이미지 shape: {color_image.shape}\")\n",
    "print(f\"\\n흑백 이미지 총 픽셀 수: {grayscale_image.numel()}\")\n",
    "print(f\"컬러 이미지 총 픽셀 수: {color_image.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 DNN으로는 이미지 처리가 어려울까?\n",
    "\n",
    "1. **파라미터 폭발**: 224x224x3 이미지를 펼치면 150,528개 입력! 첫 은닉층 1000개만 해도 1.5억 개 파라미터\n",
    "2. **공간 정보 손실**: 이미지를 1D로 펼치면 인접 픽셀 관계(엣지, 질감) 정보가 사라짐\n",
    "3. **위치 의존성**: DNN은 고양이가 왼쪽에 있든 오른쪽에 있든 다른 패턴으로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN으로 이미지 처리 시 파라미터 수 비교\n",
    "image_size = 224 * 224 * 3  # 컬러 이미지를 1D로 펼친 크기\n",
    "hidden_neurons = 1000\n",
    "\n",
    "dnn_params = image_size * hidden_neurons  # 첫 번째 레이어만\n",
    "print(f\"DNN 첫 레이어 파라미터: {dnn_params:,} 개\")\n",
    "print(f\"약 {dnn_params / 1e6:.1f}M 파라미터\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 정규화\n",
    "\n",
    "신경망 학습을 위해 픽셀 값을 정규화합니다.\n",
    "\n",
    "| 방식 | 범위 | 수식 |\n",
    "|-----|------|------|\n",
    "| [0, 1] 정규화 | 0 ~ 1 | pixel / 255 |\n",
    "| [-1, 1] 정규화 | -1 ~ 1 | (pixel / 255 - 0.5) / 0.5 |\n",
    "| ImageNet 정규화 | 표준화 | (pixel - mean) / std |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 예시\n",
    "# 0-255 범위의 원본 이미지 (시뮬레이션)\n",
    "raw_image = torch.randint(0, 256, (3, 4, 4), dtype=torch.float32)\n",
    "print(f\"원본 이미지 (0-255):\\n{raw_image[0]}\")\n",
    "\n",
    "# [0, 1] 정규화\n",
    "normalized_01 = raw_image / 255.0\n",
    "print(f\"\\n[0, 1] 정규화:\\n{normalized_01[0]}\")\n",
    "\n",
    "# [-1, 1] 정규화\n",
    "normalized_11 = (raw_image / 255.0 - 0.5) / 0.5\n",
    "print(f\"\\n[-1, 1] 정규화:\\n{normalized_11[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 합성곱(Convolution) 연산\n",
    "\n",
    "### 합성곱이란?\n",
    "\n",
    "**합성곱**: 작은 필터(커널)를 이미지 위에서 슬라이딩하며 요소별 곱의 합을 계산\n",
    "\n",
    "```\n",
    "입력 이미지    필터(커널)       특징 맵\n",
    "[1 2 3 4]     [1 0]          [? ? ?]\n",
    "[5 6 7 8]  *  [0 1]    =     [? ? ?]\n",
    "[9 0 1 2]                    [? ? ?]\n",
    "[3 4 5 6]\n",
    "```\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "| 개념 | 설명 | 비유 |\n",
    "|-----|------|------|\n",
    "| 필터/커널 | 특정 패턴을 감지하는 가중치 행렬 | 돋보기 |\n",
    "| 특징 맵 | 필터 적용 결과, 특징의 위치와 강도 | 지도 |\n",
    "| 수용 영역 | 하나의 출력 뉴런이 보는 입력 영역 | 시야 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 합성곱 연산 직접 구현\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 5x5 입력 이미지 (1채널)\n",
    "image = torch.tensor([\n",
    "    [1, 2, 3, 0, 1],\n",
    "    [0, 1, 2, 3, 2],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [2, 1, 0, 1, 0],\n",
    "    [0, 1, 2, 1, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 3x3 필터 (수직 엣지 검출)\n",
    "vertical_edge_filter = torch.tensor([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"입력 이미지:\\n{image}\")\n",
    "print(f\"\\n수직 엣지 필터:\\n{vertical_edge_filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d로 합성곱 수행\n",
    "# (배치, 채널, 높이, 너비) 형태로 변환\n",
    "image_batch = image.unsqueeze(0).unsqueeze(0)  # (1, 1, 5, 5)\n",
    "filter_batch = vertical_edge_filter.unsqueeze(0).unsqueeze(0)  # (1, 1, 3, 3)\n",
    "\n",
    "# F.conv2d로 직접 합성곱\n",
    "output = F.conv2d(image_batch, filter_batch)\n",
    "print(f\"합성곱 결과 shape: {output.shape}\")\n",
    "print(f\"\\n특징 맵:\\n{output.squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d 사용하기\n",
    "\n",
    "```python\n",
    "nn.Conv2d(\n",
    "    in_channels,   # 입력 채널 수 (RGB면 3)\n",
    "    out_channels,  # 출력 채널 수 = 필터 개수\n",
    "    kernel_size,   # 필터 크기 (3이면 3x3)\n",
    "    stride=1,      # 필터 이동 간격\n",
    "    padding=0      # 입력 가장자리 패딩\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Conv2d 예시\n",
    "# 입력: 3채널(RGB), 출력: 16개 특징 맵, 3x3 커널\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "\n",
    "# 더미 입력: (배치=1, 채널=3, 높이=32, 너비=32)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 합성곱 수행\n",
    "output = conv(dummy_input)\n",
    "\n",
    "print(f\"입력 shape: {dummy_input.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "print(f\"\\n출력 크기가 30x30인 이유: 32 - 3 + 1 = 30 (패딩 없이 3x3 커널)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 실무 예시: 다양한 필터로 특징 추출\n",
    "\n",
    "CNN은 학습을 통해 최적의 필터를 자동으로 찾습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 종류의 엣지 검출 필터\n",
    "# 수직 엣지\n",
    "vertical = torch.tensor([[-1, 0, 1],\n",
    "                         [-1, 0, 1],\n",
    "                         [-1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "# 수평 엣지\n",
    "horizontal = torch.tensor([[-1, -1, -1],\n",
    "                           [ 0,  0,  0],\n",
    "                           [ 1,  1,  1]], dtype=torch.float32)\n",
    "\n",
    "# Sobel 필터 (엣지 강조)\n",
    "sobel_x = torch.tensor([[-1, 0, 1],\n",
    "                        [-2, 0, 2],\n",
    "                        [-1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "print(\"수직 엣지 필터:\")\n",
    "print(vertical)\n",
    "print(\"\\n수평 엣지 필터:\")\n",
    "print(horizontal)\n",
    "print(\"\\nSobel X 필터:\")\n",
    "print(sobel_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Padding과 Stride\n",
    "\n",
    "### Stride (보폭)\n",
    "\n",
    "필터가 이동하는 간격\n",
    "\n",
    "| Stride | 효과 |\n",
    "|--------|------|\n",
    "| 1 | 출력 크기 유지에 가까움 |\n",
    "| 2 | 출력 크기 약 절반 |\n",
    "| n | 출력 크기 약 1/n |\n",
    "\n",
    "### Padding (패딩)\n",
    "\n",
    "입력 이미지 가장자리에 값(보통 0)을 추가\n",
    "\n",
    "| Padding | 효과 |\n",
    "|---------|------|\n",
    "| 0 | 출력 크기 감소 |\n",
    "| 'same' | 출력 크기 = 입력 크기 |\n",
    "| (k-1)/2 | 3x3 커널이면 padding=1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stride와 Padding 효과 비교\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 기본 (stride=1, padding=0)\n",
    "conv_default = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=0)\n",
    "out_default = conv_default(dummy_input)\n",
    "print(f\"기본 (s=1, p=0): {dummy_input.shape} -> {out_default.shape}\")\n",
    "\n",
    "# padding=1 (출력 크기 유지)\n",
    "conv_padded = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "out_padded = conv_padded(dummy_input)\n",
    "print(f\"패딩 (s=1, p=1): {dummy_input.shape} -> {out_padded.shape}\")\n",
    "\n",
    "# stride=2 (다운샘플링)\n",
    "conv_stride = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "out_stride = conv_stride(dummy_input)\n",
    "print(f\"스트라이드 (s=2, p=1): {dummy_input.shape} -> {out_stride.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력 크기 계산 공식\n",
    "\n",
    "```\n",
    "출력 크기 = floor((입력 크기 + 2*padding - kernel_size) / stride) + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(input_size, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"합성곱 출력 크기 계산\"\"\"\n",
    "    return (input_size + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "# 예시 계산\n",
    "examples = [\n",
    "    (32, 3, 1, 0),   # 기본\n",
    "    (32, 3, 1, 1),   # 패딩으로 크기 유지\n",
    "    (32, 3, 2, 1),   # 다운샘플링\n",
    "    (224, 7, 2, 3),  # ResNet 첫 레이어\n",
    "]\n",
    "\n",
    "print(\"출력 크기 계산 예시:\")\n",
    "print(\"=\"*50)\n",
    "for inp, k, s, p in examples:\n",
    "    out = calc_output_size(inp, k, s, p)\n",
    "    print(f\"입력={inp}, 커널={k}x{k}, stride={s}, padding={p} -> 출력={out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Pooling (풀링)\n",
    "\n",
    "### Pooling이란?\n",
    "\n",
    "**풀링**: 특징 맵의 크기를 줄이는 다운샘플링 연산\n",
    "\n",
    "| 종류 | 연산 | 특징 |\n",
    "|-----|------|------|\n",
    "| Max Pooling | 영역 내 최댓값 | 가장 강한 활성화 보존 |\n",
    "| Average Pooling | 영역 내 평균 | 부드러운 다운샘플링 |\n",
    "| Global Average Pooling | 전체 평균 | FC 레이어 대체 가능 |\n",
    "\n",
    "### Pooling의 장점\n",
    "\n",
    "1. **계산량 감소**: 파라미터 없이 차원 축소\n",
    "2. **과적합 방지**: 불필요한 세부 정보 제거\n",
    "3. **위치 불변성**: 특징의 정확한 위치보다 존재 여부 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling vs Average Pooling\n",
    "feature_map = torch.tensor([\n",
    "    [[1, 2, 3, 4],\n",
    "     [5, 6, 7, 8],\n",
    "     [9, 0, 1, 2],\n",
    "     [3, 4, 5, 6]]\n",
    "], dtype=torch.float32).unsqueeze(0)  # (1, 1, 4, 4)\n",
    "\n",
    "print(f\"입력 특징 맵:\\n{feature_map.squeeze()}\")\n",
    "\n",
    "# 2x2 Max Pooling\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "max_pooled = max_pool(feature_map)\n",
    "print(f\"\\n2x2 Max Pooling:\\n{max_pooled.squeeze()}\")\n",
    "\n",
    "# 2x2 Average Pooling\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avg_pooled = avg_pool(feature_map)\n",
    "print(f\"\\n2x2 Average Pooling:\\n{avg_pooled.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling (GAP)\n",
    "# 각 채널의 전체 평균 -> (N, C, H, W) -> (N, C, 1, 1)\n",
    "feature_maps = torch.randn(1, 16, 7, 7)  # 16개 채널의 7x7 특징 맵\n",
    "\n",
    "gap = nn.AdaptiveAvgPool2d(1)  # 출력 크기를 1x1로\n",
    "gap_output = gap(feature_maps)\n",
    "\n",
    "print(f\"입력: {feature_maps.shape}\")\n",
    "print(f\"GAP 출력: {gap_output.shape}\")\n",
    "print(f\"\\n이렇게 하면 FC 레이어 없이 바로 분류 가능!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 CNN 아키텍처 구조\n",
    "\n",
    "### 전형적인 CNN 구조\n",
    "\n",
    "```\n",
    "입력 이미지\n",
    "    ↓\n",
    "[Conv -> ReLU -> Pool] x N  (특징 추출부)\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "[FC -> ReLU] x M  (분류부)\n",
    "    ↓\n",
    "출력 (클래스별 점수)\n",
    "```\n",
    "\n",
    "### CNN의 핵심 아이디어\n",
    "\n",
    "1. **지역 연결성**: 각 뉴런이 입력의 일부만 봄 (수용 영역)\n",
    "2. **파라미터 공유**: 같은 필터를 전체 이미지에 적용\n",
    "3. **계층적 학습**: 저수준(엣지) → 고수준(얼굴) 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 CNN 모델 정의\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 특징 추출부 (Convolutional Layers)\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1: (3, 32, 32) -> (32, 16, 16)\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 2: (32, 16, 16) -> (64, 8, 8)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 3: (64, 8, 8) -> (128, 4, 4)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # 분류부 (Fully Connected Layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # (128, 4, 4) -> 2048\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 모델 생성 및 테스트\n",
    "model = SimpleCNN(num_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파 테스트\n",
    "dummy_input = torch.randn(4, 3, 32, 32)  # 배치 4개, RGB, 32x32\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"입력 shape: {dummy_input.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "\n",
    "# 파라미터 수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n총 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 torchvision으로 이미지 다루기\n",
    "\n",
    "### torchvision 구성요소\n",
    "\n",
    "| 모듈 | 설명 |\n",
    "|-----|------|\n",
    "| `datasets` | 유명 데이터셋 (MNIST, CIFAR, ImageNet 등) |\n",
    "| `transforms` | 이미지 전처리/증강 |\n",
    "| `models` | 사전 학습 모델 (ResNet, VGG 등) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 기본 전처리 파이프라인\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # PIL Image -> Tensor (0-255 -> 0-1)\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1] 정규화\n",
    "])\n",
    "\n",
    "print(\"transforms.Compose 파이프라인:\")\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 데이터셋 로드\n",
    "# 다운로드 경로 설정\n",
    "data_path = './datasets'\n",
    "\n",
    "# 훈련 데이터\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 테스트 데이터\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"훈련 데이터: {len(train_dataset)} 샘플\")\n",
    "print(f\"테스트 데이터: {len(test_dataset)} 샘플\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 클래스\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 데이터 확인\n",
    "image, label = train_dataset[0]\n",
    "print(f\"이미지 shape: {image.shape}\")\n",
    "print(f\"레이블: {label} ({classes[label]})\")\n",
    "print(f\"픽셀 값 범위: [{image.min():.2f}, {image.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # 멀티프로세싱\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"훈련 배치 수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 시각화\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 첫 배치 가져오기\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# 이미지 역정규화 (시각화용)\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "# 처음 16개 이미지 표시\n",
    "fig = make_subplots(rows=2, cols=8, subplot_titles=[classes[labels[i]] for i in range(16)])\n",
    "\n",
    "for i in range(16):\n",
    "    img = denormalize(images[i]).permute(1, 2, 0).numpy()  # (C,H,W) -> (H,W,C)\n",
    "    row = i // 8 + 1\n",
    "    col = i % 8 + 1\n",
    "    fig.add_trace(go.Image(z=(img * 255).astype(np.uint8)), row=row, col=col)\n",
    "\n",
    "fig.update_layout(title='CIFAR-10 샘플 이미지', height=300, showlegend=False)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Data Augmentation (데이터 증강)\n",
    "\n",
    "### 왜 데이터 증강이 필요한가?\n",
    "\n",
    "1. **데이터 부족 해결**: 적은 데이터로 더 많은 학습\n",
    "2. **과적합 방지**: 다양한 변형에 강건한 모델\n",
    "3. **일반화 성능 향상**: 실제 환경의 다양성 반영\n",
    "\n",
    "### 주요 증강 기법\n",
    "\n",
    "| 기법 | 설명 | transforms 함수 |\n",
    "|-----|------|-----------------|\n",
    "| 수평 뒤집기 | 좌우 반전 | RandomHorizontalFlip |\n",
    "| 회전 | 각도 회전 | RandomRotation |\n",
    "| 크롭 | 무작위 자르기 | RandomCrop, RandomResizedCrop |\n",
    "| 색상 변환 | 밝기/대비/채도 | ColorJitter |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation 파이프라인\n",
    "train_transform = transforms.Compose([\n",
    "    # 무작위 자르기 후 리사이즈\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    # 50% 확률로 좌우 반전\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 색상 변환\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    # 텐서 변환\n",
    "    transforms.ToTensor(),\n",
    "    # 정규화\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 테스트용 (증강 없음)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"훈련 데이터 증강 파이프라인:\")\n",
    "print(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 효과 시각화\n",
    "from PIL import Image\n",
    "\n",
    "# 원본 이미지 가져오기 (증강 전)\n",
    "simple_transform = transforms.Compose([transforms.ToTensor()])\n",
    "simple_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path, train=True, download=False, transform=simple_transform\n",
    ")\n",
    "original_img, label = simple_dataset[0]\n",
    "\n",
    "# PIL Image로 변환 (증강 적용을 위해)\n",
    "pil_img = transforms.ToPILImage()(original_img)\n",
    "\n",
    "# 다양한 증강 적용\n",
    "augmentations = [\n",
    "    ('원본', transforms.ToTensor()),\n",
    "    ('수평 뒤집기', transforms.Compose([transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor()])),\n",
    "    ('회전', transforms.Compose([transforms.RandomRotation(30), transforms.ToTensor()])),\n",
    "    ('색상 변환', transforms.Compose([transforms.ColorJitter(brightness=0.5), transforms.ToTensor()])),\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=4, subplot_titles=[name for name, _ in augmentations])\n",
    "\n",
    "for i, (name, aug_transform) in enumerate(augmentations):\n",
    "    aug_img = aug_transform(pil_img).permute(1, 2, 0).numpy()\n",
    "    fig.add_trace(go.Image(z=(aug_img * 255).astype(np.uint8)), row=1, col=i+1)\n",
    "\n",
    "fig.update_layout(title=f'Data Augmentation 효과 - {classes[label]}', height=200)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 유명 CNN 아키텍처\n",
    "\n",
    "### CNN 발전 역사\n",
    "\n",
    "| 모델 | 년도 | 특징 | 파라미터 |\n",
    "|-----|------|------|----------|\n",
    "| LeNet-5 | 1998 | 최초의 성공적 CNN | 60K |\n",
    "| AlexNet | 2012 | 딥러닝 부흥 시작 | 60M |\n",
    "| VGG16 | 2014 | 깊고 단순한 구조 | 138M |\n",
    "| ResNet | 2015 | Skip Connection | 25M (ResNet-50) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 스타일 모델 (간단한 구조)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),    # (3, 32, 32) -> (6, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                # (6, 14, 14)\n",
    "            nn.Conv2d(6, 16, kernel_size=5),   # (16, 10, 10)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)                 # (16, 5, 5)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "lenet = LeNet()\n",
    "print(f\"LeNet 파라미터: {sum(p.numel() for p in lenet.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG 스타일 블록 (3x3 Conv 반복)\n",
    "def make_vgg_block(in_channels, out_channels, num_convs):\n",
    "    \"\"\"VGG 스타일 합성곱 블록\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(2, 2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# 간단한 VGG 스타일 모델\n",
    "class MiniVGG(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            make_vgg_block(3, 64, 2),      # (64, 16, 16)\n",
    "            make_vgg_block(64, 128, 2),    # (128, 8, 8)\n",
    "            make_vgg_block(128, 256, 2),   # (256, 4, 4)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "vgg = MiniVGG()\n",
    "print(f\"MiniVGG 파라미터: {sum(p.numel() for p in vgg.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet의 혁신: Skip Connection (잔차 연결)\n",
    "\n",
    "```\n",
    "기존:  x -> [Conv -> ReLU -> Conv] -> F(x)\n",
    "ResNet: x -> [Conv -> ReLU -> Conv] + x -> F(x) + x\n",
    "```\n",
    "\n",
    "Skip Connection의 장점:\n",
    "- 기울기 소실 문제 완화\n",
    "- 매우 깊은 네트워크 학습 가능 (100+ layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection (차원이 다를 경우 조정)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # Skip Connection!\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# 테스트\n",
    "block = ResidualBlock(64, 128, stride=2)\n",
    "x = torch.randn(1, 64, 16, 16)\n",
    "out = block(x)\n",
    "print(f\"입력: {x.shape} -> 출력: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 Transfer Learning (전이 학습)\n",
    "\n",
    "### 전이 학습이란?\n",
    "\n",
    "**전이 학습**: 대규모 데이터셋(ImageNet)으로 사전 학습된 모델을 가져와 새로운 과제에 적용\n",
    "\n",
    "### 두 가지 전략\n",
    "\n",
    "| 전략 | 설명 | 언제 사용? |\n",
    "|-----|------|----------|\n",
    "| Feature Extraction | 사전 학습 가중치 고정, 마지막 레이어만 학습 | 데이터가 매우 적을 때 |\n",
    "| Fine-tuning | 전체 또는 일부 레이어 재학습 | 데이터가 어느 정도 있을 때 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# 사전 학습된 ResNet18 불러오기\n",
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "print(\"ResNet18 마지막 레이어:\")\n",
    "print(resnet18.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction: 사전 학습 가중치 고정\n",
    "# 마지막 FC 레이어만 새로 정의\n",
    "\n",
    "# 1. 모든 파라미터 동결\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 마지막 FC 레이어 교체 (10개 클래스용)\n",
    "num_features = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "print(f\"새 FC 레이어 입력 크기: {num_features}\")\n",
    "print(f\"학습 가능한 파라미터: {sum(p.numel() for p in resnet18.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: 일부 레이어 동결 해제\n",
    "resnet18_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# 1. 모든 파라미터 동결\n",
    "for param in resnet18_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 마지막 레이어들 동결 해제 (layer4 + fc)\n",
    "for param in resnet18_ft.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. FC 레이어 교체\n",
    "resnet18_ft.fc = nn.Linear(resnet18_ft.fc.in_features, 10)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in resnet18_ft.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in resnet18_ft.parameters())\n",
    "\n",
    "print(f\"전체 파라미터: {total_params:,}\")\n",
    "print(f\"학습 가능 파라미터: {trainable_params:,}\")\n",
    "print(f\"학습 비율: {trainable_params/total_params:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 실무 팁: ImageNet 정규화\n",
    "\n",
    "사전 학습 모델 사용 시 반드시 동일한 정규화 적용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet 정규화 값\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 전이 학습용 전처리\n",
    "transfer_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # ResNet 입력 크기\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "print(\"ImageNet 정규화 값:\")\n",
    "print(f\"Mean: {imagenet_mean}\")\n",
    "print(f\"Std: {imagenet_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 CIFAR-10 분류 실습\n",
    "\n",
    "### 전체 학습 파이프라인\n",
    "\n",
    "1. 데이터 준비 (증강 포함)\n",
    "2. 모델 정의\n",
    "3. 학습 루프\n",
    "4. 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 준비\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # CIFAR-10 통계\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# 데이터셋\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path, train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"훈련 배치: {len(train_loader)}, 테스트 배치: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 정의 (SimpleCNN 사용)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 device: {device}\")\n",
    "\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 학습 함수\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 실행 (5 에포크만 - 시간 절약)\n",
    "epochs = 5\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print(\"학습 시작!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "          f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 학습 곡선 시각화\n",
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df['epoch'] = range(1, epochs + 1)\n",
    "\n",
    "# 손실 곡선\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Loss', 'Accuracy'])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=history_df['epoch'], y=history_df['train_loss'], name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=history_df['epoch'], y=history_df['test_loss'], name='Test Loss'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=history_df['epoch'], y=history_df['train_acc'], name='Train Acc'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=history_df['epoch'], y=history_df['test_acc'], name='Test Acc'), row=1, col=2)\n",
    "\n",
    "fig.update_layout(title='학습 곡선', height=400)\n",
    "fig.update_xaxes(title_text='Epoch')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 혼동 행렬 (Confusion Matrix)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 시각화\n",
    "fig = px.imshow(cm, \n",
    "                labels=dict(x=\"예측\", y=\"실제\", color=\"개수\"),\n",
    "                x=classes, y=classes,\n",
    "                color_continuous_scale='Blues')\n",
    "fig.update_layout(title='혼동 행렬 (Confusion Matrix)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 예측 샘플 시각화\n",
    "model.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 처음 8개 샘플\n",
    "fig = make_subplots(rows=2, cols=4)\n",
    "\n",
    "for i in range(8):\n",
    "    img = images[i].cpu()\n",
    "    # 역정규화\n",
    "    img = img * torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1) + \\\n",
    "          torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    row = i // 4 + 1\n",
    "    col = i % 4 + 1\n",
    "    fig.add_trace(go.Image(z=(img * 255).astype(np.uint8)), row=row, col=col)\n",
    "    \n",
    "    label = classes[labels[i]]\n",
    "    pred = classes[predicted[i]]\n",
    "    color = 'green' if label == pred else 'red'\n",
    "    fig.add_annotation(x=0.5, y=-0.1, text=f\"{pred}\", \n",
    "                       xref=f\"x{i+1} domain\", yref=f\"y{i+1} domain\",\n",
    "                       showarrow=False, font=dict(color=color))\n",
    "\n",
    "fig.update_layout(title='예측 결과 (초록=정답, 빨강=오답)', height=300)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 실습 퀴즈\n",
    "\n",
    "**난이도**: ⭐ (쉬움) ~ ⭐⭐⭐⭐⭐ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Conv2d 출력 크기 계산 ⭐\n",
    "\n",
    "**문제**: 64x64 크기의 RGB 이미지에 다음 Conv2d를 적용했을 때 출력 크기를 계산하세요.\n",
    "\n",
    "```python\n",
    "nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Max Pooling 효과 ⭐\n",
    "\n",
    "**문제**: 4x4 특징 맵에 2x2 Max Pooling (stride=2)을 적용한 결과를 출력하세요.\n",
    "\n",
    "```python\n",
    "feature_map = torch.tensor([[1, 3, 2, 4],\n",
    "                            [5, 6, 7, 8],\n",
    "                            [3, 2, 1, 0],\n",
    "                            [9, 8, 7, 6]], dtype=torch.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 이미지 정규화 ⭐⭐\n",
    "\n",
    "**문제**: 0-255 범위의 이미지를 [-1, 1] 범위로 정규화하는 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-255 범위 이미지 (시뮬레이션)\n",
    "raw_image = torch.randint(0, 256, (3, 32, 32), dtype=torch.float32)\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. CNN 아키텍처 분석 ⭐⭐\n",
    "\n",
    "**문제**: 다음 CNN 모델의 각 레이어 출력 shape를 계산하세요.\n",
    "\n",
    "입력: (1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),    # ?\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),                 # ?\n",
    "    nn.Conv2d(16, 32, 3, padding=1),   # ?\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),                 # ?\n",
    "    nn.Flatten(),                       # ?\n",
    ")\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Data Augmentation 적용 ⭐⭐\n",
    "\n",
    "**문제**: 다음 증강 기법을 포함하는 transform 파이프라인을 작성하세요.\n",
    "\n",
    "1. 무작위 크롭 (32x32, padding=4)\n",
    "2. 무작위 수평 뒤집기\n",
    "3. 무작위 회전 (최대 15도)\n",
    "4. 텐서 변환\n",
    "5. 정규화 (mean=0.5, std=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. torchvision 데이터셋 로드 ⭐⭐\n",
    "\n",
    "**문제**: MNIST 데이터셋을 로드하고, 첫 번째 이미지의 shape와 레이블을 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Transfer Learning 설정 ⭐⭐⭐\n",
    "\n",
    "**문제**: 사전 학습된 ResNet18을 불러와서 5개 클래스 분류 문제에 적용하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 모든 레이어 동결\n",
    "2. 마지막 FC 레이어만 5개 클래스용으로 교체\n",
    "3. 학습 가능한 파라미터 수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Fine-tuning 구현 ⭐⭐⭐\n",
    "\n",
    "**문제**: ResNet18에서 layer4와 fc 레이어만 학습 가능하도록 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. 커스텀 CNN 모델 ⭐⭐⭐⭐\n",
    "\n",
    "**문제**: 다음 조건을 만족하는 CNN 모델을 정의하세요.\n",
    "\n",
    "- 입력: 1채널 (흑백), 28x28\n",
    "- Conv 블록 3개 (채널: 32 -> 64 -> 128)\n",
    "- 각 블록: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "- Global Average Pooling\n",
    "- 출력: 10개 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 전체 학습 파이프라인 ⭐⭐⭐⭐⭐\n",
    "\n",
    "**문제**: CIFAR-10 데이터셋으로 CNN을 학습하는 전체 파이프라인을 구현하세요.\n",
    "\n",
    "요구사항:\n",
    "1. Data Augmentation 적용 (훈련 데이터)\n",
    "2. 커스텀 CNN 모델 사용\n",
    "3. 3 에포크 학습\n",
    "4. 학습/테스트 손실 및 정확도 기록\n",
    "5. Plotly로 학습 곡선 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 함수 | 실무 활용 |\n",
    "|-----|----------|----------|\n",
    "| 이미지 텐서 | (N, C, H, W) | 배치 처리 |\n",
    "| Conv2d | nn.Conv2d(in, out, kernel) | 특징 추출 |\n",
    "| Pooling | nn.MaxPool2d, nn.AvgPool2d | 다운샘플링 |\n",
    "| Padding/Stride | padding=1, stride=2 | 출력 크기 조절 |\n",
    "| torchvision | datasets, transforms | 데이터 전처리 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 메서드 | 언제 사용? |\n",
    "|-----|-----------|----------|\n",
    "| Data Augmentation | RandomCrop, RandomHorizontalFlip | 과적합 방지 |\n",
    "| LeNet/VGG/ResNet | torchvision.models | 검증된 아키텍처 |\n",
    "| Feature Extraction | requires_grad=False | 데이터 적을 때 |\n",
    "| Fine-tuning | 일부 레이어 해제 | 데이터 충분할 때 |\n",
    "\n",
    "### CNN 설계 핵심 패턴\n",
    "\n",
    "```python\n",
    "# 특징 추출부\n",
    "for i in range(num_blocks):\n",
    "    Conv2d -> BatchNorm -> ReLU -> MaxPool\n",
    "    (채널 수 증가, 공간 크기 감소)\n",
    "\n",
    "# 분류부\n",
    "Flatten / GlobalAvgPool\n",
    "-> Linear -> ReLU -> Dropout -> Linear\n",
    "```\n",
    "\n",
    "### 💡 실무 팁\n",
    "\n",
    "1. **데이터가 적으면**: Transfer Learning 필수\n",
    "2. **과적합 문제**: Data Augmentation + Dropout\n",
    "3. **학습이 불안정하면**: BatchNorm 추가\n",
    "4. **기울기 소실**: Skip Connection (ResNet 스타일)\n",
    "5. **ImageNet 모델 사용 시**: 반드시 동일 정규화 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
