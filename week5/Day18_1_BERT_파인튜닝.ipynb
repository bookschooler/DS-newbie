{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day18_1: BERT 파인튜닝\n",
        "\n",
        "## 학습 목표\n",
        "\n",
        "**Part 1: 기초**\n",
        "1. BERT 개념과 아키텍처 이해하기\n",
        "2. 사전학습 방식(MLM, NSP) 이해하기\n",
        "3. Hugging Face Transformers 라이브러리 사용하기\n",
        "4. 토크나이저(Tokenizer) 활용하기\n",
        "5. 사전학습 모델 로드하기\n",
        "\n",
        "**Part 2: 심화**\n",
        "1. 다운스트림 태스크 이해하기\n",
        "2. Fine-tuning 전략 이해하기\n",
        "3. 한국어 BERT (multilingual) 활용하기\n",
        "4. NSMC 한국어 감성 분석 실습하기\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 왜 이것을 배우나요?\n",
        "\n",
        "| 개념 | 실무 활용 | 예시 |\n",
        "|------|----------|------|\n",
        "| BERT | 범용 언어 이해 모델 | 문서 분류, QA, 개체명 인식 |\n",
        "| 사전학습 | 대규모 데이터로 언어 이해 | Wikipedia, 뉴스 코퍼스 |\n",
        "| Fine-tuning | 특정 태스크 적응 | 감성 분석, 스팸 탐지 |\n",
        "| Hugging Face | 쉬운 모델 활용 | 3줄 코드로 최신 모델 사용 |\n",
        "\n",
        "**분석가 관점**: BERT는 NLP의 ImageNet 모멘트를 가져온 혁명적인 모델입니다. Hugging Face 라이브러리를 사용하면 수천 개의 사전학습 모델을 단 몇 줄의 코드로 활용할 수 있어, 텍스트 분류, 감성 분석, 질의응답 등 다양한 NLP 태스크를 빠르게 해결할 수 있습니다!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: 기초\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 BERT란?\n",
        "\n",
        "### BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "2018년 구글이 발표한 **양방향 사전학습 언어 모델**입니다.\n",
        "\n",
        "```\n",
        "BERT의 핵심 특징:\n",
        "1. Bidirectional (양방향): 문맥의 좌우를 모두 고려\n",
        "2. Transformer Encoder: Self-Attention 기반 구조\n",
        "3. Pre-training + Fine-tuning: 사전학습 후 미세조정 패러다임\n",
        "```\n",
        "\n",
        "### 기존 모델과의 차이점\n",
        "\n",
        "| 모델 | 방향성 | 문맥 이해 |\n",
        "|------|--------|----------|\n",
        "| GPT | 단방향 (좌 -> 우) | 과거 문맥만 참조 |\n",
        "| ELMo | 양방향 (좌->우 + 우->좌) | 별도 학습 후 결합 |\n",
        "| **BERT** | **진정한 양방향** | **동시에 좌우 문맥 참조** |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 필수 라이브러리 설치 (Colab 또는 새 환경에서 실행)\n",
        "# !pip install transformers datasets evaluate accelerate torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 양방향 문맥의 중요성 예시\n",
        "example1 = \"나는 [MASK]에 가서 돈을 찾았다.\"\n",
        "example2 = \"물고기가 [MASK]에서 헤엄치고 있다.\"\n",
        "\n",
        "print(\"양방향 문맥의 중요성:\")\n",
        "print(f\"\\n예시 1: {example1}\")\n",
        "print(\"  -> '돈을 찾았다'를 보면 '은행'(금융)임을 알 수 있음\")\n",
        "print(f\"\\n예시 2: {example2}\")\n",
        "print(\"  -> '물고기', '헤엄치고'를 보면 '강/바다'임을 알 수 있음\")\n",
        "print(\"\\n단방향 모델은 [MASK] 이후의 정보를 사용할 수 없습니다!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT 아키텍처\n",
        "\n",
        "```\n",
        "BERT-base:  12 Layers, 768 Hidden, 12 Heads, 110M Parameters\n",
        "BERT-large: 24 Layers, 1024 Hidden, 16 Heads, 340M Parameters\n",
        "\n",
        "입력 구조:\n",
        "[CLS] 토큰1 토큰2 ... 토큰N [SEP] (문장1)\n",
        "[CLS] 토큰1 토큰2 [SEP] 토큰A 토큰B [SEP] (문장 쌍)\n",
        "\n",
        "특수 토큰:\n",
        "- [CLS]: Classification 토큰, 문장 전체 표현\n",
        "- [SEP]: Separator 토큰, 문장 구분\n",
        "- [PAD]: Padding 토큰, 길이 맞추기\n",
        "- [MASK]: Masking 토큰, MLM 학습용\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT 아키텍처 구조\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "    Input[\"Input Tokens<br/>[CLS] token1 token2 ... [SEP]\"] --> Embed[\"Token Embedding<br/>+ Position Embedding<br/>+ Segment Embedding\"]\n",
        "    Embed --> Encoder1[\"Encoder Layer 1<br/>Multi-Head Attention<br/>+ Feed Forward\"]\n",
        "    Encoder1 --> Encoder2[\"Encoder Layer 2\"]\n",
        "    Encoder2 --> EncoderN[\"Encoder Layer N<br/>(12 for base, 24 for large)\"]\n",
        "    EncoderN --> Outputs[\"Outputs<br/>(batch, seq_len, hidden_size)\"]\n",
        "    Outputs --> CLS[\"[CLS] Token Output<br/>(batch, hidden_size)\"]\n",
        "    Outputs --> TokenOutputs[\"Token Outputs<br/>(batch, seq_len, hidden_size)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style Embed fill:#ffffff,color:#000000\n",
        "    style Encoder1 fill:#ffffff,color:#000000\n",
        "    style Encoder2 fill:#ffffff,color:#000000\n",
        "    style EncoderN fill:#ffffff,color:#000000\n",
        "    style Outputs fill:#ffffff,color:#000000\n",
        "    style CLS fill:#ffffff,color:#000000\n",
        "    style TokenOutputs fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.2 사전학습 방식 (Pre-training)\n",
        "\n",
        "### BERT의 두 가지 사전학습 과제\n",
        "\n",
        "BERT는 레이블 없는 대규모 텍스트(Wikipedia + BookCorpus)로 자기지도 학습합니다.\n",
        "\n",
        "### 1) MLM (Masked Language Model)\n",
        "\n",
        "입력 토큰의 15%를 마스킹하고, 원래 토큰을 예측하는 과제\n",
        "\n",
        "```\n",
        "원본: 나는 오늘 학교에 갔다\n",
        "마스킹: 나는 오늘 [MASK]에 갔다\n",
        "예측: 학교\n",
        "\n",
        "마스킹 전략 (15% 토큰 중):\n",
        "- 80%: [MASK]로 변환\n",
        "- 10%: 랜덤 토큰으로 변환\n",
        "- 10%: 원래 토큰 유지\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MLM 예시 시뮬레이션\n",
        "def simulate_mlm(sentence, mask_prob=0.15):\n",
        "    \"\"\"MLM 마스킹 시뮬레이션\"\"\"\n",
        "    tokens = sentence.split()\n",
        "    masked_tokens = []\n",
        "    labels = []\n",
        "    \n",
        "    for token in tokens:\n",
        "        if np.random.random() < mask_prob:\n",
        "            labels.append(token)\n",
        "            r = np.random.random()\n",
        "            if r < 0.8:  # 80%: [MASK]\n",
        "                masked_tokens.append('[MASK]')\n",
        "            elif r < 0.9:  # 10%: 랜덤 토큰\n",
        "                masked_tokens.append('[RANDOM]')\n",
        "            else:  # 10%: 원래 토큰\n",
        "                masked_tokens.append(token)\n",
        "        else:\n",
        "            masked_tokens.append(token)\n",
        "            labels.append('-')\n",
        "    \n",
        "    return masked_tokens, labels\n",
        "\n",
        "# 시뮬레이션\n",
        "np.random.seed(42)\n",
        "sentence = \"BERT는 양방향 트랜스포머 인코더로 문맥을 이해합니다\"\n",
        "masked, labels = simulate_mlm(sentence, mask_prob=0.3)  # 시연용으로 30%\n",
        "\n",
        "print(\"MLM (Masked Language Model) 예시:\")\n",
        "print(f\"원본: {sentence}\")\n",
        "print(f\"마스킹: {' '.join(masked)}\")\n",
        "print(f\"레이블: {' '.join(labels)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) NSP (Next Sentence Prediction)\n",
        "\n",
        "두 문장이 연속된 문장인지 예측하는 이진 분류 과제\n",
        "\n",
        "```\n",
        "[IsNext 예시]\n",
        "문장 A: 오늘 날씨가 좋다.\n",
        "문장 B: 공원에 산책을 갔다.\n",
        "-> IsNext (연속된 문장)\n",
        "\n",
        "[NotNext 예시]\n",
        "문장 A: 오늘 날씨가 좋다.\n",
        "문장 B: 주식 시장이 폭락했다.\n",
        "-> NotNext (관련 없는 문장)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NSP 예시\n",
        "nsp_examples = [\n",
        "    {\n",
        "        \"A\": \"서울에서 부산까지 KTX로 2시간 30분 걸린다.\",\n",
        "        \"B\": \"고속철도 덕분에 당일치기 여행이 가능해졌다.\",\n",
        "        \"label\": \"IsNext\"\n",
        "    },\n",
        "    {\n",
        "        \"A\": \"서울에서 부산까지 KTX로 2시간 30분 걸린다.\",\n",
        "        \"B\": \"파이썬은 인터프리터 언어이다.\",\n",
        "        \"label\": \"NotNext\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"NSP (Next Sentence Prediction) 예시:\")\n",
        "print(\"=\"*60)\n",
        "for i, ex in enumerate(nsp_examples, 1):\n",
        "    print(f\"\\n예시 {i}:\")\n",
        "    print(f\"  문장 A: {ex['A']}\")\n",
        "    print(f\"  문장 B: {ex['B']}\")\n",
        "    print(f\"  레이블: {ex['label']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 사전학습의 의의\n",
        "\n",
        "```\n",
        "전통적 방식:        데이터 -> 모델 학습 -> 예측\n",
        "                   (태스크별로 처음부터)\n",
        "\n",
        "BERT 방식:         대규모 코퍼스 -> 사전학습 -> 파인튜닝 -> 예측\n",
        "                   (한 번 학습, 다양한 태스크에 적용)\n",
        "\n",
        "장점:\n",
        "1. 적은 데이터로도 높은 성능\n",
        "2. 다양한 태스크에 범용 적용\n",
        "3. 학습 시간 대폭 단축\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.3 Hugging Face Transformers 라이브러리\n",
        "\n",
        "### Hugging Face 소개\n",
        "\n",
        "**Hugging Face**는 NLP 모델의 민주화를 이끄는 회사로, transformers 라이브러리를 통해 최신 모델을 쉽게 사용할 수 있게 합니다.\n",
        "\n",
        "```\n",
        "주요 구성 요소:\n",
        "1. transformers: 모델 및 토크나이저\n",
        "2. datasets: 데이터셋 로드 및 처리\n",
        "3. evaluate: 평가 지표\n",
        "4. accelerate: 분산 학습 지원\n",
        "5. Hub: 모델 저장소 (300,000+ 모델)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT 기본 모델 구조\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len)<br/>token indices\"] --> Embedding[\"Embedding<br/>Token + Position + Segment\"]\n",
        "    Embedding --> Encoder[\"Transformer<br/>Encoder<br/>(N layers)\"]\n",
        "    Encoder --> LastHidden[\"Last Hidden State<br/>(batch, seq_len, hidden_size)\"]\n",
        "    Encoder --> Pooler[\"Pooler<br/>[CLS] token\"]\n",
        "    Pooler --> PoolerOut[\"Pooler Output<br/>(batch, hidden_size)\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style Embedding fill:#ffffff,color:#000000\n",
        "    style Encoder fill:#ffffff,color:#000000\n",
        "    style LastHidden fill:#ffffff,color:#000000\n",
        "    style Pooler fill:#ffffff,color:#000000\n",
        "    style PoolerOut fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hugging Face transformers 기본 사용법\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "\n",
        "# 모델 이름 (Hugging Face Hub에서 검색 가능)\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "# 설정 확인\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "print(f\"모델: {model_name}\")\n",
        "print(f\"\\n모델 설정:\")\n",
        "print(f\"  - Hidden Size: {config.hidden_size}\")\n",
        "print(f\"  - Num Layers: {config.num_hidden_layers}\")\n",
        "print(f\"  - Num Attention Heads: {config.num_attention_heads}\")\n",
        "print(f\"  - Vocab Size: {config.vocab_size}\")\n",
        "print(f\"  - Max Position Embeddings: {config.max_position_embeddings}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auto Classes\n",
        "\n",
        "Hugging Face의 `Auto` 클래스는 모델 이름만으로 적절한 클래스를 자동 선택합니다.\n",
        "\n",
        "| Auto Class | 용도 |\n",
        "|------------|------|\n",
        "| AutoModel | 기본 모델 |\n",
        "| AutoModelForSequenceClassification | 텍스트 분류 |\n",
        "| AutoModelForTokenClassification | 토큰 분류 (NER) |\n",
        "| AutoModelForQuestionAnswering | 질의응답 |\n",
        "| AutoTokenizer | 토크나이저 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.4 토크나이저 (Tokenizer)\n",
        "\n",
        "### 토크나이저란?\n",
        "\n",
        "텍스트를 모델이 처리할 수 있는 숫자(토큰 ID)로 변환하는 도구입니다.\n",
        "\n",
        "```\n",
        "토큰화 과정:\n",
        "텍스트 -> 토큰 분리 -> 토큰 ID 변환 -> 특수 토큰 추가\n",
        "\n",
        "\"안녕하세요\" -> [\"안녕\", \"##하세요\"] -> [23456, 78901] -> [101, 23456, 78901, 102]\n",
        "                                                        [CLS]            [SEP]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 토크나이저 로드 및 사용\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# 기본 토큰화\n",
        "text = \"BERT는 자연어 처리의 혁명입니다.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f\"원본 텍스트: {text}\")\n",
        "print(f\"\\n토큰: {tokens}\")\n",
        "print(f\"\\n토큰 ID: {token_ids}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# encode_plus: 실전에서 자주 사용하는 방법\n",
        "encoded = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,    # [CLS], [SEP] 추가\n",
        "    max_length=20,              # 최대 길이\n",
        "    padding='max_length',       # 패딩 방식\n",
        "    truncation=True,            # 잘림 여부\n",
        "    return_tensors='pt',        # PyTorch 텐서 반환\n",
        "    return_attention_mask=True  # attention mask 반환\n",
        ")\n",
        "\n",
        "print(\"encode_plus 결과:\")\n",
        "print(f\"  input_ids shape: {encoded['input_ids'].shape}\")\n",
        "print(f\"  input_ids: {encoded['input_ids'][0].tolist()}\")\n",
        "print(f\"  attention_mask: {encoded['attention_mask'][0].tolist()}\")\n",
        "print(f\"  token_type_ids: {encoded['token_type_ids'][0].tolist()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 특수 토큰 확인\n",
        "print(\"특수 토큰:\")\n",
        "print(f\"  [CLS] ID: {tokenizer.cls_token_id} -> {tokenizer.cls_token}\")\n",
        "print(f\"  [SEP] ID: {tokenizer.sep_token_id} -> {tokenizer.sep_token}\")\n",
        "print(f\"  [PAD] ID: {tokenizer.pad_token_id} -> {tokenizer.pad_token}\")\n",
        "print(f\"  [MASK] ID: {tokenizer.mask_token_id} -> {tokenizer.mask_token}\")\n",
        "print(f\"  [UNK] ID: {tokenizer.unk_token_id} -> {tokenizer.unk_token}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 디코딩: 토큰 ID -> 텍스트\n",
        "decoded = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=False)\n",
        "decoded_clean = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"디코딩 (특수 토큰 포함): {decoded}\")\n",
        "print(f\"디코딩 (특수 토큰 제외): {decoded_clean}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention Mask\n",
        "\n",
        "```\n",
        "Attention Mask:\n",
        "- 1: 실제 토큰 (모델이 주목)\n",
        "- 0: 패딩 토큰 (모델이 무시)\n",
        "\n",
        "예시:\n",
        "input_ids:      [101, 1234, 5678, 102, 0, 0, 0, 0]\n",
        "attention_mask: [1,   1,    1,    1,   0, 0, 0, 0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BERT for Sequence Classification 아키텍처\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Input[\"Input<br/>(batch, seq_len)<br/>token indices\"] --> BERT[\"BERT Base<br/>Transformer Encoder<br/>(12 layers)\"]\n",
        "    BERT --> LastHidden[\"Last Hidden State<br/>(batch, seq_len, 768)\"]\n",
        "    LastHidden --> CLS[\"[CLS] Token<br/>(batch, 768)\"]\n",
        "    CLS --> Dropout[\"Dropout\"]\n",
        "    Dropout --> Classifier[\"Classifier<br/>Linear(768 → num_labels)\"]\n",
        "    Classifier --> Output[\"Output<br/>(batch, num_labels)<br/>logits\"]\n",
        "    \n",
        "    style Input fill:#ffffff,color:#000000\n",
        "    style BERT fill:#ffffff,color:#000000\n",
        "    style LastHidden fill:#ffffff,color:#000000\n",
        "    style CLS fill:#ffffff,color:#000000\n",
        "    style Dropout fill:#ffffff,color:#000000\n",
        "    style Classifier fill:#ffffff,color:#000000\n",
        "    style Output fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.5 사전학습 모델 로드\n",
        "\n",
        "### 모델 로드 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 기본 BERT 모델 로드\n",
        "from transformers import AutoModel\n",
        "\n",
        "# 사전학습된 BERT 모델 로드\n",
        "model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "print(f\"모델 타입: {type(model).__name__}\")\n",
        "print(f\"\\n파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 모델 구조 확인\n",
        "print(\"BERT 모델 구조:\")\n",
        "print(\"=\"*60)\n",
        "for name, module in model.named_children():\n",
        "    print(f\"\\n{name}:\")\n",
        "    if hasattr(module, 'named_children'):\n",
        "        for sub_name, sub_module in list(module.named_children())[:3]:\n",
        "            print(f\"  - {sub_name}: {type(sub_module).__name__}\")\n",
        "        if len(list(module.named_children())) > 3:\n",
        "            print(f\"  ... ({len(list(module.named_children()))} 개 레이어)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 모델 순전파 테스트\n",
        "model.eval()\n",
        "\n",
        "# 샘플 입력\n",
        "sample_text = \"BERT 모델을 테스트합니다.\"\n",
        "inputs = tokenizer(sample_text, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(f\"입력 텍스트: {sample_text}\")\n",
        "print(f\"\\n출력 구조:\")\n",
        "print(f\"  - last_hidden_state: {outputs.last_hidden_state.shape}\")\n",
        "print(f\"    (batch_size, sequence_length, hidden_size)\")\n",
        "print(f\"  - pooler_output: {outputs.pooler_output.shape}\")\n",
        "print(f\"    ([CLS] 토큰의 출력)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 실무 예시: 문장 임베딩 추출\n",
        "\n",
        "[CLS] 토큰의 출력(pooler_output)은 문장 전체의 표현으로 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 문장 임베딩 추출 함수\n",
        "def get_sentence_embedding(text, model, tokenizer):\n",
        "    \"\"\"BERT를 사용한 문장 임베딩 추출\"\"\"\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # pooler_output: [CLS] 토큰의 hidden state\n",
        "    return outputs.pooler_output\n",
        "\n",
        "# 유사한 문장과 다른 문장 비교\n",
        "sentences = [\n",
        "    \"오늘 날씨가 좋다.\",\n",
        "    \"날씨가 맑고 화창하다.\",\n",
        "    \"파이썬 프로그래밍을 배운다.\"\n",
        "]\n",
        "\n",
        "embeddings = [get_sentence_embedding(s, model, tokenizer) for s in sentences]\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "print(\"문장 간 코사인 유사도:\")\n",
        "print(f\"  '{sentences[0]}' vs '{sentences[1]}': {cosine_similarity(embeddings[0], embeddings[1]).item():.4f}\")\n",
        "print(f\"  '{sentences[0]}' vs '{sentences[2]}': {cosine_similarity(embeddings[0], embeddings[2]).item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: 심화\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 다운스트림 태스크 (Downstream Tasks)\n",
        "\n",
        "### 다운스트림 태스크란?\n",
        "\n",
        "사전학습된 BERT를 특정 목적에 맞게 활용하는 태스크들입니다.\n",
        "\n",
        "| 태스크 | 설명 | 출력 형태 |\n",
        "|--------|------|----------|\n",
        "| 텍스트 분류 | 문장 -> 카테고리 | [CLS] -> softmax |\n",
        "| 토큰 분류 (NER) | 각 토큰 -> 라벨 | 모든 토큰 -> softmax |\n",
        "| 질의응답 (QA) | 질문+문서 -> 답변 위치 | start, end 인덱스 |\n",
        "| 문장 유사도 | 문장 쌍 -> 유사도 | [CLS] -> sigmoid |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 태스크별 모델 헤드 구조 시각화\n",
        "tasks_info = {\n",
        "    '텍스트 분류': 'BERT -> [CLS] -> Linear -> Softmax -> 클래스',\n",
        "    '토큰 분류 (NER)': 'BERT -> 모든 토큰 -> Linear -> Softmax -> 라벨',\n",
        "    '질의응답 (QA)': 'BERT -> 모든 토큰 -> Linear(2) -> Start/End 위치',\n",
        "    '문장 유사도': 'BERT -> [CLS] -> Linear -> Sigmoid -> 유사도 점수'\n",
        "}\n",
        "\n",
        "print(\"다운스트림 태스크별 모델 구조:\")\n",
        "print(\"=\"*70)\n",
        "for task, structure in tasks_info.items():\n",
        "    print(f\"\\n{task}:\")\n",
        "    print(f\"  {structure}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 텍스트 분류용 모델 로드\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# 이진 분류 모델 (num_labels=2)\n",
        "classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "print(\"분류 모델 구조:\")\n",
        "print(f\"  - BERT: {type(classification_model.bert).__name__}\")\n",
        "print(f\"  - Classifier: {classification_model.classifier}\")\n",
        "print(f\"\\n분류 헤드: Linear(768 -> 2)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.2 Fine-tuning 전략\n",
        "\n",
        "### Fine-tuning이란?\n",
        "\n",
        "사전학습된 모델의 가중치를 특정 태스크의 데이터로 미세 조정하는 과정입니다.\n",
        "\n",
        "```\n",
        "Fine-tuning 과정:\n",
        "1. 사전학습 모델 로드 (pre-trained weights)\n",
        "2. 태스크별 헤드 추가 (classification head)\n",
        "3. 태스크 데이터로 전체 모델 학습\n",
        "4. 학습률을 낮게 설정 (보통 2e-5 ~ 5e-5)\n",
        "5. 적은 에포크 (2~4 epochs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tuning 전략 비교\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "    subgraph Full[\"전체 Fine-tuning\"]\n",
        "        FullBERT[\"BERT<br/>(학습 가능)\"] --> FullHead[\"Classification Head<br/>(학습 가능)\"]\n",
        "        FullHead --> FullOut[\"Output\"]\n",
        "    end\n",
        "    \n",
        "    subgraph Feature[\"Feature Extraction\"]\n",
        "        FeatureBERT[\"BERT<br/>(고정, 학습 안함)\"] --> FeatureHead[\"Classification Head<br/>(학습 가능)\"]\n",
        "        FeatureHead --> FeatureOut[\"Output\"]\n",
        "    end\n",
        "    \n",
        "    subgraph Gradual[\"Gradual Unfreezing\"]\n",
        "        GradualBERT1[\"BERT 상위 레이어<br/>(학습 가능)\"] --> GradualBERT2[\"BERT 하위 레이어<br/>(고정)\"]\n",
        "        GradualBERT2 --> GradualHead[\"Classification Head<br/>(학습 가능)\"]\n",
        "        GradualHead --> GradualOut[\"Output\"]\n",
        "    end\n",
        "    \n",
        "    style FullBERT fill:#ffffff,color:#000000\n",
        "    style FullHead fill:#ffffff,color:#000000\n",
        "    style FullOut fill:#ffffff,color:#000000\n",
        "    style FeatureBERT fill:#ffffff,color:#000000\n",
        "    style FeatureHead fill:#ffffff,color:#000000\n",
        "    style FeatureOut fill:#ffffff,color:#000000\n",
        "    style GradualBERT1 fill:#ffffff,color:#000000\n",
        "    style GradualBERT2 fill:#ffffff,color:#000000\n",
        "    style GradualHead fill:#ffffff,color:#000000\n",
        "    style GradualOut fill:#ffffff,color:#000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fine-tuning 전략 비교\n",
        "strategies = {\n",
        "    \"전체 Fine-tuning\": {\n",
        "        \"설명\": \"BERT + 분류 헤드 전체 학습\",\n",
        "        \"장점\": \"최고 성능\",\n",
        "        \"단점\": \"많은 연산량, 과적합 위험\",\n",
        "        \"권장 상황\": \"데이터가 충분할 때\"\n",
        "    },\n",
        "    \"Feature Extraction\": {\n",
        "        \"설명\": \"BERT 고정, 분류 헤드만 학습\",\n",
        "        \"장점\": \"빠른 학습, 과적합 방지\",\n",
        "        \"단점\": \"성능 제한\",\n",
        "        \"권장 상황\": \"데이터가 적을 때\"\n",
        "    },\n",
        "    \"Gradual Unfreezing\": {\n",
        "        \"설명\": \"상위 레이어부터 점진적으로 학습\",\n",
        "        \"장점\": \"안정적인 학습\",\n",
        "        \"단점\": \"복잡한 구현\",\n",
        "        \"권장 상황\": \"도메인이 다를 때\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Fine-tuning 전략 비교:\")\n",
        "print(\"=\"*70)\n",
        "for name, info in strategies.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    for key, value in info.items():\n",
        "        print(f\"  - {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature Extraction: BERT 가중치 고정\n",
        "def freeze_bert(model):\n",
        "    \"\"\"BERT 레이어 가중치 고정\"\"\"\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"BERT 가중치가 고정되었습니다.\")\n",
        "\n",
        "def unfreeze_bert(model):\n",
        "    \"\"\"BERT 레이어 가중치 해제\"\"\"\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(\"BERT 가중치 고정이 해제되었습니다.\")\n",
        "\n",
        "# 학습 가능한 파라미터 수 확인\n",
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"전체 파라미터: {sum(p.numel() for p in classification_model.parameters()):,}\")\n",
        "print(f\"학습 가능 파라미터 (전체): {count_trainable_params(classification_model):,}\")\n",
        "\n",
        "freeze_bert(classification_model)\n",
        "print(f\"학습 가능 파라미터 (BERT 고정): {count_trainable_params(classification_model):,}\")\n",
        "\n",
        "unfreeze_bert(classification_model)\n",
        "print(f\"학습 가능 파라미터 (고정 해제): {count_trainable_params(classification_model):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tuning 하이퍼파라미터 권장 값\n",
        "\n",
        "| 파라미터 | 권장 값 | 설명 |\n",
        "|---------|---------|------|\n",
        "| Learning Rate | 2e-5 ~ 5e-5 | 사전학습 가중치를 크게 변경하지 않도록 |\n",
        "| Epochs | 2 ~ 4 | 적은 에포크로 충분 |\n",
        "| Batch Size | 16 ~ 32 | GPU 메모리에 따라 조정 |\n",
        "| Warmup | 전체의 6~10% | 학습 초기 안정화 |\n",
        "| Weight Decay | 0.01 | 과적합 방지 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.3 한국어 BERT (Multilingual BERT)\n",
        "\n",
        "### 한국어 지원 모델\n",
        "\n",
        "| 모델 | 설명 | 특징 |\n",
        "|------|------|------|\n",
        "| bert-base-multilingual-cased | 104개 언어 지원 | 범용성 높음 |\n",
        "| klue/bert-base | 한국어 특화 | KLUE 벤치마크 최적화 |\n",
        "| monologg/kobert | SKT KoBERT 기반 | 한국어 성능 우수 |\n",
        "| beomi/kcbert-base | 한국어 댓글 학습 | 비격식체 처리 |\n",
        "\n",
        "이번 실습에서는 범용적인 `bert-base-multilingual-cased`를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 한국어 토큰화 테스트\n",
        "korean_texts = [\n",
        "    \"오늘 영화를 봤는데 정말 재미있었어요!\",\n",
        "    \"이 영화는 시간 낭비였습니다. 최악이에요.\",\n",
        "    \"배우들의 연기가 인상적이었습니다.\"\n",
        "]\n",
        "\n",
        "print(\"한국어 토큰화 결과:\")\n",
        "print(\"=\"*60)\n",
        "for text in korean_texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\n원문: {text}\")\n",
        "    print(f\"토큰: {tokens}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WordPiece 토큰화\n",
        "\n",
        "BERT는 **WordPiece** 토큰화를 사용합니다.\n",
        "\n",
        "```\n",
        "WordPiece:\n",
        "- 자주 등장하는 서브워드 단위로 분할\n",
        "- ##: 단어 중간/끝 부분임을 표시\n",
        "- OOV(Out-of-Vocabulary) 문제 해결\n",
        "\n",
        "예시:\n",
        "\"재미있었어요\" -> [\"재미\", \"##있\", \"##었\", \"##어요\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.4 NSMC 한국어 감성 분석 실습\n",
        "\n",
        "### NSMC (Naver Sentiment Movie Corpus)\n",
        "\n",
        "네이버 영화 리뷰 데이터셋으로, 한국어 감성 분석의 대표적인 벤치마크입니다.\n",
        "\n",
        "```\n",
        "데이터셋 구성:\n",
        "- 훈련 데이터: 150,000개\n",
        "- 테스트 데이터: 50,000개\n",
        "- 라벨: 0 (부정), 1 (긍정)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NSMC 데이터 로드\n",
        "# 데이터셋 경로 설정 (로컬 파일 또는 URL)\n",
        "try:\n",
        "    # 로컬 파일 시도\n",
        "    train_df = pd.read_csv(\"../datasets/text/nsmc/ratings_train.txt\", sep=\"\\t\")\n",
        "    test_df = pd.read_csv(\"../datasets/text/nsmc/ratings_test.txt\", sep=\"\\t\")\n",
        "except:\n",
        "    # URL에서 다운로드\n",
        "    train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
        "    test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
        "    train_df = pd.read_csv(train_url, sep=\"\\t\")\n",
        "    test_df = pd.read_csv(test_url, sep=\"\\t\")\n",
        "\n",
        "print(f\"훈련 데이터: {len(train_df):,}개\")\n",
        "print(f\"테스트 데이터: {len(test_df):,}개\")\n",
        "print(f\"\\n컬럼: {train_df.columns.tolist()}\")\n",
        "print(f\"\\n샘플 데이터:\")\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 라벨 분포 확인\n",
        "label_counts = train_df['label'].value_counts().reset_index()\n",
        "label_counts.columns = ['label', 'count']\n",
        "label_counts['label_name'] = label_counts['label'].map({0: '부정 (0)', 1: '긍정 (1)'})\n",
        "\n",
        "fig = px.bar(\n",
        "    label_counts, x='label_name', y='count', \n",
        "    color='label_name',\n",
        "    title='NSMC 라벨 분포',\n",
        "    labels={'label_name': '라벨', 'count': '개수'}\n",
        ")\n",
        "fig.update_layout(template='plotly_white', showlegend=False)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 결측치 처리\n",
        "print(f\"결측치 확인:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# 결측치 제거\n",
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "print(f\"\\n결측치 제거 후: 훈련 {len(train_df):,}개, 테스트 {len(test_df):,}개\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 샘플링 (빠른 실습을 위해 데이터 일부만 사용)\n",
        "# 전체 데이터로 학습하려면 이 셀을 건너뛰세요\n",
        "SAMPLE_SIZE = 5000  # 훈련용\n",
        "TEST_SAMPLE_SIZE = 1000  # 테스트용\n",
        "\n",
        "train_sample = train_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "test_sample = test_df.sample(n=TEST_SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"샘플링된 데이터:\")\n",
        "print(f\"  훈련: {len(train_sample):,}개\")\n",
        "print(f\"  테스트: {len(test_sample):,}개\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 토크나이저 및 모델 준비\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"모델: {MODEL_NAME}\")\n",
        "print(f\"디바이스: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 데이터셋 클래스 정의\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NSMCDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.df.iloc[idx]['document'])\n",
        "        label = self.df.iloc[idx]['label']\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = NSMCDataset(train_sample, tokenizer)\n",
        "test_dataset = NSMCDataset(test_sample, tokenizer)\n",
        "\n",
        "# 데이터로더 생성\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"데이터셋 준비 완료!\")\n",
        "print(f\"  훈련 배치 수: {len(train_loader)}\")\n",
        "print(f\"  테스트 배치 수: {len(test_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 학습 설정\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# 하이퍼파라미터\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "# 스케줄러 (warmup + linear decay)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"학습 설정:\")\n",
        "print(f\"  에포크: {EPOCHS}\")\n",
        "print(f\"  학습률: {LEARNING_RATE}\")\n",
        "print(f\"  총 스텝: {total_steps}\")\n",
        "print(f\"  Warmup 스텝: {warmup_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 학습 함수\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        # 정확도 계산\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            \n",
        "            total_loss += outputs.loss.item()\n",
        "            \n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 학습 실행\n",
        "print(\"BERT 파인튜닝 시작!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "eval_losses = []\n",
        "eval_accs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    # 훈련\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    \n",
        "    # 평가\n",
        "    eval_loss, eval_acc = evaluate(model, test_loader, device)\n",
        "    eval_losses.append(eval_loss)\n",
        "    eval_accs.append(eval_acc)\n",
        "    \n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Eval Loss: {eval_loss:.4f}, Eval Acc: {eval_acc:.4f}\")\n",
        "\n",
        "print(\"\\n학습 완료!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 학습 곡선 시각화\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=['Loss', 'Accuracy'])\n",
        "\n",
        "# Loss\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=train_losses, name='Train Loss', mode='lines+markers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=eval_losses, name='Eval Loss', mode='lines+markers'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Accuracy\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=train_accs, name='Train Acc', mode='lines+markers'),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=eval_accs, name='Eval Acc', mode='lines+markers'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text='Epoch')\n",
        "fig.update_layout(\n",
        "    title='BERT Fine-tuning 학습 곡선',\n",
        "    height=400,\n",
        "    template='plotly_white'\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 새로운 문장 예측\n",
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    \"\"\"감성 예측 함수\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0][pred].item()\n",
        "    \n",
        "    sentiment = \"긍정\" if pred == 1 else \"부정\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# 테스트\n",
        "test_reviews = [\n",
        "    \"정말 재미있는 영화였어요! 다시 보고 싶네요.\",\n",
        "    \"시간 낭비했습니다. 최악의 영화예요.\",\n",
        "    \"배우들 연기는 좋았지만 스토리가 아쉬웠어요.\",\n",
        "    \"완전 감동받았어요. 눈물이 멈추지 않았습니다.\",\n",
        "    \"지루해서 중간에 나왔어요.\"\n",
        "]\n",
        "\n",
        "print(\"감성 분석 결과:\")\n",
        "print(\"=\"*60)\n",
        "for review in test_reviews:\n",
        "    sentiment, confidence = predict_sentiment(review, model, tokenizer, device)\n",
        "    print(f\"\\n리뷰: {review}\")\n",
        "    print(f\"예측: {sentiment} (신뢰도: {confidence:.2%})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 혼동 행렬 생성\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 전체 테스트 데이터 예측\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels']\n",
        "        \n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1).cpu()\n",
        "        \n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "# 혼동 행렬\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# 시각화\n",
        "fig = px.imshow(\n",
        "    cm,\n",
        "    labels=dict(x=\"예측\", y=\"실제\", color=\"개수\"),\n",
        "    x=['부정', '긍정'],\n",
        "    y=['부정', '긍정'],\n",
        "    text_auto=True,\n",
        "    title='BERT 감성 분류 혼동 행렬'\n",
        ")\n",
        "fig.update_layout(template='plotly_white')\n",
        "fig.show()\n",
        "\n",
        "# 분류 리포트\n",
        "print(\"\\n분류 리포트:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['부정', '긍정']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 실습 퀴즈\n",
        "\n",
        "**난이도**: (쉬움) ~ (어려움)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. BERT 개념 이해하기\n",
        "\n",
        "**문제**: BERT가 \"Bidirectional\"이라고 불리는 이유를 설명하고, 기존 단방향 모델(GPT 등)과의 차이점을 서술하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 답을 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. MLM과 NSP 이해하기\n",
        "\n",
        "**문제**: BERT의 두 가지 사전학습 과제인 MLM과 NSP의 역할을 각각 한 문장으로 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 답을 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. 토크나이저 사용하기\n",
        "\n",
        "**문제**: 아래 텍스트를 토크나이저로 변환하고, 결과를 출력하세요.\n",
        "\n",
        "```python\n",
        "text = \"Hugging Face 라이브러리는 정말 편리합니다!\"\n",
        "```\n",
        "\n",
        "출력해야 할 것:\n",
        "1. 토큰 리스트\n",
        "2. 토큰 ID 리스트\n",
        "3. attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "text = \"Hugging Face 라이브러리는 정말 편리합니다!\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. 모델 로드 및 출력 shape\n",
        "\n",
        "**문제**: `bert-base-multilingual-cased` 모델을 로드하고, 아래 텍스트의 출력 shape을 확인하세요.\n",
        "\n",
        "```python\n",
        "text = \"딥러닝 모델을 학습합니다.\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "text = \"딥러닝 모델을 학습합니다.\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. 특수 토큰 이해\n",
        "\n",
        "**문제**: BERT의 특수 토큰 [CLS], [SEP], [PAD], [MASK]의 역할을 각각 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 답을 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. 다운스트림 태스크 이해\n",
        "\n",
        "**문제**: BERT를 사용한 텍스트 분류에서 왜 [CLS] 토큰의 출력을 사용하는지 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 답을 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. Fine-tuning 전략 비교\n",
        "\n",
        "**문제**: \"전체 Fine-tuning\"과 \"Feature Extraction\" 전략의 차이점을 설명하고, 각각 언제 사용하면 좋은지 서술하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 답을 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. 한국어 토큰화\n",
        "\n",
        "**문제**: 아래 두 문장을 토크나이저로 변환하고, 토큰화 결과를 비교 분석하세요.\n",
        "\n",
        "```python\n",
        "text1 = \"자연어 처리\"\n",
        "text2 = \"Natural Language Processing\"\n",
        "```\n",
        "\n",
        "한국어와 영어의 토큰화 차이점을 설명하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text1 = \"자연어 처리\"\n",
        "text2 = \"Natural Language Processing\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. 감성 분류 모델 개선\n",
        "\n",
        "**문제**: 본문의 NSMC 감성 분류 코드를 수정하여 다음을 구현하세요.\n",
        "\n",
        "1. BERT 레이어를 고정(freeze)한 상태로 학습\n",
        "2. 고정된 상태와 고정하지 않은 상태의 성능 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 여기에 코드를 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. 전체 파이프라인 구현\n",
        "\n",
        "**문제**: Hugging Face의 `pipeline` API를 사용하여 감성 분석을 수행하는 코드를 작성하세요.\n",
        "\n",
        "테스트 리뷰:\n",
        "```python\n",
        "reviews = [\n",
        "    \"이 제품 정말 좋아요! 강추합니다.\",\n",
        "    \"배송이 너무 늦고 포장이 엉망이었어요.\",\n",
        "    \"가격 대비 괜찮은 것 같아요.\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "reviews = [\n",
        "    \"이 제품 정말 좋아요! 강추합니다.\",\n",
        "    \"배송이 너무 늦고 포장이 엉망이었어요.\",\n",
        "    \"가격 대비 괜찮은 것 같아요.\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 학습 정리\n",
        "\n",
        "### Part 1: 기초 핵심 요약\n",
        "\n",
        "| 개념 | 핵심 내용 | 실무 활용 |\n",
        "|-----|----------|----------|\n",
        "| BERT | 양방향 Transformer Encoder | 범용 언어 이해 모델 |\n",
        "| MLM | 마스킹된 토큰 예측 | 문맥적 단어 이해 |\n",
        "| NSP | 다음 문장 예측 | 문장 관계 이해 |\n",
        "| Hugging Face | transformers 라이브러리 | 3줄로 최신 모델 사용 |\n",
        "| Tokenizer | 텍스트 -> 토큰 ID | 모델 입력 준비 |\n",
        "\n",
        "### Part 2: 심화 핵심 요약\n",
        "\n",
        "| 개념 | 핵심 내용 | 언제 사용? |\n",
        "|-----|----------|----------|\n",
        "| 다운스트림 태스크 | 분류, NER, QA 등 | 특정 목적의 NLP |\n",
        "| Fine-tuning | 사전학습 모델 미세조정 | 적은 데이터로 고성능 |\n",
        "| Feature Extraction | BERT 고정, 헤드만 학습 | 데이터 부족 시 |\n",
        "| 한국어 BERT | mBERT, KoBERT 등 | 한국어 NLP 태스크 |\n",
        "\n",
        "### BERT 활용 가이드\n",
        "\n",
        "```\n",
        "1. 모델 선택:\n",
        "   - 범용: bert-base-multilingual-cased\n",
        "   - 한국어 특화: klue/bert-base, monologg/kobert\n",
        "\n",
        "2. Fine-tuning 전략:\n",
        "   - 데이터 충분: 전체 Fine-tuning\n",
        "   - 데이터 부족: Feature Extraction\n",
        "   - 도메인 차이: Gradual Unfreezing\n",
        "\n",
        "3. 하이퍼파라미터:\n",
        "   - Learning Rate: 2e-5 ~ 5e-5\n",
        "   - Epochs: 2 ~ 4\n",
        "   - Batch Size: 16 ~ 32\n",
        "```\n",
        "\n",
        "### 실무 팁\n",
        "\n",
        "1. **토크나이저와 모델은 같은 것 사용**: 반드시 동일한 pretrained 모델명 사용\n",
        "2. **max_length 설정**: 데이터에 맞게 조정 (메모리 vs 정보량)\n",
        "3. **Warmup 사용**: 학습 초기 불안정 방지\n",
        "4. **Gradient Clipping**: 기울기 폭발 방지 (max_norm=1.0)\n",
        "5. **조기 종료**: 검증 성능 기반 Early Stopping 권장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}