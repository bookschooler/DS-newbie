{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day19_3: OpenAI 파인튜닝 - LLM을 내 목적에 맞게 맞춤 제작하기\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 파인튜닝(Fine-Tuning)의 개념을 이해하고, 프롬프트 엔지니어링, RAG와의 차이점을 설명할 수 있습니다.\n",
    "2. 파인튜닝이 적합한 시나리오와 부적합한 시나리오를 구별할 수 있습니다.\n",
    "3. OpenAI 파인튜닝을 위한 JSONL 데이터 형식을 이해하고 올바르게 작성할 수 있습니다.\n",
    "4. 고품질 학습 데이터의 특성을 이해하고, 데이터 준비 과정을 수행할 수 있습니다.\n",
    "5. OpenAI Files API를 사용하여 학습 데이터를 업로드할 수 있습니다.\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. OpenAI Fine-tuning API를 사용하여 파인튜닝 작업을 생성하고 모니터링할 수 있습니다.\n",
    "2. 파인튜닝된 모델을 호출하고, 기본 모델과의 성능 차이를 비교 분석할 수 있습니다.\n",
    "3. 하이퍼파라미터(epochs, learning_rate_multiplier)를 조정하여 학습을 최적화할 수 있습니다.\n",
    "4. 실무에서 파인튜닝을 활용하는 전략과 비용 최적화 방법을 이해합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| 파인튜닝 개념 | 도메인 특화 AI 구축 | 법률 문서 초안 작성, 브랜드 톤앤매너 유지 |\n",
    "| Prompting vs RAG vs Fine-tuning | 최적 기술 선택 | 비용-성능 트레이드오프 분석 |\n",
    "| JSONL 데이터 형식 | 학습 데이터 준비 | 대화 데이터셋 구축 |\n",
    "| 파인튜닝 API | 맞춤형 모델 제작 | 고객 응대 챗봇, 코드 리뷰 봇 |\n",
    "\n",
    "**분석가 관점**: 프롬프트 엔지니어링이나 RAG로 해결하기 어려운 일관된 스타일, 특정 출력 형식, 도메인 언어 습득이 필요한 경우 파인튜닝이 강력한 솔루션입니다. 한 번 학습시키면 짧은 프롬프트로도 높은 품질의 결과를 얻을 수 있어 API 비용을 절감할 수 있습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 파인튜닝(Fine-Tuning)이란?\n",
    "\n",
    "### LLM의 행동 자체를 변화시키는 기술\n",
    "\n",
    "**파인튜닝(Fine-Tuning)** 또는 **미세조정**은 기존의 사전학습된 LLM을 우리만의 데이터로 추가 학습시켜, 모델의 **내부 가중치(weights)**를 업데이트하는 과정입니다.\n",
    "\n",
    "| 특성 | 설명 |\n",
    "|------|------|\n",
    "| **핵심 아이디어** | 모델의 '뇌'를 직접 재훈련 |\n",
    "| **변화 대상** | 모델 자체의 행동 방식, 스타일, 출력 패턴 |\n",
    "| **필요 데이터** | 고품질 예시 데이터 (입력-출력 쌍) |\n",
    "| **비유** | 천재 배우에게 특정 캐릭터 연기 훈련시키기 |\n",
    "\n",
    "### 파인튜닝의 효과\n",
    "\n",
    "```\n",
    "기본 모델 (GPT-4o):\n",
    "  - 범용적, 다재다능\n",
    "  - 어떤 질문에도 답변 가능\n",
    "  - 하지만 '우리 회사 스타일'로 답하지는 않음\n",
    "\n",
    "파인튜닝된 모델:\n",
    "  - 특정 도메인/스타일에 최적화\n",
    "  - 짧은 프롬프트로도 원하는 출력\n",
    "  - 일관된 품질 유지\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파인튜닝이 적합한 상황\n",
    "\n",
    "| 적합한 경우 | 부적합한 경우 |\n",
    "|------------|-------------|\n",
    "| 일관된 **스타일/톤앤매너** 필요 | 단순히 새로운 **지식** 추가 필요 |\n",
    "| 특정 **출력 형식** 고정 (JSON, 특정 포맷) | 최신 정보가 계속 변경됨 |\n",
    "| **도메인 언어/전문 용어** 학습 | 고품질 학습 데이터가 부족함 |\n",
    "| 복잡한 지시를 **짧은 프롬프트**로 처리 | 빠른 프로토타이핑이 필요함 |\n",
    "| **API 비용 절감** (긴 프롬프트 대체) | 비용 제약이 매우 큼 |\n",
    "\n",
    "> **핵심 원칙**:  \n",
    "> 새로운 '**지식**'을 가르쳐야 한다면 **RAG**를,  \n",
    "> 모델의 '**행동 방식**'을 바꾸고 싶다면 **파인튜닝**을 선택하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Prompting vs RAG vs Fine-tuning 비교\n",
    "\n",
    "### 세 가지 LLM 최적화 기법\n",
    "\n",
    "LLM을 특정 작업에 맞게 조정하는 대표적인 세 가지 방법을 비교해봅시다.\n",
    "\n",
    "| 구분 | **프롬프트 엔지니어링** | **RAG (검색 증강 생성)** | **파인튜닝 (Fine-Tuning)** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **핵심 아이디어** | '지시사항(프롬프트)'을 정교하게 작성 | 외부 지식을 '참고 자료'로 제공 | 모델의 '내부 가중치'를 업데이트 |\n",
    "| **모델 변화** | 없음 | 없음 | **있음** (재학습) |\n",
    "| **적합한 상황** | 간단한 작업, 빠른 프로토타입 | 최신 정보, 사실 기반 답변 | 스타일/톤/형식 일관성 |\n",
    "| **장점** | 빠르고 저렴, 즉시 적용 | 지식 업데이트 용이, 환각 감소 | 짧은 프롬프트로 높은 성능 |\n",
    "| **단점** | 긴 프롬프트 필요, 일관성 부족 | 검색 시스템 구축 필요 | 고품질 데이터 필요, 비용 발생 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 가지 접근법 비교 시각화\n",
    "comparison = {\n",
    "    \"기법\": [\"프롬프트 엔지니어링\", \"RAG\", \"파인튜닝\"],\n",
    "    \"구현 난이도\": [\"낮음\", \"중간\", \"높음\"],\n",
    "    \"초기 비용\": [\"없음\", \"중간 (벡터DB)\", \"높음 (학습)\"],\n",
    "    \"운영 비용\": [\"높음 (긴 프롬프트)\", \"중간 (검색+생성)\", \"낮음 (짧은 프롬프트)\"],\n",
    "    \"지식 업데이트\": [\"즉시\", \"쉬움\", \"재학습 필요\"],\n",
    "    \"일관성\": [\"낮음\", \"중간\", \"높음\"]\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LLM 최적화 기법 비교\")\n",
    "print(\"=\" * 70)\n",
    "for key, values in comparison.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    for i, val in enumerate(values):\n",
    "        print(f\"  - {comparison['기법'][i]}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무에서의 선택 가이드\n",
    "\n",
    "```\n",
    "질문 1: \"최신 정보나 회사 내부 데이터가 필요한가?\"\n",
    "  -> YES: RAG 우선 고려\n",
    "  -> NO: 다음 질문으로\n",
    "\n",
    "질문 2: \"일관된 스타일/형식/어조가 필요한가?\"\n",
    "  -> YES: 파인튜닝 고려\n",
    "  -> NO: 프롬프트 엔지니어링으로 충분\n",
    "\n",
    "질문 3: \"고품질 예시 데이터가 충분한가? (최소 50개 이상)\"\n",
    "  -> YES: 파인튜닝 진행\n",
    "  -> NO: Few-shot 프롬프팅으로 시작\n",
    "```\n",
    "\n",
    "> **실무 팁**: 세 가지 기법은 상호 보완적입니다. 파인튜닝된 모델 + RAG 조합도 가능합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 JSONL 데이터 형식 이해하기\n",
    "\n",
    "### JSONL (JSON Lines) 형식\n",
    "\n",
    "OpenAI 파인튜닝은 **JSONL** 형식의 데이터를 요구합니다. JSONL은 파일의 각 줄(line)이 하나의 독립된 JSON 객체인 형태입니다.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"시스템 메시지\"}, {\"role\": \"user\", \"content\": \"질문\"}, {\"role\": \"assistant\", \"content\": \"이상적인 답변\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"시스템 메시지\"}, {\"role\": \"user\", \"content\": \"두 번째 질문\"}, {\"role\": \"assistant\", \"content\": \"두 번째 답변\"}]}\n",
    "```\n",
    "\n",
    "### 메시지 역할 구조\n",
    "\n",
    "| 역할 | 필수 여부 | 설명 |\n",
    "|------|----------|------|\n",
    "| `system` | 선택 | 모델의 행동 방식, 역할 정의 |\n",
    "| `user` | **필수** | 사용자의 입력/질문 |\n",
    "| `assistant` | **필수** | 모델이 학습할 이상적인 답변 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 형식 예시\n",
    "import json\n",
    "\n",
    "# 단일 학습 예시\n",
    "training_example = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 IT 전문 기자입니다. 뉴스 기사를 '핵심 요약'과 '주요 배경'으로 구조화하여 요약합니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"다음 뉴스 기사를 요약해 주세요:\\n\\n제목: AI 신약 개발 속도 10배 앞당긴다\\n내용: 인공지능이 신약 개발 분야에서 혁신을 주도하고 있다...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"- 핵심 요약: AI가 신약 개발 기간을 10년에서 1-2년으로 단축시키고 있습니다.\\n- 주요 배경: AI 시뮬레이션과 데이터 분석을 통해 유효 물질을 빠르게 선별할 수 있게 되었습니다.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"JSONL 형식 예시:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(training_example, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고품질 학습 데이터의 특성\n",
    "\n",
    "| 특성 | 설명 | 예시 |\n",
    "|------|------|------|\n",
    "| **일관성** | 동일한 형식/스타일 유지 | 모든 답변이 '핵심 요약' + '배경'으로 구성 |\n",
    "| **다양성** | 다양한 입력 케이스 포함 | 긍정적/부정적 뉴스, 다양한 주제 |\n",
    "| **정확성** | 정확한 정보, 오류 없음 | 팩트 체크된 답변 |\n",
    "| **충분한 양** | 최소 50개 이상 권장 | 일반적으로 100-500개 |\n",
    "| **적절한 길이** | 너무 길지도 짧지도 않게 | 입력과 출력의 균형 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 학습 데이터 준비하기\n",
    "\n",
    "### 실습: AI 뉴스 요약 모델을 위한 데이터셋 만들기\n",
    "\n",
    "IT/AI 뉴스 기사를 '핵심 요약'과 '주요 배경' 두 가지 항목으로 구조화하여 요약하는 모델을 위한 학습 데이터를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 설정\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 API 키 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API 키 확인\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"OpenAI API Key 로드 완료: {api_key[:8]}...{api_key[-4:]}\")\n",
    "else:\n",
    "    print(\"주의: OPENAI_API_KEY가 설정되지 않았습니다!\")\n",
    "    print(\".env 파일 또는 환경변수에 API 키를 설정하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝용 샘플 AI 뉴스 데이터\n",
    "training_data = [\n",
    "    {\n",
    "        \"title\": \"데이터브릭스, 차세대 'AI 거버넌스' 프레임워크 '유니티 카탈로그 2.0' 발표\",\n",
    "        \"content\": \"2025년 6월 20일, 데이터 및 AI 기업 데이터브릭스는 연례 컨퍼런스에서 차세대 데이터 거버넌스 프레임워크인 '유니티 카탈로그 2.0'을 공개했다. 이 프레임워크는 데이터, AI 모델, 관련 파이프라인 전체에 걸쳐 통합된 보안 및 거버넌스를 제공하는 것이 특징이다. 특히, 생성형 AI 모델의 출력 결과에 대한 실시간 모니터링과 유해성 탐지 기능을 탑재하여 기업이 책임감 있는 AI를 구현할 수 있도록 지원한다.\",\n",
    "        \"source\": \"데이터 이코노미\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 데이터브릭스가 차세대 데이터 거버넌스 프레임워크 '유니티 카탈로그 2.0'을 발표했습니다.\\n- 주요 배경: 데이터, AI 모델, 파이프라인 전체에 통합된 보안 및 거버넌스를 제공하고, 생성형 AI의 유해성 탐지 기능으로 책임감 있는 AI 구현을 지원하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"AI 옵저버빌리티 플랫폼 '뉴럴와처', 2억 달러 투자 유치\",\n",
    "        \"content\": \"AI 모델의 운영 상태를 실시간으로 감시하고 분석하는 'AI 옵저버빌리티' 분야가 급성장하고 있다. 관련 스타트업 '뉴럴와처(NeuralWatcher)'는 최근 시리즈 C 펀딩에서 2억 달러 규모의 투자를 유치했다고 밝혔다. 뉴럴와처의 플랫폼은 LLM 애플리케이션에서 발생하는 데이터 드리프트, 성능 저하, 환각(Hallucination) 현상을 자동으로 탐지하고 개발자에게 경고한다.\",\n",
    "        \"source\": \"AI 스타트업 위클리\",\n",
    "        \"ideal_response\": \"- 핵심 요약: AI 옵저버빌리티 스타트업 '뉴럴와처'가 시리즈 C 펀딩에서 2억 달러를 유치했습니다.\\n- 주요 배경: LLM 애플리케이션의 데이터 드리프트, 성능 저하, 환각 현상을 자동 탐지하여 AI 서비스의 신뢰도를 높이고 비즈니스 손실을 최소화하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"구글 클라우드, '버텍스 AI 에이전트 빌더' 정식 출시\",\n",
    "        \"content\": \"구글 클라우드는 자사의 AI 플랫폼 버텍스 AI(Vertex AI)에 '에이전트 빌더' 기능을 정식으로 추가했다고 발표했다. 이 기능은 코딩 경험이 없는 사용자도 간단한 자연어 명령어와 그래픽 인터페이스를 통해 복잡한 AI 에이전트를 구축할 수 있도록 돕는다. 사용자는 지식 베이스를 연결하고, 구글 검색, 지도, 캘린더 등 다양한 구글 서비스를 도구로 추가할 수 있다.\",\n",
    "        \"source\": \"클라우드 인사이트\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 구글 클라우드가 버텍스 AI에 '에이전트 빌더' 기능을 정식 출시했습니다.\\n- 주요 배경: 코딩 없이 자연어와 그래픽 인터페이스로 AI 에이전트를 구축할 수 있어, 전문 개발자뿐 아니라 현업 실무자도 직접 AI 솔루션을 만들 수 있는 노코드 AI 시대를 열기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"메타, 차세대 멀티모달 AI 모델 'LLaMA-Vision 3.0' 공개\",\n",
    "        \"content\": \"메타는 텍스트, 이미지, 비디오를 동시에 처리할 수 있는 멀티모달 AI 모델 'LLaMA-Vision 3.0'을 공개했다. 이 모델은 기존 LLaMA 시리즈 대비 시각적 이해 능력이 40% 향상되었으며, 실시간 비디오 분석과 3D 객체 인식 기능을 지원한다. 특히 의료 영상 분석, 자율주행차 인식 시스템, AR/VR 콘텐츠 생성 등 다양한 산업 분야에서 활용 가능하다.\",\n",
    "        \"source\": \"AI 테크 리뷰\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 메타가 텍스트, 이미지, 비디오를 동시에 처리하는 멀티모달 AI 모델 'LLaMA-Vision 3.0'을 공개했습니다.\\n- 주요 배경: 기존 모델 대비 시각적 이해 능력이 40% 향상되었으며, 의료 영상 분석, 자율주행, AR/VR 등 다양한 산업에서 멀티모달 AI 활용이 필요했기 때문입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"오픈AI, 'GPT-5' 개발 중단 발표... 대신 'GPT-4o Ultra' 집중 개발\",\n",
    "        \"content\": \"오픈AI가 차세대 언어모델 'GPT-5' 개발을 중단하고, 대신 현재 GPT-4o의 성능을 극대화한 'GPT-4o Ultra' 개발에 집중한다고 발표했다. 이는 모델 크기를 무작정 키우는 것보다 효율성과 안전성을 우선시하는 전략 변화로 해석된다. GPT-4o Ultra는 기존 모델 대비 추론 속도가 3배 빨라지고, 에너지 효율성은 50% 개선될 예정이다.\",\n",
    "        \"source\": \"AI 뉴스 데일리\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 오픈AI가 GPT-5 개발을 중단하고 'GPT-4o Ultra' 개발에 집중한다고 발표했습니다.\\n- 주요 배경: 모델 크기 증가보다 효율성과 안전성을 우선시하는 전략 변화로, 추론 속도 3배 향상과 에너지 효율성 50% 개선을 통해 AI 발전의 새로운 패러다임을 제시하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"마이크로소프트, 'Azure AI Studio' 대폭 업데이트\",\n",
    "        \"content\": \"마이크로소프트가 클라우드 기반 AI 개발 플랫폼 'Azure AI Studio'의 대규모 업데이트를 발표했다. 새로운 버전은 기업용 AI 애플리케이션 개발을 위한 통합 환경을 제공하며, 데이터 전처리부터 모델 배포까지 전 과정을 하나의 플랫폼에서 처리할 수 있다. 특히 '원클릭 AI 파이프라인' 기능으로 복잡한 MLOps 과정을 자동화하여 개발 시간을 80% 단축시킨다.\",\n",
    "        \"source\": \"엔터프라이즈 IT\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 마이크로소프트가 'Azure AI Studio'의 대규모 업데이트를 발표했습니다.\\n- 주요 배경: 기업용 AI 개발을 위한 통합 환경 제공과 '원클릭 AI 파이프라인'을 통해 MLOps를 자동화하여 개발 시간을 80% 단축하고 엔터프라이즈 AI의 진입 장벽을 낮추기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"엔비디아, 'H200 Ultra' GPU 공개... AI 훈련 성능 2배 향상\",\n",
    "        \"content\": \"엔비디아가 차세대 AI 전용 GPU 'H200 Ultra'를 공개했다. 이 GPU는 기존 H100 대비 AI 모델 훈련 성능이 2배 향상되었으며, 메모리 용량도 188GB로 확대되어 더 큰 규모의 모델 훈련이 가능하다. 특히 트랜스포머 아키텍처 기반 모델의 훈련에 최적화되어 있어, LLM 개발 시간을 크게 단축시킬 수 있다. 또한 새로운 'NVLink 5.0' 기술로 GPU 간 데이터 전송 속도가 3배 빨라졌다.\",\n",
    "        \"source\": \"하드웨어 인사이트\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 엔비디아가 AI 전용 GPU 'H200 Ultra'를 공개했습니다.\\n- 주요 배경: 기존 H100 대비 훈련 성능 2배 향상, 메모리 188GB 확대, NVLink 5.0으로 데이터 전송 속도 3배 향상을 통해 대규모 LLM 개발을 위한 핵심 인프라를 구축하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"스탠포드 대학, AI 안전성 벤치마크 'SafetyBench 2.0' 발표\",\n",
    "        \"content\": \"스탠포드 대학 AI 연구소가 AI 모델의 안전성을 종합적으로 평가하는 새로운 벤치마크 'SafetyBench 2.0'을 발표했다. 이 벤치마크는 편향성, 독성, 환각, 개인정보 유출 등 12개 카테고리에서 AI 모델의 위험성을 정량적으로 측정한다. 기존 평가 방식과 달리 실제 사용 환경을 모방한 시나리오 기반 테스트를 도입하여 더욱 현실적인 평가가 가능하다.\",\n",
    "        \"source\": \"AI 안전성 저널\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 스탠포드 대학이 AI 안전성 평가 벤치마크 'SafetyBench 2.0'을 발표했습니다.\\n- 주요 배경: 편향성, 독성, 환각, 개인정보 유출 등 12개 카테고리에서 실제 사용 환경을 모방한 시나리오 기반 테스트로 AI 모델의 위험성을 정량적으로 측정하여 AI 안전성 연구의 중요성을 부각시키기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"테슬라, FSD v13 업데이트 배포... 도심 주행 성공률 95% 달성\",\n",
    "        \"content\": \"테슬라가 완전 자율주행 소프트웨어 'FSD v13' 업데이트를 배포했다. 이번 버전은 복잡한 도심 환경에서의 주행 성공률이 95%에 달해, 상용화 수준에 근접했다는 평가를 받고 있다. 특히 교차로 좌회전, 보행자 회피, 공사 구간 우회 등 까다로운 상황에서의 판단 능력이 크게 향상되었다. 테슬라는 2025년 말까지 완전 무인 택시 서비스를 시작할 계획이다.\",\n",
    "        \"source\": \"자율주행 테크\",\n",
    "        \"ideal_response\": \"- 핵심 요약: 테슬라가 FSD v13 업데이트를 배포하여 도심 주행 성공률 95%를 달성했습니다.\\n- 주요 배경: 교차로 좌회전, 보행자 회피, 공사 구간 우회 등 까다로운 상황에서의 판단 능력 향상을 통해 2025년 말 완전 무인 택시 서비스 시작을 목표로 하는 교통 혁명을 준비하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"IBM, 양자-AI 융합 플랫폼 'Quantum-AI Hybrid' 공개\",\n",
    "        \"content\": \"IBM이 양자 컴퓨팅과 인공지능을 결합한 혁신적인 플랫폼 'Quantum-AI Hybrid'를 공개했다. 이 플랫폼은 양자 컴퓨터의 병렬 처리 능력과 AI의 패턴 인식 능력을 결합하여, 기존 컴퓨터로는 해결하기 어려운 복잡한 최적화 문제를 해결한다. 특히 신약 개발, 금융 리스크 분석, 물류 최적화 등의 분야에서 기존 방식 대비 1000배 빠른 처리 속도를 보여준다.\",\n",
    "        \"source\": \"퀀텀 컴퓨팅 리뷰\",\n",
    "        \"ideal_response\": \"- 핵심 요약: IBM이 양자 컴퓨팅과 AI를 결합한 'Quantum-AI Hybrid' 플랫폼을 공개했습니다.\\n- 주요 배경: 양자 컴퓨터의 병렬 처리와 AI의 패턴 인식을 결합하여 신약 개발, 금융 리스크, 물류 최적화 등에서 기존 대비 1000배 빠른 처리로 컴퓨팅 패러다임의 근본적 변화를 가져오기 위함입니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"총 {len(training_data)}개의 학습 데이터 준비 완료\")\n",
    "print(\"\\n데이터 미리보기:\")\n",
    "for i, data in enumerate(training_data[:3], 1):\n",
    "    print(f\"\\n[{i}] {data['title'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 JSONL 파일 생성하기\n",
    "\n",
    "### 학습 데이터를 JSONL 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 시스템 메시지 정의\n",
    "system_message = \"당신은 IT 전문 기자입니다. 주어진 뉴스 기사를 '핵심 요약'과 '주요 배경'으로 나누어 구조화된 형식으로 요약해야 합니다.\"\n",
    "\n",
    "# JSONL 데이터 생성\n",
    "jsonl_data = []\n",
    "\n",
    "for article in training_data:\n",
    "    user_prompt = f\"다음 뉴스 기사를 요약해 주세요:\\n\\n제목: {article['title']}\\n내용: {article['content']}\"\n",
    "    \n",
    "    jsonl_data.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": article['ideal_response']}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# 파일 저장\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "file_path = \"datasets/news_finetuning_data.jsonl\"\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL 파일 생성 완료: {file_path}\")\n",
    "print(f\"총 {len(jsonl_data)}개의 학습 예시 포함\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 JSONL 파일 확인\n",
    "print(\"생성된 JSONL 파일 내용 (처음 2줄):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 2:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        print(f\"\\n[예시 {i+1}]\")\n",
    "        print(f\"System: {data['messages'][0]['content'][:50]}...\")\n",
    "        print(f\"User: {data['messages'][1]['content'][:50]}...\")\n",
    "        print(f\"Assistant: {data['messages'][2]['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: 심화\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 OpenAI 파인튜닝 프로세스\n",
    "\n",
    "### 파인튜닝 4단계\n",
    "\n",
    "| 단계 | 설명 | 소요 시간 |\n",
    "|------|------|----------|\n",
    "| **Step 1** | 데이터셋 준비 (JSONL) | 수 시간~수 일 |\n",
    "| **Step 2** | 파일 업로드 | 수 초 |\n",
    "| **Step 3** | 파인튜닝 작업 생성 및 실행 | 수 분~수 시간 |\n",
    "| **Step 4** | 파인튜닝된 모델 사용 | 즉시 |\n",
    "\n",
    "### 지원 모델 (2025년 1월 기준)\n",
    "\n",
    "| 모델 | 특징 | 파인튜닝 비용 |\n",
    "|------|------|-------------|\n",
    "| gpt-4o-2024-08-06 | 최신 플래그십 | $25/1M 토큰 |\n",
    "| gpt-4o-mini-2024-07-18 | 비용 효율적 | $3/1M 토큰 |\n",
    "| gpt-4.1-mini-2025-04-14 | 최신 경량 모델 | $3/1M 토큰 |\n",
    "| gpt-3.5-turbo-0125 | 레거시 | $8/1M 토큰 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 클라이언트 초기화\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"OpenAI 클라이언트 초기화 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 학습 데이터 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 업로드\n",
    "training_file = client.files.create(\n",
    "    file=open(file_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(f\"파일 업로드 완료!\")\n",
    "print(f\"파일 ID: {training_file.id}\")\n",
    "print(f\"파일명: {training_file.filename}\")\n",
    "print(f\"상태: {training_file.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: 파인튜닝 작업 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝 작업 생성\n",
    "# 참고: gpt-4o-mini 또는 gpt-4.1-mini 사용 권장 (비용 효율적)\n",
    "target_model = \"gpt-4o-mini-2024-07-18\"  # 또는 \"gpt-4.1-mini-2025-04-14\"\n",
    "\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file.id,\n",
    "    model=target_model,\n",
    "    # hyperparameters={  # 선택적 하이퍼파라미터\n",
    "    #     \"n_epochs\": 3,\n",
    "    #     \"learning_rate_multiplier\": 1.0\n",
    "    # }\n",
    ")\n",
    "\n",
    "job_id = fine_tuning_job.id\n",
    "print(f\"파인튜닝 작업 생성 완료!\")\n",
    "print(f\"작업 ID: {job_id}\")\n",
    "print(f\"상태: {fine_tuning_job.status}\")\n",
    "print(f\"기반 모델: {target_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 파인튜닝 작업 모니터링\n",
    "\n",
    "### 작업 상태 확인\n",
    "\n",
    "파인튜닝 작업은 데이터 크기에 따라 수 분에서 수 시간까지 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작업 상태 확인 함수\n",
    "def check_finetuning_status(job_id):\n",
    "    \"\"\"파인튜닝 작업 상태 확인\"\"\"\n",
    "    job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    \n",
    "    print(f\"작업 ID: {job_info.id}\")\n",
    "    print(f\"상태: {job_info.status}\")\n",
    "    print(f\"기반 모델: {job_info.model}\")\n",
    "    \n",
    "    if job_info.finished_at:\n",
    "        print(f\"완료 시간: {job_info.finished_at}\")\n",
    "    \n",
    "    if job_info.fine_tuned_model:\n",
    "        print(f\"파인튜닝된 모델: {job_info.fine_tuned_model}\")\n",
    "    \n",
    "    if job_info.error:\n",
    "        print(f\"에러: {job_info.error}\")\n",
    "    \n",
    "    return job_info\n",
    "\n",
    "# 현재 상태 확인\n",
    "job_info = check_finetuning_status(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 모니터링 (작업 완료까지 대기)\n",
    "import time\n",
    "\n",
    "def wait_for_finetuning(job_id, check_interval=60):\n",
    "    \"\"\"파인튜닝 작업 완료까지 대기\"\"\"\n",
    "    print(f\"파인튜닝 작업 모니터링 시작 (매 {check_interval}초마다 확인)...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        status = job_info.status\n",
    "        \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] 상태: {status}\")\n",
    "        \n",
    "        if status == \"succeeded\":\n",
    "            print(\"\\n파인튜닝 완료!\")\n",
    "            print(f\"새로운 모델 ID: {job_info.fine_tuned_model}\")\n",
    "            return job_info.fine_tuned_model\n",
    "        \n",
    "        elif status == \"failed\":\n",
    "            print(\"\\n파인튜닝 실패!\")\n",
    "            print(f\"에러: {job_info.error}\")\n",
    "            return None\n",
    "        \n",
    "        elif status == \"cancelled\":\n",
    "            print(\"\\n파인튜닝 취소됨\")\n",
    "            return None\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "\n",
    "# 주의: 이 셀은 파인튜닝이 완료될 때까지 대기합니다 (수 분~수 시간)\n",
    "# 테스트 시에는 주석 처리하고, 수동으로 상태를 확인하세요\n",
    "\n",
    "# fine_tuned_model_id = wait_for_finetuning(job_id, check_interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝 이벤트 로그 확인\n",
    "events = client.fine_tuning.jobs.list_events(job_id, limit=10)\n",
    "\n",
    "print(\"파인튜닝 이벤트 로그 (최근 10개):\")\n",
    "print(\"=\" * 60)\n",
    "for event in events.data:\n",
    "    print(f\"[{event.created_at}] {event.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 파인튜닝된 모델 사용하기\n",
    "\n",
    "### 파인튜닝 완료 후 모델 ID 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완료된 파인튜닝 작업에서 모델 ID 가져오기\n",
    "job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "if job_info.status == \"succeeded\":\n",
    "    fine_tuned_model_id = job_info.fine_tuned_model\n",
    "    print(f\"파인튜닝된 모델 ID: {fine_tuned_model_id}\")\n",
    "else:\n",
    "    print(f\"작업이 아직 완료되지 않았습니다. 현재 상태: {job_info.status}\")\n",
    "    fine_tuned_model_id = None  # 또는 이전에 완료된 모델 ID를 직접 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 파인튜닝한 모델이 있다면 직접 ID 입력\n",
    "# fine_tuned_model_id = \"ft:gpt-4o-mini-2024-07-18:your-org::xxxxx\"\n",
    "\n",
    "if fine_tuned_model_id:\n",
    "    print(f\"사용할 모델: {fine_tuned_model_id}\")\n",
    "else:\n",
    "    print(\"파인튜닝된 모델 ID가 없습니다. 작업 완료를 기다리거나 ID를 직접 입력하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 모델 vs 파인튜닝 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 새로운 뉴스 기사\n",
    "test_article = {\n",
    "    \"title\": \"메타, 차세대 언어 모델 '라마 4' 공개... 온디바이스 AI 시장 정조준\",\n",
    "    \"content\": \"페이스북의 모회사 메타가 최신 거대 언어 모델(LLM)인 '라마 4(Llama 4)'를 공개했습니다. 라마 4는 이전 모델보다 크기는 줄이면서도 성능은 대폭 향상시킨 것이 특징입니다. 특히, 스마트폰과 같은 개인 기기에서 직접 구동되는 온디바이스 AI 환경에 최적화되어, 인터넷 연결 없이도 빠른 속도로 고품질의 AI 기능을 제공할 수 있습니다. 메타는 라마 4를 오픈소스로 공개하여, 더 많은 개발자들이 AI 기술에 접근하고 혁신을 가속화할 수 있도록 지원할 것이라고 밝혔습니다.\"\n",
    "}\n",
    "\n",
    "test_prompt = f\"다음 뉴스 기사를 요약해 주세요:\\n\\n제목: {test_article['title']}\\n내용: {test_article['content']}\"\n",
    "\n",
    "print(\"테스트 기사:\")\n",
    "print(f\"제목: {test_article['title']}\")\n",
    "print(f\"내용: {test_article['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 모델 응답\n",
    "print(\"=\" * 70)\n",
    "print(\"1. 기본 모델 (gpt-4o-mini) 응답\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_response = client.chat.completions.create(\n",
    "    model=target_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": test_prompt}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(base_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델 응답\n",
    "if fine_tuned_model_id:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"2. 파인튜닝된 모델 응답\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 파인튜닝된 모델은 시스템 메시지 없이도 학습된 형식으로 응답\n",
    "    ft_response = client.chat.completions.create(\n",
    "        model=fine_tuned_model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": test_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(ft_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"파인튜닝된 모델이 없어 비교할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 하이퍼파라미터 조정하기\n",
    "\n",
    "### 주요 하이퍼파라미터\n",
    "\n",
    "| 파라미터 | 설명 | 기본값 | 권장 범위 |\n",
    "|---------|------|--------|----------|\n",
    "| `n_epochs` | 학습 반복 횟수 | 자동 | 1-10 |\n",
    "| `learning_rate_multiplier` | 학습률 배수 | 자동 | 0.1-2.0 |\n",
    "| `batch_size` | 배치 크기 | 자동 | 1-64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터를 지정한 파인튜닝 (예시)\n",
    "# 주의: 실제 실행 시 비용이 발생합니다\n",
    "\n",
    "# fine_tuning_job_custom = client.fine_tuning.jobs.create(\n",
    "#     training_file=training_file.id,\n",
    "#     model=\"gpt-4o-mini-2024-07-18\",\n",
    "#     hyperparameters={\n",
    "#         \"n_epochs\": 3,  # 학습 반복 횟수\n",
    "#         \"learning_rate_multiplier\": 1.5,  # 학습률 배수\n",
    "#     }\n",
    "# )\n",
    "\n",
    "print(\"하이퍼파라미터 조정 예시:\")\n",
    "print(\"- n_epochs: 3 (기본: 자동)\")\n",
    "print(\"- learning_rate_multiplier: 1.5 (기본: 자동)\")\n",
    "print(\"\\n주의: 실제 실행 시 추가 비용이 발생합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 조정 가이드\n",
    "\n",
    "| 상황 | 권장 조정 |\n",
    "|------|----------|\n",
    "| 과적합 (train loss 낮고 val loss 높음) | `n_epochs` 감소, `learning_rate_multiplier` 감소 |\n",
    "| 과소적합 (train loss 높음) | `n_epochs` 증가, `learning_rate_multiplier` 증가 |\n",
    "| 데이터가 적을 때 (50개 미만) | `n_epochs` 증가, `learning_rate_multiplier` 감소 |\n",
    "| 데이터가 많을 때 (1000개 이상) | `n_epochs` 감소 (3 이하) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.5 실무 활용 및 비용 최적화\n",
    "\n",
    "### 파인튜닝 비용 구조\n",
    "\n",
    "```\n",
    "총 비용 = 학습 비용 + 추론 비용\n",
    "\n",
    "학습 비용:\n",
    "  - gpt-4o-mini: $3 / 1M 학습 토큰\n",
    "  - 예: 100개 예시 x 500 토큰 x 3 에포크 = 150K 토큰 = $0.45\n",
    "\n",
    "추론 비용 (파인튜닝된 모델):\n",
    "  - gpt-4o-mini: 입력 $0.30 / 출력 $1.20 per 1M 토큰\n",
    "  - 기본 모델보다 약간 비싸지만, 짧은 프롬프트로 상쇄 가능\n",
    "```\n",
    "\n",
    "### 비용 최적화 전략\n",
    "\n",
    "| 전략 | 설명 |\n",
    "|------|------|\n",
    "| 작은 모델 선택 | gpt-4o보다 gpt-4o-mini 사용 |\n",
    "| 데이터 품질 > 양 | 50개 고품질 > 500개 저품질 |\n",
    "| 에포크 조정 | 과적합 방지로 학습 비용 절감 |\n",
    "| 짧은 프롬프트 | 학습 후 시스템 메시지 생략 가능 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 추정 함수\n",
    "def estimate_finetuning_cost(num_examples, avg_tokens_per_example, epochs=3):\n",
    "    \"\"\"파인튜닝 비용 추정 (gpt-4o-mini 기준)\"\"\"\n",
    "    training_cost_per_1m = 3.0  # $3 per 1M tokens\n",
    "    \n",
    "    total_tokens = num_examples * avg_tokens_per_example * epochs\n",
    "    estimated_cost = (total_tokens / 1_000_000) * training_cost_per_1m\n",
    "    \n",
    "    print(f\"파인튜닝 비용 추정 (gpt-4o-mini):\")\n",
    "    print(f\"  - 예시 수: {num_examples}개\")\n",
    "    print(f\"  - 예시당 평균 토큰: {avg_tokens_per_example}\")\n",
    "    print(f\"  - 에포크: {epochs}\")\n",
    "    print(f\"  - 총 학습 토큰: {total_tokens:,}\")\n",
    "    print(f\"  - 예상 비용: ${estimated_cost:.2f}\")\n",
    "    \n",
    "    return estimated_cost\n",
    "\n",
    "# 현재 데이터셋 비용 추정\n",
    "estimate_finetuning_cost(len(training_data), 500, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파인튜닝 모델 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델 목록 확인\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"파인튜닝된 모델 목록:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ft_models = [m for m in models.data if m.id.startswith(\"ft:\")]\n",
    "\n",
    "if ft_models:\n",
    "    for model in ft_models:\n",
    "        print(f\"  - {model.id}\")\n",
    "        print(f\"    생성일: {model.created}\")\n",
    "else:\n",
    "    print(\"  파인튜닝된 모델이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델 삭제 (필요시)\n",
    "# 주의: 삭제된 모델은 복구할 수 없습니다\n",
    "\n",
    "# if fine_tuned_model_id:\n",
    "#     client.models.delete(fine_tuned_model_id)\n",
    "#     print(f\"모델 삭제됨: {fine_tuned_model_id}\")\n",
    "\n",
    "print(\"모델 삭제 예시 (주석 처리됨):\")\n",
    "print(\"client.models.delete(model_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈\n",
    "\n",
    "**난이도 분포**: 기본 3개 / 응용 3개 / 복합 2개 / 종합 2개\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 파인튜닝 개념 이해 (기본)\n",
    "\n",
    "**문제**: 다음 빈칸을 채우세요.\n",
    "\n",
    "1. 파인튜닝은 모델의 내부 (______)을 업데이트하는 과정입니다.\n",
    "2. 새로운 '지식'을 추가하려면 (______)가 적합하고, '행동 방식'을 바꾸려면 (______)가 적합합니다.\n",
    "3. OpenAI 파인튜닝 데이터는 (______) 형식으로 준비해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 답을 작성하세요\n",
    "answer1 = \"\"\n",
    "answer2_knowledge = \"\"\n",
    "answer2_behavior = \"\"\n",
    "answer3 = \"\"\n",
    "\n",
    "print(f\"1. {answer1}\")\n",
    "print(f\"2. 지식: {answer2_knowledge}, 행동: {answer2_behavior}\")\n",
    "print(f\"3. {answer3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. JSONL 형식 작성 (기본)\n",
    "\n",
    "**문제**: 다음 정보를 JSONL 형식으로 변환하세요.\n",
    "\n",
    "- 시스템 메시지: \"당신은 친절한 고객 상담원입니다.\"\n",
    "- 사용자 질문: \"환불은 어떻게 하나요?\"\n",
    "- 이상적 답변: \"환불은 구매일로부터 7일 이내에 가능합니다. 마이페이지에서 신청하실 수 있습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 적합한 기술 선택 (기본)\n",
    "\n",
    "**문제**: 다음 각 상황에 가장 적합한 기술(프롬프트 엔지니어링, RAG, 파인튜닝)을 선택하세요.\n",
    "\n",
    "1. 회사의 최신 제품 정보를 기반으로 고객 질문에 답변하는 챗봇\n",
    "2. 모든 마케팅 문구를 특정 브랜드 톤앤매너로 작성하는 봇\n",
    "3. 간단한 텍스트 번역 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 답을 작성하세요\n",
    "scenario_1 = \"\"  # RAG / Fine-tuning / Prompting\n",
    "scenario_2 = \"\"\n",
    "scenario_3 = \"\"\n",
    "\n",
    "print(f\"1. 최신 제품 정보 챗봇: {scenario_1}\")\n",
    "print(f\"2. 브랜드 톤앤매너 유지: {scenario_2}\")\n",
    "print(f\"3. 간단한 번역: {scenario_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. 학습 데이터 생성 (응용)\n",
    "\n",
    "**문제**: \"친근한 말투의 AI 비서\"를 파인튜닝하기 위한 학습 데이터 3개를 JSONL 형식으로 작성하세요. 각 예시는 다른 주제(날씨, 일정, 추천)여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. 파인튜닝 작업 관리 (응용)\n",
    "\n",
    "**문제**: 다음 작업을 수행하는 코드를 작성하세요.\n",
    "\n",
    "1. 현재 진행 중인 파인튜닝 작업 목록 확인\n",
    "2. 가장 최근 작업의 상태와 이벤트 로그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. 비용 계산 (응용)\n",
    "\n",
    "**문제**: 다음 조건의 파인튜닝 비용을 계산하세요.\n",
    "\n",
    "- 학습 데이터: 200개 예시\n",
    "- 예시당 평균 토큰: 800\n",
    "- 에포크: 5\n",
    "- 모델: gpt-4o-mini ($3/1M 학습 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. 데이터 품질 검증 (복합)\n",
    "\n",
    "**문제**: JSONL 파일을 읽어 다음을 검증하는 함수를 작성하세요.\n",
    "\n",
    "1. 각 줄이 유효한 JSON인지 확인\n",
    "2. 필수 키(messages)가 있는지 확인\n",
    "3. 각 메시지에 role과 content가 있는지 확인\n",
    "4. 총 예시 수와 평균 토큰 수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_jsonl(file_path):\n",
    "    \"\"\"JSONL 파일 품질 검증\"\"\"\n",
    "    # 여기에 코드를 작성하세요\n",
    "    pass\n",
    "\n",
    "# 테스트\n",
    "# validate_jsonl(\"datasets/news_finetuning_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. 모델 비교 함수 (복합)\n",
    "\n",
    "**문제**: 기본 모델과 파인튜닝된 모델의 응답을 비교하는 함수를 작성하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 동일한 프롬프트로 두 모델 호출\n",
    "2. 응답 내용, 응답 시간, 토큰 사용량 비교\n",
    "3. 결과를 딕셔너리로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_models(prompt, base_model, finetuned_model, system_message=None):\n",
    "    \"\"\"기본 모델과 파인튜닝 모델 비교\"\"\"\n",
    "    # 여기에 코드를 작성하세요\n",
    "    pass\n",
    "\n",
    "# 테스트\n",
    "# result = compare_models(\n",
    "#     \"AI의 미래에 대해 설명해주세요.\",\n",
    "#     \"gpt-4o-mini\",\n",
    "#     fine_tuned_model_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. 도메인 특화 데이터셋 생성 (종합)\n",
    "\n",
    "**문제**: 특정 도메인(예: 법률, 의료, 금융 중 선택)에 특화된 Q&A 데이터셋을 5개 이상 생성하고, JSONL 파일로 저장하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 일관된 답변 형식 유지\n",
    "2. 도메인 전문 용어 포함\n",
    "3. 다양한 질문 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 파인튜닝 파이프라인 구축 (종합)\n",
    "\n",
    "**문제**: 파인튜닝 전체 파이프라인을 클래스로 구현하세요.\n",
    "\n",
    "요구 기능:\n",
    "1. `prepare_data()`: 데이터 준비 및 JSONL 생성\n",
    "2. `upload_and_train()`: 파일 업로드 및 파인튜닝 시작\n",
    "3. `monitor()`: 작업 상태 모니터링\n",
    "4. `evaluate()`: 파인튜닝된 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningPipeline:\n",
    "    \"\"\"OpenAI 파인튜닝 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-4o-mini-2024-07-18\"):\n",
    "        self.client = OpenAI()\n",
    "        self.model_name = model_name\n",
    "        self.training_file_id = None\n",
    "        self.job_id = None\n",
    "        self.finetuned_model_id = None\n",
    "    \n",
    "    def prepare_data(self, data, output_path):\n",
    "        \"\"\"학습 데이터 준비 및 JSONL 저장\"\"\"\n",
    "        # 여기에 코드를 작성하세요\n",
    "        pass\n",
    "    \n",
    "    def upload_and_train(self, file_path, hyperparameters=None):\n",
    "        \"\"\"파일 업로드 및 파인튜닝 시작\"\"\"\n",
    "        # 여기에 코드를 작성하세요\n",
    "        pass\n",
    "    \n",
    "    def monitor(self, check_interval=60):\n",
    "        \"\"\"파인튜닝 작업 모니터링\"\"\"\n",
    "        # 여기에 코드를 작성하세요\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, test_prompts):\n",
    "        \"\"\"파인튜닝된 모델 평가\"\"\"\n",
    "        # 여기에 코드를 작성하세요\n",
    "        pass\n",
    "\n",
    "# 테스트\n",
    "# pipeline = FineTuningPipeline()\n",
    "# pipeline.prepare_data(training_data, \"datasets/custom_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 실무 활용 |\n",
    "|------|----------|----------|\n",
    "| 파인튜닝 | 모델의 내부 가중치를 업데이트 | 스타일/톤/형식 일관성 |\n",
    "| Prompting vs RAG vs FT | 각각 다른 문제 해결 | 상황에 맞는 기술 선택 |\n",
    "| JSONL 형식 | messages 배열 (system, user, assistant) | 학습 데이터 준비 |\n",
    "| 데이터 품질 | 일관성, 다양성, 정확성 | 최소 50개 이상 권장 |\n",
    "| 파일 업로드 | client.files.create() | purpose=\"fine-tune\" |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 기법 | 권장 설정 |\n",
    "|------|----------|----------|\n",
    "| 파인튜닝 API | client.fine_tuning.jobs.create() | gpt-4o-mini 권장 |\n",
    "| 모니터링 | jobs.retrieve(), list_events() | 60초 간격 체크 |\n",
    "| 하이퍼파라미터 | n_epochs, learning_rate_multiplier | 자동 설정 우선 |\n",
    "| 비용 최적화 | 작은 모델, 품질 데이터, 짧은 프롬프트 | $3/1M 토큰 (학습) |\n",
    "\n",
    "### 파인튜닝 체크리스트\n",
    "\n",
    "```\n",
    "1. 사전 준비:\n",
    "   [ ] 파인튜닝이 적합한 상황인지 확인\n",
    "   [ ] 고품질 예시 데이터 50개 이상 준비\n",
    "   [ ] 일관된 형식/스타일 유지\n",
    "   \n",
    "2. 데이터 준비:\n",
    "   [ ] JSONL 형식으로 변환\n",
    "   [ ] messages 구조 검증 (system, user, assistant)\n",
    "   [ ] 토큰 수 확인 (너무 길지 않게)\n",
    "   \n",
    "3. 파인튜닝 실행:\n",
    "   [ ] 파일 업로드 (purpose=\"fine-tune\")\n",
    "   [ ] 작업 생성 (모델, 하이퍼파라미터)\n",
    "   [ ] 상태 모니터링\n",
    "   \n",
    "4. 평가 및 배포:\n",
    "   [ ] 기본 모델과 성능 비교\n",
    "   [ ] 다양한 테스트 케이스 검증\n",
    "   [ ] 프로덕션 적용\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **RAG vs 파인튜닝**: 지식 추가는 RAG, 행동 변화는 파인튜닝\n",
    "2. **데이터 품질이 핵심**: 50개 고품질 > 500개 저품질\n",
    "3. **작은 모델부터**: gpt-4o-mini로 시작, 필요시 gpt-4o로 업그레이드\n",
    "4. **시스템 메시지 생략**: 파인튜닝 후에는 짧은 프롬프트로 충분\n",
    "5. **비용 모니터링**: 학습 비용 + 추론 비용 함께 고려"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
