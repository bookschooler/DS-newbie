{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day16_1: MLP (다층 퍼셉트론)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: 기초**\n",
    "1. 퍼셉트론의 구조와 동작 원리 이해하기\n",
    "2. 활성화 함수(Sigmoid, Tanh, ReLU) 비교하기\n",
    "3. 손실 함수(MSE, Cross-Entropy) 이해하기\n",
    "4. 경사 하강법 개념 이해하기\n",
    "5. XOR 문제와 MLP의 필요성 이해하기\n",
    "\n",
    "**Part 2: 심화**\n",
    "1. 역전파 알고리즘 개념적으로 이해하기\n",
    "2. PyTorch로 MLP 구현하기\n",
    "3. 옵티마이저(SGD, Adam) 비교하기\n",
    "4. MNIST 손글씨 분류 실습하기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| 퍼셉트론 | 신경망의 기본 단위 | 모든 딥러닝 모델의 구성 요소 |\n",
    "| 활성화 함수 | 비선형성 추가 | ReLU로 깊은 네트워크 학습 가능 |\n",
    "| 손실 함수 | 학습 목표 정의 | 분류/회귀에 맞는 손실 선택 |\n",
    "| MLP | 정형 데이터 분류/회귀 | 고객 이탈 예측, 가격 예측 |\n",
    "\n",
    "**분석가 관점**: MLP는 딥러닝의 기본 구조입니다. CNN, RNN, Transformer 모두 MLP를 기반으로 확장됩니다. 이 개념을 확실히 이해하면 모든 딥러닝 아키텍처의 동작 원리를 파악할 수 있습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 기초\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 퍼셉트론 (Perceptron)\n",
    "\n",
    "### 생물학적 뉴런 비유\n",
    "\n",
    "```\n",
    "생물학적 뉴런:           인공 뉴런 (퍼셉트론):\n",
    "  수상돌기 (입력)    ->    입력 (x1, x2, ...)\n",
    "  세포체 (처리)      ->    가중합 + 편향\n",
    "  축삭돌기 (출력)    ->    활성화 함수 -> 출력\n",
    "```\n",
    "\n",
    "### 퍼셉트론 수식\n",
    "\n",
    "```\n",
    "y = f(w1*x1 + w2*x2 + ... + wn*xn + b)\n",
    "y = f(sum(wi*xi) + b)\n",
    "y = f(W @ X + b)\n",
    "\n",
    "- X: 입력 벡터\n",
    "- W: 가중치 벡터 (학습되는 파라미터)\n",
    "- b: 편향 (학습되는 파라미터)\n",
    "- f: 활성화 함수\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 퍼셉트론 구현 (NumPy)\n",
    "def perceptron(x, w, b, activation='step'):\n",
    "    \"\"\"\n",
    "    단일 퍼셉트론\n",
    "    x: 입력 벡터\n",
    "    w: 가중치 벡터\n",
    "    b: 편향\n",
    "    activation: 활성화 함수 종류\n",
    "    \"\"\"\n",
    "    # 가중합 계산\n",
    "    z = np.dot(w, x) + b\n",
    "    \n",
    "    # 활성화 함수 적용\n",
    "    if activation == 'step':  # 계단 함수 (원래 퍼셉트론)\n",
    "        return 1 if z >= 0 else 0\n",
    "    elif activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        return z  # 선형 (활성화 없음)\n",
    "\n",
    "# AND 게이트 구현\n",
    "print(\"AND 게이트 (퍼셉트론)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 가중치와 편향 (수동 설정)\n",
    "w_and = np.array([0.5, 0.5])\n",
    "b_and = -0.7\n",
    "\n",
    "# 테스트\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "for x in inputs:\n",
    "    output = perceptron(np.array(x), w_and, b_and, 'step')\n",
    "    print(f\"AND({x[0]}, {x[1]}) = {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR 게이트 구현\n",
    "print(\"OR 게이트 (퍼셉트론)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "w_or = np.array([0.5, 0.5])\n",
    "b_or = -0.2\n",
    "\n",
    "for x in inputs:\n",
    "    output = perceptron(np.array(x), w_or, b_or, 'step')\n",
    "    print(f\"OR({x[0]}, {x[1]}) = {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 퍼셉트론의 결정 경계\n",
    "\n",
    "퍼셉트론은 **선형 결정 경계**를 학습합니다:\n",
    "- w1*x1 + w2*x2 + b = 0\n",
    "- 이 직선이 두 클래스를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND 게이트의 결정 경계 시각화\n",
    "x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "\n",
    "# 결정 경계: w1*x1 + w2*x2 + b = 0\n",
    "# x2 = -(w1*x1 + b) / w2\n",
    "x2_boundary_and = -(w_and[0] * x1_range + b_and) / w_and[1]\n",
    "x2_boundary_or = -(w_or[0] * x1_range + b_or) / w_or[1]\n",
    "\n",
    "# 데이터 포인트\n",
    "points = pd.DataFrame({\n",
    "    'x1': [0, 0, 1, 1],\n",
    "    'x2': [0, 1, 0, 1],\n",
    "    'AND': [0, 0, 0, 1],\n",
    "    'OR': [0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# 시각화\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['AND 게이트', 'OR 게이트'])\n",
    "\n",
    "# AND 게이트\n",
    "colors_and = ['blue' if y == 0 else 'red' for y in points['AND']]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=points['x1'], y=points['x2'], mode='markers', \n",
    "               marker=dict(size=15, color=colors_and), name='AND 데이터'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x1_range, y=x2_boundary_and, mode='lines',\n",
    "               line=dict(color='green', dash='dash'), name='결정 경계'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# OR 게이트\n",
    "colors_or = ['blue' if y == 0 else 'red' for y in points['OR']]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=points['x1'], y=points['x2'], mode='markers',\n",
    "               marker=dict(size=15, color=colors_or), name='OR 데이터'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=x1_range, y=x2_boundary_or, mode='lines',\n",
    "               line=dict(color='green', dash='dash'), name='결정 경계'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[-0.5, 1.5], title_text='x1')\n",
    "fig.update_yaxes(range=[-0.5, 1.5], title_text='x2')\n",
    "fig.update_layout(title='퍼셉트론의 선형 결정 경계', height=400, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈 정답\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 퍼셉트론 출력 계산하기 \n",
    "\n",
    "**문제**: 아래 퍼셉트론의 출력을 계산하세요 (Sigmoid 활성화 사용).\n",
    "\n",
    "- 입력: x = [1.0, 2.0]\n",
    "- 가중치: w = [0.5, -0.5]\n",
    "- 편향: b = 0.1\n",
    "\n",
    "**기대 결과**: Sigmoid 출력 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 정답\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([0.5, -0.5])\n",
    "b = 0.1\n",
    "\n",
    "# 1. 가중합 계산: z = w @ x + b\n",
    "z = np.dot(w, x) + b\n",
    "print(f\"가중합 z = w @ x + b\")\n",
    "print(f\"z = {w[0]}*{x[0]} + {w[1]}*{x[1]} + {b}\")\n",
    "print(f\"z = {w[0]*x[0]} + {w[1]*x[1]} + {b}\")\n",
    "print(f\"z = {z}\")\n",
    "\n",
    "# 2. Sigmoid 활성화 적용\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "output = sigmoid(z)\n",
    "print(f\"\\nSigmoid(z) = 1 / (1 + exp(-{z}))\")\n",
    "print(f\"출력: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 케이스로 검증\n",
    "expected_z = 0.5*1.0 + (-0.5)*2.0 + 0.1  # = 0.5 - 1.0 + 0.1 = -0.4\n",
    "expected_output = 1 / (1 + np.exp(0.4))  # sigmoid(-0.4)\n",
    "\n",
    "assert abs(z - (-0.4)) < 1e-6, f\"가중합 계산 오류: {z}\"\n",
    "assert abs(output - expected_output) < 1e-6, f\"Sigmoid 계산 오류: {output}\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 퍼셉트론의 계산 과정을 순서대로 수행합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "1. **가중합**: z = w1*x1 + w2*x2 + b = 0.5*1.0 + (-0.5)*2.0 + 0.1 = -0.4\n",
    "2. **Sigmoid**: f(z) = 1 / (1 + exp(-z)) = 1 / (1 + exp(0.4)) = 0.4013\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# PyTorch로 계산\n",
    "import torch\n",
    "x_t = torch.tensor([1.0, 2.0])\n",
    "w_t = torch.tensor([0.5, -0.5])\n",
    "b_t = torch.tensor(0.1)\n",
    "output_t = torch.sigmoid(torch.dot(w_t, x_t) + b_t)\n",
    "```\n",
    "\n",
    "**흔한 실수**: 가중합 계산 시 순서 혼동 (w @ x가 아닌 x @ w로 계산)\n",
    "\n",
    "**실무 팁**: Sigmoid 출력이 0.5보다 크면 클래스 1, 작으면 클래스 0으로 분류합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q2. 활성화 함수 선택하기 \n",
    "\n",
    "**문제**: 다음 상황에 적합한 활성화 함수를 선택하고, 이유를 설명하세요.\n",
    "\n",
    "1. 은닉층 (일반적인 경우)\n",
    "2. 이진 분류의 출력층 (0~1 확률)\n",
    "3. 다중 분류의 출력층 (여러 클래스 중 하나 선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 정답\n",
    "print(\"활성화 함수 선택 가이드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "answers = {\n",
    "    \"1. 은닉층 (일반적인 경우)\": {\n",
    "        \"활성화 함수\": \"ReLU (Rectified Linear Unit)\",\n",
    "        \"이유\": [\n",
    "            \"계산이 빠름 (max(0, x))\",\n",
    "            \"기울기 소실 문제 완화 (양수 영역에서 기울기 = 1)\",\n",
    "            \"희소 활성화로 효율적 학습\",\n",
    "            \"딥러닝의 사실상 표준\"\n",
    "        ],\n",
    "        \"PyTorch\": \"nn.ReLU()\"\n",
    "    },\n",
    "    \"2. 이진 분류 출력층\": {\n",
    "        \"활성화 함수\": \"Sigmoid\",\n",
    "        \"이유\": [\n",
    "            \"출력 범위가 (0, 1)로 확률 해석 가능\",\n",
    "            \"이진 분류에서 클래스 1의 확률 P(y=1|x)\",\n",
    "            \"임계값 0.5로 분류 결정\"\n",
    "        ],\n",
    "        \"PyTorch\": \"nn.Sigmoid() + nn.BCELoss()\"\n",
    "    },\n",
    "    \"3. 다중 분류 출력층\": {\n",
    "        \"활성화 함수\": \"Softmax\",\n",
    "        \"이유\": [\n",
    "            \"모든 출력의 합이 1 (확률 분포)\",\n",
    "            \"각 클래스의 확률 P(y=k|x)\",\n",
    "            \"가장 높은 확률의 클래스 선택\"\n",
    "        ],\n",
    "        \"PyTorch\": \"nn.CrossEntropyLoss() (Softmax 포함)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for situation, info in answers.items():\n",
    "    print(f\"\\n{situation}\")\n",
    "    print(f\"  정답: {info['활성화 함수']}\")\n",
    "    print(f\"  이유:\")\n",
    "    for reason in info['이유']:\n",
    "        print(f\"    - {reason}\")\n",
    "    print(f\"  PyTorch: {info['PyTorch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용 예시\n",
    "import torch.nn as nn\n",
    "\n",
    "# 이진 분류 모델\n",
    "binary_classifier = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),          # 은닉층: ReLU\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()        # 출력층: Sigmoid\n",
    ")\n",
    "\n",
    "# 다중 분류 모델\n",
    "multi_classifier = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),          # 은닉층: ReLU\n",
    "    nn.Linear(32, 5)    # 출력층: Softmax는 CrossEntropyLoss에 포함\n",
    ")\n",
    "\n",
    "print(\"이진 분류 모델:\")\n",
    "print(binary_classifier)\n",
    "print(\"\\n다중 분류 모델:\")\n",
    "print(multi_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 각 상황의 요구사항에 맞는 활성화 함수의 특성을 매칭합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- **ReLU**: max(0, x), 은닉층 기본\n",
    "- **Sigmoid**: 1/(1+e^-x), 0~1 확률\n",
    "- **Softmax**: 다중 클래스 확률 분포\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 은닉층 대안\n",
    "nn.LeakyReLU(0.1)  # dying ReLU 방지\n",
    "nn.GELU()          # Transformer에서 사용\n",
    "nn.Tanh()          # RNN 은닉층\n",
    "```\n",
    "\n",
    "**흔한 실수**: CrossEntropyLoss는 내부에 Softmax가 포함되어 있어, 모델 출력에 Softmax를 추가하면 중복 적용됩니다.\n",
    "\n",
    "**실무 팁**: ReLU 변형 중 GELU는 최근 Transformer 기반 모델에서 많이 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q3. 손실 함수 계산하기 \n",
    "\n",
    "**문제**: 다음 예측과 실제 값으로 BCE Loss를 수동 계산하세요.\n",
    "\n",
    "- 예측 확률: y_pred = [0.9, 0.3, 0.8]\n",
    "- 실제 레이블: y_true = [1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 정답\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.array([0.9, 0.3, 0.8])\n",
    "y_true = np.array([1, 0, 1])\n",
    "\n",
    "# BCE Loss 수식: -[y*log(p) + (1-y)*log(1-p)]\n",
    "def bce_loss_manual(y_pred, y_true):\n",
    "    epsilon = 1e-15  # log(0) 방지\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    losses = []\n",
    "    for p, y in zip(y_pred, y_true):\n",
    "        loss = -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "        losses.append(loss)\n",
    "        print(f\"  y={y}, p={p:.1f}: -{y}*log({p:.1f}) - {1-y}*log({1-p:.1f}) = {loss:.4f}\")\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "print(\"BCE Loss 계산 과정:\")\n",
    "bce = bce_loss_manual(y_pred, y_true)\n",
    "print(f\"\\n평균 BCE Loss: {bce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch로 검증\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y_pred_t = torch.tensor([0.9, 0.3, 0.8])\n",
    "y_true_t = torch.tensor([1.0, 0.0, 1.0])\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "pytorch_bce = bce_loss(y_pred_t, y_true_t)\n",
    "\n",
    "print(f\"수동 계산: {bce:.4f}\")\n",
    "print(f\"PyTorch 계산: {pytorch_bce.item():.4f}\")\n",
    "\n",
    "# 테스트\n",
    "assert abs(bce - pytorch_bce.item()) < 1e-4, \"BCE 계산 오류\"\n",
    "print(\"\\n모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: BCE Loss 공식을 각 샘플에 적용하고 평균을 구합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- BCE Loss = -[y*log(p) + (1-y)*log(1-p)]\n",
    "- y=1일 때: -log(p), 확률이 높을수록 손실 작음\n",
    "- y=0일 때: -log(1-p), 확률이 낮을수록 손실 작음\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 벡터화 계산\n",
    "bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "```\n",
    "\n",
    "**흔한 실수**: log(0)은 -infinity이므로 epsilon으로 클리핑해야 합니다.\n",
    "\n",
    "**실무 팁**: PyTorch의 BCEWithLogitsLoss는 Sigmoid + BCE를 합쳐서 수치적으로 더 안정적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q4. 경사 하강법 구현하기 \n",
    "\n",
    "**문제**: 함수 f(x) = (x - 3)^2의 최솟값을 경사 하강법으로 찾으세요.\n",
    "\n",
    "- 시작점: x = 10\n",
    "- 학습률: 0.1\n",
    "- 반복 횟수: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 정답\n",
    "import numpy as np\n",
    "\n",
    "# 함수 정의\n",
    "def f(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "# 미분: df/dx = 2(x - 3)\n",
    "def gradient(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# 경사 하강법\n",
    "x = 10.0  # 시작점\n",
    "lr = 0.1  # 학습률\n",
    "iterations = 50\n",
    "\n",
    "x_history = [x]\n",
    "loss_history = [f(x)]\n",
    "\n",
    "print(\"경사 하강법 실행\")\n",
    "print(\"=\"*50)\n",
    "print(f\"시작점: x = {x}, f(x) = {f(x)}\")\n",
    "print(f\"목표: x = 3 (최솟값)\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    grad = gradient(x)\n",
    "    x = x - lr * grad  # 경사 하강\n",
    "    \n",
    "    x_history.append(x)\n",
    "    loss_history.append(f(x))\n",
    "    \n",
    "    if i < 5 or i == iterations - 1:\n",
    "        print(f\"Iteration {i+1:2d}: x = {x:.6f}, f(x) = {f(x):.6f}, grad = {grad:.4f}\")\n",
    "    elif i == 5:\n",
    "        print(\"  ...\")\n",
    "\n",
    "print(f\"\\n최종 결과: x = {x:.6f} (목표: 3.0)\")\n",
    "print(f\"최솟값: f(x) = {f(x):.10f} (목표: 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "x_range = np.linspace(-2, 12, 100)\n",
    "y_range = f(x_range)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# 함수 곡선\n",
    "fig.add_trace(go.Scatter(x=x_range, y=y_range, mode='lines', name='f(x) = (x-3)^2'))\n",
    "\n",
    "# 경사 하강 경로\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_history, y=loss_history, mode='markers+lines',\n",
    "    marker=dict(size=8, color='red'),\n",
    "    name='경사 하강 경로'\n",
    "))\n",
    "\n",
    "# 최솟값 표시\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[3], y=[0], mode='markers',\n",
    "    marker=dict(size=15, color='green', symbol='star'),\n",
    "    name='최솟값 (x=3)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='경사 하강법: f(x) = (x-3)^2의 최솟값 찾기',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert abs(x - 3.0) < 0.01, f\"x가 3에 수렴하지 않음: {x}\"\n",
    "assert abs(f(x) - 0.0) < 0.0001, f\"최솟값에 도달하지 않음: {f(x)}\"\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 경사 하강법 공식 x_new = x_old - lr * gradient를 반복 적용합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- 함수: f(x) = (x - 3)^2\n",
    "- 미분: df/dx = 2(x - 3)\n",
    "- 최솟값: x = 3에서 f(x) = 0\n",
    "- 업데이트: x = x - 0.1 * 2(x - 3) = x - 0.2(x - 3) = 0.8x + 0.6\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# PyTorch Autograd 사용\n",
    "x = torch.tensor([10.0], requires_grad=True)\n",
    "for _ in range(50):\n",
    "    loss = (x - 3) ** 2\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= 0.1 * x.grad\n",
    "        x.grad.zero_()\n",
    "```\n",
    "\n",
    "**흔한 실수**: 학습률이 너무 크면 발산, 너무 작으면 수렴이 느립니다.\n",
    "\n",
    "**실무 팁**: 실제 딥러닝에서는 학습률 스케줄러를 사용하여 학습 중 학습률을 조절합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q5. XOR MLP 수정하기 \n",
    "\n",
    "**문제**: XOR 문제를 해결하는 MLP를 은닉층 2개로 수정하세요.\n",
    "\n",
    "- 구조: 입력(2) -> 은닉(8) -> 은닉(4) -> 출력(1)\n",
    "- 활성화: ReLU (은닉), Sigmoid (출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR 데이터\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 2-은닉층 MLP\n",
    "class XOR_MLP_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 8),    # 입력(2) -> 은닉1(8)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),    # 은닉1(8) -> 은닉2(4)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),    # 은닉2(4) -> 출력(1)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "model_v2 = XOR_MLP_v2()\n",
    "print(\"XOR MLP v2 (2-은닉층):\")\n",
    "print(model_v2)\n",
    "print(f\"\\n총 파라미터 수: {sum(p.numel() for p in model_v2.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_v2.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model_v2(X_xor)\n",
    "    loss = criterion(y_pred, y_xor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# 최종 예측\n",
    "print(\"\\n===== XOR 학습 완료 =====\")\n",
    "with torch.no_grad():\n",
    "    predictions = model_v2(X_xor)\n",
    "    for i in range(4):\n",
    "        x = X_xor[i].numpy()\n",
    "        pred = predictions[i].item()\n",
    "        actual = y_xor[i].item()\n",
    "        pred_class = 1 if pred > 0.5 else 0\n",
    "        correct = \"O\" if pred_class == int(actual) else \"X\"\n",
    "        print(f\"XOR({int(x[0])}, {int(x[1])}) = {pred:.3f} -> {pred_class} (실제: {int(actual)}) [{correct}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "with torch.no_grad():\n",
    "    preds = model_v2(X_xor)\n",
    "    pred_classes = (preds > 0.5).float()\n",
    "    accuracy = (pred_classes == y_xor).float().mean()\n",
    "    \n",
    "assert accuracy == 1.0, f\"XOR 정확도 100%가 아님: {accuracy*100:.0f}%\"\n",
    "print(f\"\\nXOR 정확도: {accuracy*100:.0f}%\")\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: nn.Sequential로 2개의 은닉층을 가진 MLP를 구성합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- 구조: 2 -> 8 -> 4 -> 1\n",
    "- 은닉층: Linear + ReLU\n",
    "- 출력층: Linear + Sigmoid (이진 분류)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 클래스 버전\n",
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.sigmoid(self.fc3(x))\n",
    "```\n",
    "\n",
    "**흔한 실수**: XOR 문제는 작은 데이터셋이라 빠르게 과적합됩니다. 실제 문제에서는 검증 데이터로 확인하세요.\n",
    "\n",
    "**실무 팁**: XOR 문제는 신경망이 비선형 패턴을 학습할 수 있는지 테스트하는 좋은 sanity check입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q6. 역전파 그래디언트 확인하기 \n",
    "\n",
    "**문제**: y = 2x^3 - 3x^2 + x 함수에서 x=2일 때 dy/dx를 Autograd로 계산하고, 수동 계산과 비교하세요.\n",
    "\n",
    "힌트: dy/dx = 6x^2 - 6x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 정답\n",
    "import torch\n",
    "\n",
    "# x=2에서 그래디언트 계산\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 함수: y = 2x^3 - 3x^2 + x\n",
    "y = 2 * x**3 - 3 * x**2 + x\n",
    "\n",
    "# 역전파\n",
    "y.backward()\n",
    "\n",
    "# 수동 계산: dy/dx = 6x^2 - 6x + 1\n",
    "# x=2: 6*4 - 6*2 + 1 = 24 - 12 + 1 = 13\n",
    "manual_grad = 6 * (2.0)**2 - 6 * (2.0) + 1\n",
    "\n",
    "print(\"함수: y = 2x^3 - 3x^2 + x\")\n",
    "print(\"미분: dy/dx = 6x^2 - 6x + 1\")\n",
    "print(f\"\\nx = {x.item()}\")\n",
    "print(f\"y = 2*{x.item()}^3 - 3*{x.item()}^2 + {x.item()} = {y.item()}\")\n",
    "print(f\"\\nAutograd dy/dx: {x.grad.item()}\")\n",
    "print(f\"수동 계산 dy/dx: 6*{x.item()}^2 - 6*{x.item()} + 1 = {manual_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert abs(x.grad.item() - 13.0) < 1e-6, f\"그래디언트 오류: {x.grad.item()}\"\n",
    "assert abs(x.grad.item() - manual_grad) < 1e-6, \"Autograd와 수동 계산 불일치\"\n",
    "print(\"\\n모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: PyTorch의 Autograd로 자동 미분하고, 수동 계산과 비교합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- 원 함수: y = 2x^3 - 3x^2 + x\n",
    "- 미분: dy/dx = 6x^2 - 6x + 1 (멱함수 미분 규칙)\n",
    "- x=2 대입: 6(4) - 6(2) + 1 = 24 - 12 + 1 = 13\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# torch.autograd.grad 직접 사용\n",
    "grad = torch.autograd.grad(y, x)\n",
    "print(grad[0].item())\n",
    "```\n",
    "\n",
    "**흔한 실수**: backward()는 스칼라에 대해서만 호출 가능합니다. 벡터 출력이면 gradient 인자가 필요합니다.\n",
    "\n",
    "**실무 팁**: Autograd 결과를 수동 계산과 비교하는 것은 커스텀 레이어 구현 시 디버깅에 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q7. 옵티마이저 선택하기 \n",
    "\n",
    "**문제**: 아래 상황에 적합한 옵티마이저를 선택하고 이유를 설명하세요.\n",
    "\n",
    "1. 빠른 프로토타이핑이 필요한 경우\n",
    "2. 일반화 성능이 중요한 최종 모델\n",
    "3. 학습률 튜닝 시간이 부족한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 정답\n",
    "print(\"옵티마이저 선택 가이드\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimizer_guide = {\n",
    "    \"1. 빠른 프로토타이핑\": {\n",
    "        \"추천\": \"Adam\",\n",
    "        \"이유\": [\n",
    "            \"빠른 수렴 속도\",\n",
    "            \"기본 lr=0.001로 대부분 잘 동작\",\n",
    "            \"하이퍼파라미터 튜닝 최소화\",\n",
    "            \"연구 단계에서 빠른 실험 가능\"\n",
    "        ],\n",
    "        \"PyTorch\": \"optim.Adam(model.parameters(), lr=0.001)\"\n",
    "    },\n",
    "    \"2. 일반화 성능 중요\": {\n",
    "        \"추천\": \"SGD + Momentum + Weight Decay\",\n",
    "        \"이유\": [\n",
    "            \"Adam보다 일반화 성능이 좋은 경우가 많음\",\n",
    "            \"ImageNet 등 대규모 벤치마크에서 검증됨\",\n",
    "            \"학습률 스케줄링과 함께 사용\",\n",
    "            \"최종 프로덕션 모델에 권장\"\n",
    "        ],\n",
    "        \"PyTorch\": \"optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\"\n",
    "    },\n",
    "    \"3. 학습률 튜닝 시간 부족\": {\n",
    "        \"추천\": \"Adam 또는 AdamW\",\n",
    "        \"이유\": [\n",
    "            \"적응적 학습률로 lr에 덜 민감\",\n",
    "            \"기본값으로도 안정적 학습\",\n",
    "            \"AdamW는 weight decay 개선 버전\",\n",
    "            \"Transformer 계열 모델에서 표준\"\n",
    "        ],\n",
    "        \"PyTorch\": \"optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for situation, info in optimizer_guide.items():\n",
    "    print(f\"\\n{situation}\")\n",
    "    print(f\"  추천: {info['추천']}\")\n",
    "    print(f\"  이유:\")\n",
    "    for reason in info['이유']:\n",
    "        print(f\"    - {reason}\")\n",
    "    print(f\"  PyTorch: {info['PyTorch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 사용 예시\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 예시 모델\n",
    "model = nn.Linear(10, 2)\n",
    "\n",
    "# 상황별 옵티마이저 설정\n",
    "opt_prototype = optim.Adam(model.parameters(), lr=0.001)\n",
    "opt_production = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "opt_quick = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(\"옵티마이저 설정 완료:\")\n",
    "print(f\"  프로토타입: {type(opt_prototype).__name__}\")\n",
    "print(f\"  프로덕션: {type(opt_production).__name__}\")\n",
    "print(f\"  빠른 개발: {type(opt_quick).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 각 상황의 우선순위(속도/일반화/편의성)에 맞는 옵티마이저를 선택합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- **Adam**: 빠른 수렴, 적응적 학습률\n",
    "- **SGD+Momentum**: 느리지만 일반화 좋음\n",
    "- **AdamW**: Adam + 개선된 weight decay\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 학습률 스케줄러와 함께 사용\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# 또는 OneCycleLR (빠른 학습)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, epochs=100, steps_per_epoch=len(train_loader))\n",
    "```\n",
    "\n",
    "**흔한 실수**: Adam을 사용할 때 weight_decay를 L2 정규화처럼 적용하면 문제가 있음. AdamW 권장.\n",
    "\n",
    "**실무 팁**: 최근 트렌드는 AdamW + Cosine Annealing 또는 WarmupCosine 조합입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q8. MLP 구조 설계하기 \n",
    "\n",
    "**문제**: Fashion MNIST (28x28 이미지, 10개 클래스) 분류를 위한 MLP를 설계하세요.\n",
    "\n",
    "요구사항:\n",
    "- 은닉층 3개 (256, 128, 64)\n",
    "- 드롭아웃 0.3\n",
    "- BatchNorm 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FashionMNIST_MLP(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # 입력층 -> 은닉층1\n",
    "            nn.Linear(784, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 은닉층1 -> 은닉층2\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 은닉층2 -> 은닉층3\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 은닉층3 -> 출력층\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # (batch, 1, 28, 28) -> (batch, 784)\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = FashionMNIST_MLP(dropout=0.3)\n",
    "print(\"Fashion MNIST MLP (3-은닉층 + BatchNorm + Dropout):\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 수 확인\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n총 파라미터 수: {count_parameters(model):,}\")\n",
    "\n",
    "# 레이어별 파라미터\n",
    "print(\"\\n레이어별 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.numel():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 (순전파)\n",
    "x_test = torch.randn(4, 1, 28, 28)  # 배치 4개\n",
    "model.eval()  # 평가 모드 (BatchNorm, Dropout 비활성화)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x_test)\n",
    "\n",
    "print(f\"\\n입력 shape: {x_test.shape}\")\n",
    "print(f\"출력 shape: {output.shape}\")\n",
    "assert output.shape == torch.Size([4, 10]), \"출력 shape 오류\"\n",
    "print(\"\\n모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 각 은닉층에 Linear -> BatchNorm -> ReLU -> Dropout 순서로 구성합니다.\n",
    "\n",
    "**핵심 개념**:\n",
    "- **BatchNorm**: 학습 안정화, 빠른 수렴\n",
    "- **Dropout**: 과적합 방지\n",
    "- **순서**: Linear -> BatchNorm -> ReLU -> Dropout (권장)\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 레이어 함수로 생성\n",
    "def make_layer(in_dim, out_dim, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.BatchNorm1d(out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "```\n",
    "\n",
    "**흔한 실수**: BatchNorm은 평가 시 다르게 동작하므로 model.eval() 필수입니다.\n",
    "\n",
    "**실무 팁**: BatchNorm과 Dropout을 함께 사용할 때 순서가 중요합니다. BatchNorm -> Dropout 순서가 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q9. MNIST 정확도 향상하기 \n",
    "\n",
    "**문제**: MNIST MLP 모델을 수정하여 테스트 정확도 98% 이상을 달성하세요.\n",
    "\n",
    "힌트:\n",
    "- 은닉층 크기 증가\n",
    "- 에포크 증가\n",
    "- 학습률 스케줄러 사용\n",
    "- 배치 정규화 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9 정답\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 로드\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 개선된 MLP\n",
    "class ImprovedMNIST_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model = nn.Sequential(\n",
    "            # 더 큰 은닉층\n",
    "            nn.Linear(784, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedMNIST_MLP().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# 학습 실행\n",
    "epochs = 15\n",
    "best_acc = 0\n",
    "\n",
    "print(\"개선된 MNIST MLP 학습\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print(f\"\\n최고 테스트 정확도: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "assert best_acc >= 98.0, f\"98% 정확도 미달: {best_acc:.2f}%\"\n",
    "print(f\"\\n목표 달성! 테스트 정확도: {best_acc:.2f}% >= 98%\")\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: 모델 크기 증가, BatchNorm, 학습률 스케줄러를 조합합니다.\n",
    "\n",
    "**핵심 개선사항**:\n",
    "1. **은닉층 크기 증가**: 128,64 -> 512,256,128\n",
    "2. **BatchNorm 추가**: 학습 안정화\n",
    "3. **학습률 스케줄러**: StepLR로 점진적 감소\n",
    "4. **에포크 증가**: 10 -> 15\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 데이터 증강 추가\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "```\n",
    "\n",
    "**흔한 실수**: 학습률 스케줄러 없이 높은 학습률 유지하면 후반에 진동합니다.\n",
    "\n",
    "**실무 팁**: MLP로 MNIST 98%+는 가능하지만, CNN을 사용하면 99%+ 쉽게 달성 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q10. 종합: 이진 분류 MLP 파이프라인 \n",
    "\n",
    "**문제**: sklearn의 breast cancer 데이터셋으로 이진 분류 MLP를 구축하세요.\n",
    "\n",
    "요구사항:\n",
    "1. 데이터 표준화 (StandardScaler)\n",
    "2. MLP 모델 (은닉층 2개)\n",
    "3. Adam 옵티마이저, BCE Loss\n",
    "4. 학습 곡선 시각화\n",
    "5. 테스트 정확도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10 정답\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"훈련 데이터: {X_train_t.shape}\")\n",
    "print(f\"테스트 데이터: {X_test_t.shape}\")\n",
    "print(f\"특성 수: {X_train_t.shape[1]}\")\n",
    "print(f\"양성 비율 (훈련): {y_train.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MLP 모델 정의\n",
    "class BreastCancerMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model = BreastCancerMLP(input_dim=30)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Breast Cancer MLP:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 학습\n",
    "epochs = 100\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 훈련\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # 평가\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 훈련 정확도\n",
    "        train_preds = (model(X_train_t) > 0.5).float()\n",
    "        train_acc = (train_preds == y_train_t).float().mean().item()\n",
    "        \n",
    "        # 테스트 정확도\n",
    "        test_outputs = model(X_test_t)\n",
    "        test_loss = criterion(test_outputs, y_test_t).item()\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_acc = (test_preds == y_test_t).float().mean().item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    train_accs.append(train_acc * 100)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc * 100)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n최종 테스트 정확도: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 곡선 시각화\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['손실 (Loss)', '정확도 (Accuracy)'])\n",
    "\n",
    "fig.add_trace(go.Scatter(y=train_losses, mode='lines', name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=test_losses, mode='lines', name='Test Loss'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=train_accs, mode='lines', name='Train Acc'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=test_accs, mode='lines', name='Test Acc'), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Epoch')\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Accuracy (%)', row=1, col=2)\n",
    "fig.update_layout(title='Breast Cancer MLP 학습 곡선', height=400, template='plotly_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 최종 평가\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = (model(X_test_t) > 0.5).numpy().astype(int).flatten()\n",
    "\n",
    "print(\"분류 리포트:\")\n",
    "print(classification_report(y_test, test_preds, target_names=['악성', '양성']))\n",
    "\n",
    "# 테스트\n",
    "final_acc = (test_preds == y_test).mean() * 100\n",
    "assert final_acc >= 90, f\"정확도 90% 미달: {final_acc:.2f}%\"\n",
    "print(f\"\\n최종 정확도: {final_acc:.2f}%\")\n",
    "print(\"모든 테스트 통과!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀이 설명\n",
    "\n",
    "**접근 방법**: End-to-End 파이프라인을 구축합니다.\n",
    "\n",
    "**핵심 단계**:\n",
    "1. **데이터 전처리**: StandardScaler로 표준화\n",
    "2. **모델 설계**: 30 -> 64 -> 32 -> 1 MLP\n",
    "3. **학습**: BCE Loss + Adam, 100 에포크\n",
    "4. **시각화**: Plotly로 학습 곡선\n",
    "5. **평가**: classification_report로 상세 분석\n",
    "\n",
    "**대안 솔루션**:\n",
    "```python\n",
    "# 조기 종료 추가\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ...\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "```\n",
    "\n",
    "**흔한 실수**: 테스트 데이터에 fit_transform 적용하면 데이터 누수입니다. transform만 사용하세요.\n",
    "\n",
    "**실무 팁**: 의료 데이터에서는 Recall(재현율)이 중요합니다. 암을 놓치지 않는 것이 오진보다 중요하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: 기초 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 실무 활용 |\n",
    "|-----|----------|----------|\n",
    "| 퍼셉트론 | y = f(W @ X + b) | 신경망의 기본 단위 |\n",
    "| 활성화 함수 | 비선형성 추가 | ReLU (은닉), Sigmoid/Softmax (출력) |\n",
    "| 손실 함수 | 예측 오차 측정 | MSE (회귀), CE (분류) |\n",
    "| 경사 하강법 | w = w - lr * gradient | 손실 최소화 |\n",
    "| XOR 문제 | 단일 퍼셉트론의 한계 | MLP로 해결 |\n",
    "\n",
    "### Part 2: 심화 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 언제 사용? |\n",
    "|-----|----------|----------|\n",
    "| 역전파 | 체인 룰로 그래디언트 계산 | 모든 신경망 학습 |\n",
    "| MLP | 입력 -> 은닉층들 -> 출력 | 정형 데이터 분류/회귀 |\n",
    "| SGD vs Adam | 기본 vs 적응적 학습률 | Adam이 기본, SGD는 일반화 |\n",
    "| MNIST | 손글씨 분류 벤치마크 | 딥러닝 입문 |\n",
    "\n",
    "### MLP 학습 핵심 패턴\n",
    "\n",
    "```python\n",
    "model = MLP(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # 분류\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        outputs = model(X_batch)          # 순전파\n",
    "        loss = criterion(outputs, y_batch) # 손실 계산\n",
    "        \n",
    "        optimizer.zero_grad()   # 그래디언트 초기화\n",
    "        loss.backward()         # 역전파\n",
    "        optimizer.step()        # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **활성화 함수**: 은닉층에는 ReLU, 출력층은 문제에 맞게 (Sigmoid, Softmax, 없음)\n",
    "2. **학습률**: Adam은 0.001, SGD는 0.01~0.1로 시작\n",
    "3. **과적합 방지**: Dropout, 조기 종료, 데이터 증강\n",
    "4. **배치 크기**: 32~128이 일반적, 메모리와 속도 트레이드오프\n",
    "5. **디버깅**: 먼저 작은 데이터로 과적합 확인 (모델이 학습 가능한지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
