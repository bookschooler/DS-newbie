{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day14_1: 차원 축소 (Dimensionality Reduction)\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "**Part 1: PCA (주성분 분석)**\n",
    "1. 차원의 저주 개념 이해하기\n",
    "2. PCA의 원리와 수학적 배경 이해하기\n",
    "3. 설명된 분산(explained variance) 해석하기\n",
    "4. 최적 주성분 개수 선택하기\n",
    "5. Plotly로 2D/3D 시각화하기\n",
    "\n",
    "**Part 2: t-SNE와 특성 선택**\n",
    "1. t-SNE로 고차원 데이터 시각화하기\n",
    "2. perplexity 파라미터 이해하기\n",
    "3. SelectKBest로 통계적 특성 선택하기\n",
    "4. RFE로 재귀적 특성 제거하기\n",
    "5. Feature Importance 비교하기\n",
    "\n",
    "---\n",
    "\n",
    "## 왜 이것을 배우나요?\n",
    "\n",
    "| 개념 | 실무 활용 | 예시 |\n",
    "|------|----------|------|\n",
    "| 차원 축소 | 고차원 데이터 압축 | 이미지 압축, 노이즈 제거 |\n",
    "| PCA | 특성 추출, 전처리 | 얼굴 인식, 센서 데이터 분석 |\n",
    "| t-SNE | 데이터 시각화 | 군집 탐색, 이상치 발견 |\n",
    "| 특성 선택 | 모델 성능 향상 | 불필요한 변수 제거, 과적합 방지 |\n",
    "\n",
    "**분석가 관점**: 100개의 특성을 가진 데이터를 2개로 줄여 시각화하고, 핵심 특성만 선택해 모델 성능을 높입니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 및 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: PCA (주성분 분석)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 차원의 저주 (Curse of Dimensionality)\n",
    "\n",
    "### 차원의 저주란?\n",
    "\n",
    "차원(특성 수)이 증가하면 발생하는 문제들:\n",
    "\n",
    "1. **데이터 희소성**: 고차원 공간에서 데이터 포인트들이 멀어짐\n",
    "2. **거리 의미 상실**: 모든 점들의 거리가 비슷해짐\n",
    "3. **과적합 위험**: 특성 수 > 샘플 수이면 과적합\n",
    "4. **계산 비용 증가**: 차원에 따라 기하급수적 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원의 저주 시각화: 차원별 최소-최대 거리 비율\n",
    "np.random.seed(42)\n",
    "\n",
    "dimensions = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "n_samples = 100\n",
    "distance_ratios = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    # 고차원 데이터 생성\n",
    "    data = np.random.randn(n_samples, dim)\n",
    "    \n",
    "    # 모든 쌍의 거리 계산\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    dists = euclidean_distances(data).flatten()\n",
    "    dists = dists[dists > 0]  # 자기 자신 제외\n",
    "    \n",
    "    # 최대-최소 거리 비율\n",
    "    ratio = (dists.max() - dists.min()) / dists.min()\n",
    "    distance_ratios.append(ratio)\n",
    "\n",
    "# 시각화\n",
    "fig = px.line(\n",
    "    x=dimensions, \n",
    "    y=distance_ratios,\n",
    "    markers=True,\n",
    "    title='차원의 저주: 차원 증가에 따른 거리 비율 감소',\n",
    "    labels={'x': '차원 수', 'y': '(최대거리-최소거리)/최소거리'}\n",
    ")\n",
    "fig.add_annotation(\n",
    "    x=200, y=distance_ratios[dimensions.index(200)],\n",
    "    text=\"거리 비율 감소 = 거리 구분 어려움\",\n",
    "    showarrow=True, arrowhead=2\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: 언제 차원 축소가 필요한가?\n",
    "\n",
    "| 상황 | 특성 수 | 샘플 수 | 문제 | 해결책 |\n",
    "|------|---------|---------|------|--------|\n",
    "| 텍스트 분석 | 10,000+ (단어) | 1,000 | 희소 행렬 | TF-IDF + SVD |\n",
    "| 이미지 분류 | 784 (28x28 픽셀) | 10,000 | 계산 비용 | PCA 압축 |\n",
    "| 센서 데이터 | 500 (센서) | 5,000 | 다중공선성 | PCA/특성 선택 |\n",
    "| 유전자 분석 | 20,000+ (유전자) | 200 | 과적합 | 특성 선택 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PCA 개념과 원리\n",
    "\n",
    "### PCA(Principal Component Analysis)란?\n",
    "\n",
    "**주성분 분석**: 데이터의 분산을 최대화하는 방향(축)을 찾아 차원을 축소\n",
    "\n",
    "**핵심 아이디어**:\n",
    "1. 데이터를 가장 잘 설명하는 축(PC1) 찾기\n",
    "2. PC1과 직교하며 다음으로 분산이 큰 축(PC2) 찾기\n",
    "3. 이 과정을 반복하여 주성분들을 생성\n",
    "\n",
    "**수학적 과정**:\n",
    "1. 데이터 표준화 (평균 0, 분산 1)\n",
    "2. 공분산 행렬 계산\n",
    "3. 고유값/고유벡터 분해\n",
    "4. 상위 k개 고유벡터 선택\n",
    "5. 데이터를 새 축으로 투영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터로 PCA 시연\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris 데이터 형태:\")\n",
    "print(f\"  특성 수: {X.shape[1]}개\")\n",
    "print(f\"  샘플 수: {X.shape[0]}개\")\n",
    "print(f\"  특성명: {feature_names}\")\n",
    "print(f\"  클래스: {target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 데이터 표준화 (필수!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"표준화 전 평균:\", X.mean(axis=0).round(2))\n",
    "print(\"표준화 후 평균:\", X_scaled.mean(axis=0).round(4))\n",
    "print(\"표준화 후 표준편차:\", X_scaled.std(axis=0).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: PCA 적용\n",
    "pca = PCA(n_components=2)  # 2개의 주성분으로 축소\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"PCA 결과 형태:\", X_pca.shape)\n",
    "print(\"\\n설명된 분산 비율:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "print(f\"\\n누적 설명 분산: {sum(pca.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: 설명된 분산(Explained Variance) 해석\n",
    "\n",
    "- **explained_variance_ratio_**: 각 주성분이 원본 데이터의 분산을 얼마나 설명하는지\n",
    "- PC1이 72.96%를 설명 = 데이터 변동의 73%가 이 축으로 표현됨\n",
    "- PC1 + PC2 = 95.81% = 4차원 데이터를 2차원으로 압축해도 95%의 정보 유지!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PCA 시각화 (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 결과를 DataFrame으로\n",
    "df_pca = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'Species': [target_names[i] for i in y]\n",
    "})\n",
    "\n",
    "# 2D 산점도\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2', color='Species',\n",
    "    title=f'Iris PCA 시각화 (설명 분산: {sum(pca.explained_variance_ratio_)*100:.1f}%)',\n",
    "    labels={'PC1': f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "            'PC2': f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA 시각화\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "df_pca_3d = pd.DataFrame({\n",
    "    'PC1': X_pca_3d[:, 0],\n",
    "    'PC2': X_pca_3d[:, 1],\n",
    "    'PC3': X_pca_3d[:, 2],\n",
    "    'Species': [target_names[i] for i in y]\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_pca_3d, x='PC1', y='PC2', z='PC3', color='Species',\n",
    "    title=f'Iris 3D PCA (설명 분산: {sum(pca_3d.explained_variance_ratio_)*100:.1f}%)',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 주성분 개수 선택 (Scree Plot)\n",
    "\n",
    "### Scree Plot (스크리 플롯)\n",
    "\n",
    "- 각 주성분의 설명 분산을 시각화\n",
    "- **엘보우 포인트**: 기울기가 급격히 감소하는 지점\n",
    "- **누적 90% 기준**: 누적 설명 분산이 90%에 도달하는 지점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 주성분에 대해 PCA 수행\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Scree Plot 데이터\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# 시각화\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# 개별 분산 (막대)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f'PC{i+1}' for i in range(len(explained_variance))],\n",
    "        y=explained_variance * 100,\n",
    "        name='개별 설명 분산',\n",
    "        marker_color='steelblue'\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "# 누적 분산 (선)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[f'PC{i+1}' for i in range(len(cumulative_variance))],\n",
    "        y=cumulative_variance * 100,\n",
    "        name='누적 설명 분산',\n",
    "        mode='lines+markers',\n",
    "        marker_color='coral',\n",
    "        line=dict(width=3)\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "# 90% 기준선\n",
    "fig.add_hline(y=90, line_dash=\"dash\", line_color=\"green\", \n",
    "              annotation_text=\"90% 기준\", secondary_y=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Scree Plot: 주성분별 설명 분산',\n",
    "    xaxis_title='주성분',\n",
    "    yaxis_title='개별 분산 (%)',\n",
    "    yaxis2_title='누적 분산 (%)',\n",
    "    legend=dict(x=0.7, y=0.5)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: PCA 주성분 개수 선택 기준\n",
    "\n",
    "1. **누적 분산 90~95%**: 대부분의 정보 보존\n",
    "2. **엘보우 규칙**: 기울기 급감 지점\n",
    "3. **Kaiser 규칙**: 고유값 > 1인 성분만 선택 (표준화 데이터)\n",
    "4. **교차 검증**: 다운스트림 태스크 성능 기준\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 PCA 성분 해석 (Loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 성분(로딩) 분석\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_names\n",
    ")\n",
    "print(\"PCA 로딩 (각 원본 특성이 주성분에 기여하는 정도):\")\n",
    "print(loadings.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로딩 시각화 (화살표)\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2', color='Species',\n",
    "    title='PCA Biplot: 주성분과 원본 특성의 관계',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "    opacity=0.5\n",
    ")\n",
    "\n",
    "# 로딩 화살표 추가\n",
    "scale = 3  # 화살표 크기 조정\n",
    "for feature, (pc1, pc2) in loadings.iterrows():\n",
    "    fig.add_annotation(\n",
    "        x=pc1 * scale, y=pc2 * scale,\n",
    "        ax=0, ay=0,\n",
    "        xref=\"x\", yref=\"y\",\n",
    "        axref=\"x\", ayref=\"y\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        arrowsize=1.5,\n",
    "        arrowwidth=2,\n",
    "        arrowcolor=\"red\"\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=pc1 * scale * 1.15,\n",
    "        y=pc2 * scale * 1.15,\n",
    "        text=feature.replace(' (cm)', ''),\n",
    "        showarrow=False,\n",
    "        font=dict(size=10, color=\"red\")\n",
    "    )\n",
    "\n",
    "fig.update_layout(xaxis=dict(range=[-4, 4]), yaxis=dict(range=[-4, 4]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: MNIST 손글씨 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 손글씨 데이터 로드 (작은 버전)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"MNIST 데이터: {X_digits.shape[0]}개 샘플, {X_digits.shape[1]}개 특성 (8x8 픽셀)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 및 PCA\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "pca_digits = PCA(n_components=50)  # 64 -> 50\n",
    "X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n",
    "\n",
    "# 누적 분산\n",
    "cumvar = np.cumsum(pca_digits.explained_variance_ratio_)\n",
    "n_90 = np.argmax(cumvar >= 0.90) + 1\n",
    "n_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "\n",
    "print(f\"90% 분산 설명: {n_90}개 성분 필요 (64 -> {n_90}, {(1-n_90/64)*100:.1f}% 압축)\")\n",
    "print(f\"95% 분산 설명: {n_95}개 성분 필요 (64 -> {n_95}, {(1-n_95/64)*100:.1f}% 압축)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 2D 시각화\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_digits_2d = pca_2d.fit_transform(X_digits_scaled)\n",
    "\n",
    "df_digits = pd.DataFrame({\n",
    "    'PC1': X_digits_2d[:, 0],\n",
    "    'PC2': X_digits_2d[:, 1],\n",
    "    'Digit': y_digits.astype(str)\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_digits, x='PC1', y='PC2', color='Digit',\n",
    "    title=f'MNIST 손글씨 PCA (설명 분산: {sum(pca_2d.explained_variance_ratio_)*100:.1f}%)',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    ")\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.6))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: t-SNE와 특성 선택\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "### t-SNE란?\n",
    "\n",
    "- **비선형 차원 축소 기법**: 고차원의 복잡한 구조를 저차원에서 보존\n",
    "- **주요 용도**: 시각화 전용 (2D/3D)\n",
    "- **PCA vs t-SNE**:\n",
    "  - PCA: 선형 변환, 빠름, 전역 구조 보존\n",
    "  - t-SNE: 비선형, 느림, 지역 구조 보존\n",
    "\n",
    "### perplexity 파라미터\n",
    "\n",
    "- **perplexity**: 각 점이 고려하는 이웃의 수 (보통 5~50)\n",
    "- 작은 값: 지역 구조에 집중, 작은 군집 강조\n",
    "- 큰 값: 전역 구조에 집중, 큰 군집 강조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE 적용 (Iris)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "df_tsne = pd.DataFrame({\n",
    "    'TSNE1': X_tsne[:, 0],\n",
    "    'TSNE2': X_tsne[:, 1],\n",
    "    'Species': [target_names[i] for i in y]\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_tsne, x='TSNE1', y='TSNE2', color='Species',\n",
    "    title='Iris t-SNE 시각화 (perplexity=30)',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity 비교\n",
    "perplexities = [5, 15, 30, 50]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[f'perplexity={p}' for p in perplexities]\n",
    ")\n",
    "\n",
    "colors = {'setosa': 'blue', 'versicolor': 'green', 'virginica': 'red'}\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    tsne_temp = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=500)\n",
    "    X_temp = tsne_temp.fit_transform(X_scaled)\n",
    "    \n",
    "    for species_name in target_names:\n",
    "        mask = [target_names[i] == species_name for i in y]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_temp[mask, 0],\n",
    "                y=X_temp[mask, 1],\n",
    "                mode='markers',\n",
    "                name=species_name,\n",
    "                marker=dict(color=colors[species_name], size=6, opacity=0.7),\n",
    "                showlegend=(idx == 0)\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='t-SNE perplexity 비교',\n",
    "    height=600,\n",
    "    legend=dict(x=1.02, y=0.5)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 예시: MNIST t-SNE 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST t-SNE (시간이 오래 걸리므로 샘플링)\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(len(X_digits), 500, replace=False)\n",
    "X_sample = X_digits_scaled[sample_idx]\n",
    "y_sample = y_digits[sample_idx]\n",
    "\n",
    "print(\"t-SNE 변환 중... (약 30초 소요)\")\n",
    "tsne_digits = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_digits_tsne = tsne_digits.fit_transform(X_sample)\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST t-SNE 시각화\n",
    "df_digits_tsne = pd.DataFrame({\n",
    "    'TSNE1': X_digits_tsne[:, 0],\n",
    "    'TSNE2': X_digits_tsne[:, 1],\n",
    "    'Digit': y_sample.astype(str)\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_digits_tsne, x='TSNE1', y='TSNE2', color='Digit',\n",
    "    title='MNIST t-SNE (500 samples)',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    ")\n",
    "fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA vs t-SNE 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA vs t-SNE 비교 시각화\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['PCA', 't-SNE']\n",
    ")\n",
    "\n",
    "# PCA\n",
    "pca_sample = PCA(n_components=2)\n",
    "X_sample_pca = pca_sample.fit_transform(X_sample)\n",
    "\n",
    "for digit in range(10):\n",
    "    mask = y_sample == digit\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_sample_pca[mask, 0],\n",
    "            y=X_sample_pca[mask, 1],\n",
    "            mode='markers',\n",
    "            name=str(digit),\n",
    "            marker=dict(size=5, opacity=0.6),\n",
    "            legendgroup=str(digit)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_digits_tsne[mask, 0],\n",
    "            y=X_digits_tsne[mask, 1],\n",
    "            mode='markers',\n",
    "            name=str(digit),\n",
    "            marker=dict(size=5, opacity=0.6),\n",
    "            legendgroup=str(digit),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='MNIST: PCA vs t-SNE 비교',\n",
    "    height=500,\n",
    "    legend=dict(x=1.02, y=0.5)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: PCA와 t-SNE 선택 기준\n",
    "\n",
    "| 기준 | PCA | t-SNE |\n",
    "|------|-----|-------|\n",
    "| 속도 | 빠름 | 느림 |\n",
    "| 용도 | 전처리, 특성 추출 | 시각화 전용 |\n",
    "| 해석성 | 높음 (로딩 해석 가능) | 낮음 |\n",
    "| 재현성 | 동일 결과 | 랜덤 시드 필요 |\n",
    "| 새 데이터 | transform() 가능 | 불가 (재학습 필요) |\n",
    "| 구조 보존 | 전역 구조 | 지역 구조 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 특성 선택 (Feature Selection)\n",
    "\n",
    "### 특성 선택 vs 차원 축소\n",
    "\n",
    "| 방법 | 특성 선택 | 차원 축소 (PCA) |\n",
    "|------|----------|----------------|\n",
    "| 원리 | 원본 특성 중 선택 | 새로운 특성 생성 |\n",
    "| 해석성 | 높음 | 낮음 |\n",
    "| 예시 | [x1, x3, x5] 선택 | PC1 = 0.5*x1 + 0.3*x2 + ... |\n",
    "\n",
    "### 특성 선택 방법\n",
    "\n",
    "1. **Filter 방법**: 통계적 테스트 (SelectKBest)\n",
    "2. **Wrapper 방법**: 모델 기반 반복 선택 (RFE)\n",
    "3. **Embedded 방법**: 모델 학습 중 선택 (Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine 데이터셋 사용 (더 많은 특성)\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "wine_features = wine.feature_names\n",
    "\n",
    "print(f\"Wine 데이터: {X_wine.shape[0]}개 샘플, {X_wine.shape[1]}개 특성\")\n",
    "print(f\"특성명: {wine_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 SelectKBest (Filter 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest: 상위 K개 특성 선택 (ANOVA F-test 기반)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_kbest = selector.fit_transform(X_wine, y_wine)\n",
    "\n",
    "# 선택된 특성\n",
    "selected_features = np.array(wine_features)[selector.get_support()]\n",
    "scores = selector.scores_\n",
    "\n",
    "print(\"SelectKBest 결과 (상위 5개):\")\n",
    "print(f\"  선택된 특성: {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 특성의 점수 시각화\n",
    "df_scores = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'Score': scores\n",
    "}).sort_values('Score', ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    df_scores, x='Score', y='Feature', orientation='h',\n",
    "    title='SelectKBest: F-Score (ANOVA)',\n",
    "    color='Score',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(categoryorder='total ascending'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 RFE (Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE: 재귀적 특성 제거\n",
    "# 기본 모델로 Logistic Regression 사용\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=model, n_features_to_select=5, step=1)\n",
    "X_rfe = rfe.fit_transform(X_wine, y_wine)\n",
    "\n",
    "# 선택된 특성\n",
    "rfe_features = np.array(wine_features)[rfe.support_]\n",
    "rfe_ranking = rfe.ranking_\n",
    "\n",
    "print(\"RFE 결과 (상위 5개):\")\n",
    "print(f\"  선택된 특성: {list(rfe_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE 순위 시각화\n",
    "df_rfe = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'Ranking': rfe_ranking\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "fig = px.bar(\n",
    "    df_rfe, x='Ranking', y='Feature', orientation='h',\n",
    "    title='RFE: 특성 순위 (1=선택됨)',\n",
    "    color='Ranking',\n",
    "    color_continuous_scale='Reds_r'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(categoryorder='total descending'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_wine, y_wine)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': wine_features,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    df_importance, x='Importance', y='Feature', orientation='h',\n",
    "    title='Random Forest: Feature Importance',\n",
    "    color='Importance',\n",
    "    color_continuous_scale='Greens'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(categoryorder='total ascending'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 특성 선택 방법 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 가지 방법 비교 (상위 5개)\n",
    "# SelectKBest 상위 5개\n",
    "kbest_top5 = df_scores.nlargest(5, 'Score')['Feature'].tolist()\n",
    "\n",
    "# RFE 상위 5개 (ranking=1)\n",
    "rfe_top5 = list(rfe_features)\n",
    "\n",
    "# RF Importance 상위 5개\n",
    "rf_top5 = df_importance.nlargest(5, 'Importance')['Feature'].tolist()\n",
    "\n",
    "print(\"특성 선택 방법 비교 (상위 5개):\")\n",
    "print(f\"  SelectKBest: {kbest_top5}\")\n",
    "print(f\"  RFE:         {rfe_top5}\")\n",
    "print(f\"  RF Importance: {rf_top5}\")\n",
    "\n",
    "# 공통 특성 찾기\n",
    "common = set(kbest_top5) & set(rfe_top5) & set(rf_top5)\n",
    "print(f\"\\n모든 방법에서 공통: {list(common)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 특성 선택 방법의 모델 성능 비교\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# 전체 특성\n",
    "score_all = cross_val_score(model, X_wine, y_wine, cv=5).mean()\n",
    "\n",
    "# SelectKBest 5개\n",
    "score_kbest = cross_val_score(model, X_kbest, y_wine, cv=5).mean()\n",
    "\n",
    "# RFE 5개\n",
    "score_rfe = cross_val_score(model, X_rfe, y_wine, cv=5).mean()\n",
    "\n",
    "# RF Importance 상위 5개\n",
    "rf_mask = [f in rf_top5 for f in wine_features]\n",
    "score_rf = cross_val_score(model, X_wine[:, rf_mask], y_wine, cv=5).mean()\n",
    "\n",
    "print(\"로지스틱 회귀 교차검증 정확도:\")\n",
    "print(f\"  전체 특성 (13개): {score_all:.4f}\")\n",
    "print(f\"  SelectKBest (5개): {score_kbest:.4f}\")\n",
    "print(f\"  RFE (5개): {score_rfe:.4f}\")\n",
    "print(f\"  RF Importance (5개): {score_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 시각화\n",
    "methods = ['전체 (13개)', 'SelectKBest (5개)', 'RFE (5개)', 'RF Importance (5개)']\n",
    "scores = [score_all, score_kbest, score_rfe, score_rf]\n",
    "\n",
    "fig = px.bar(\n",
    "    x=methods, y=scores,\n",
    "    title='특성 선택 방법별 분류 정확도 비교',\n",
    "    labels={'x': '방법', 'y': '정확도'},\n",
    "    color=scores,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_layout(yaxis=dict(range=[0.9, 1.0]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실무 팁: 특성 선택 전략\n",
    "\n",
    "1. **빠른 필터링**: SelectKBest로 초기 탐색\n",
    "2. **최종 선택**: RFE나 모델 기반 중요도로 정교화\n",
    "3. **앙상블 접근**: 여러 방법의 공통 특성 선택\n",
    "4. **도메인 지식**: 통계적 결과 + 비즈니스 맥락 고려\n",
    "5. **교차 검증**: 특성 수에 따른 성능 변화 확인\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 실습 퀴즈\n",
    "\n",
    "**난이도**: (쉬움) ~ (어려움)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 데이터 표준화하기 (난이도: 1/5)\n",
    "\n",
    "**문제**: Iris 데이터를 StandardScaler로 표준화하고, 표준화 후 평균과 표준편차를 출력하세요.\n",
    "\n",
    "**힌트**: `StandardScaler().fit_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. PCA 2차원 변환 (난이도: 2/5)\n",
    "\n",
    "**문제**: 표준화된 Iris 데이터를 PCA로 2차원으로 축소하고, 설명된 분산 비율을 출력하세요.\n",
    "\n",
    "**기대 결과**: 각 주성분의 설명 분산과 누적 설명 분산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Q1에서 표준화한 데이터 사용 (X_scaled)\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. PCA 시각화 (난이도: 2/5)\n",
    "\n",
    "**문제**: PCA 변환된 Iris 데이터를 px.scatter()로 시각화하세요. 색상은 품종(Species)으로 구분합니다.\n",
    "\n",
    "**힌트**: DataFrame으로 변환 후 px.scatter() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Q2의 PCA 결과 사용\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Scree Plot 그리기 (난이도: 3/5)\n",
    "\n",
    "**문제**: Wine 데이터에 대해 모든 주성분의 설명 분산을 막대그래프로, 누적 분산을 선그래프로 그리세요.\n",
    "\n",
    "**힌트**: `PCA()` (n_components 지정 안 함), `np.cumsum()`, `make_subplots()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. t-SNE 시각화 (난이도: 3/5)\n",
    "\n",
    "**문제**: Iris 데이터를 t-SNE로 2차원 변환 후 시각화하세요. perplexity=30 사용.\n",
    "\n",
    "**힌트**: `TSNE(n_components=2, perplexity=30, random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 표준화된 Iris 데이터 사용\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. perplexity 비교 (난이도: 3/5)\n",
    "\n",
    "**문제**: perplexity를 [5, 15, 30, 50]으로 변경하며 t-SNE 결과를 2x2 서브플롯으로 비교하세요.\n",
    "\n",
    "**힌트**: `make_subplots(rows=2, cols=2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [5, 15, 30, 50]\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. SelectKBest 적용 (난이도: 4/5)\n",
    "\n",
    "**문제**: Wine 데이터에서 SelectKBest로 상위 5개 특성을 선택하고, 각 특성의 F-score를 막대그래프로 시각화하세요.\n",
    "\n",
    "**힌트**: `SelectKBest(score_func=f_classif, k=5)`, `selector.scores_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "wine_features = wine.feature_names\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. RFE 적용 (난이도: 4/5)\n",
    "\n",
    "**문제**: Logistic Regression을 기반으로 RFE를 적용하여 상위 5개 특성을 선택하고, 특성별 순위를 막대그래프로 시각화하세요.\n",
    "\n",
    "**힌트**: `RFE(estimator=model, n_features_to_select=5)`, `rfe.ranking_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Feature Importance 비교 (난이도: 5/5)\n",
    "\n",
    "**문제**: Random Forest의 feature_importances_를 추출하고, SelectKBest, RFE와 함께 세 가지 방법의 상위 5개 특성을 비교하세요. 공통 특성을 출력하세요.\n",
    "\n",
    "**힌트**: 집합(set)의 교집합 `&` 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. 종합 문제: 차원 축소 파이프라인 (난이도: 5/5)\n",
    "\n",
    "**문제**: MNIST 데이터(digits)에 대해:\n",
    "1. StandardScaler로 표준화\n",
    "2. PCA로 90% 분산을 설명하는 주성분 수 찾기\n",
    "3. 해당 수로 차원 축소 후 Logistic Regression 정확도 측정\n",
    "4. 원본(64차원) vs 축소된 데이터의 정확도 비교\n",
    "\n",
    "**힌트**: `np.cumsum(pca.explained_variance_ratio_) >= 0.9`, `cross_val_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# 여기에 코드를 작성하세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 정리\n",
    "\n",
    "### Part 1: PCA 핵심 요약\n",
    "\n",
    "| 개념 | 핵심 내용 | 코드 |\n",
    "|------|----------|------|\n",
    "| 차원의 저주 | 고차원에서 거리 의미 상실, 과적합 위험 | - |\n",
    "| PCA 원리 | 분산 최대화 축 찾기, 직교 변환 | `PCA(n_components=k)` |\n",
    "| 설명 분산 | 각 PC가 원본 정보를 얼마나 보존 | `pca.explained_variance_ratio_` |\n",
    "| 주성분 선택 | 누적 90%+ 또는 엘보우 규칙 | `np.cumsum()` |\n",
    "| PCA 로딩 | 원본 특성과 PC의 관계 | `pca.components_` |\n",
    "\n",
    "### Part 2: t-SNE와 특성 선택 핵심 요약\n",
    "\n",
    "| 방법 | 특징 | 코드 |\n",
    "|------|------|------|\n",
    "| t-SNE | 비선형, 시각화 전용, 지역 구조 보존 | `TSNE(perplexity=30)` |\n",
    "| SelectKBest | 통계적 필터링 (F-test) | `SelectKBest(f_classif, k=5)` |\n",
    "| RFE | 모델 기반 재귀적 제거 | `RFE(estimator, n_features_to_select)` |\n",
    "| Feature Importance | 트리 기반 중요도 | `rf.feature_importances_` |\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "1. **PCA 전 반드시 표준화**: 스케일 차이가 크면 결과 왜곡\n",
    "2. **t-SNE는 시각화 전용**: 새 데이터 변환 불가, 재현성 위해 random_state 설정\n",
    "3. **특성 선택 앙상블**: 여러 방법의 공통 특성이 가장 신뢰도 높음\n",
    "4. **perplexity 튜닝**: 데이터 크기에 따라 조정 (작은 데이터는 작은 값)\n",
    "5. **교차 검증 필수**: 특성 수에 따른 성능 변화 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
