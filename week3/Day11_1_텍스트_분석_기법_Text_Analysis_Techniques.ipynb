{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day11_1: 텍스트 분석 기법 (Text Analysis Techniques)\n",
        "\n",
        "## 📚 학습 목표\n",
        "\n",
        "**Part 1: 기초**\n",
        "1. 단어 빈도(Counter)로 텍스트 분석 시작하기\n",
        "2. n-gram 개념 이해하고 활용하기\n",
        "3. CountVectorizer로 문서-단어 행렬 만들기\n",
        "4. TF-IDF 개념 이해하기\n",
        "5. TfidfVectorizer로 키워드 중요도 추출하기\n",
        "\n",
        "**Part 2: 심화**\n",
        "1. 코사인 유사도로 문서 비교하기\n",
        "2. 자카드 유사도 이해하기\n",
        "3. 중복 문서 탐지하기\n",
        "4. 간단한 문서 검색 시스템 구현하기\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 왜 이것을 배우나요?\n",
        "\n",
        "| 개념 | 실무 활용 | 예시 |\n",
        "|------|----------|------|\n",
        "| TF-IDF | 문서별 핵심 키워드 추출 | 뉴스 기사 자동 태깅, SEO 최적화 |\n",
        "| n-gram | 연속된 단어 패턴 분석 | 제품 리뷰에서 \"배송이 빠르다\" 탐지 |\n",
        "| 코사인 유사도 | 유사 문서 찾기 | 중복 기사 탐지, 추천 시스템 |\n",
        "| 문서 벡터화 | 텍스트를 숫자로 변환 | ML 모델 입력 데이터 생성 |\n",
        "\n",
        "**분석가 관점**: 텍스트는 정형화되지 않은 데이터지만, 벡터화와 유사도 측정을 통해 패턴을 발견하고 분류할 수 있습니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: 기초\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 단어 빈도 분석 (Word Frequency)\n",
        "\n",
        "### Counter를 활용한 기본 빈도 분석\n",
        "\n",
        "텍스트 분석의 첫걸음은 단어 빈도를 세는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치 확인\n",
        "# !pip install scikit-learn plotly pandas\n",
        "\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# 예시 고객 리뷰 데이터\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"배송이 너무 빠르고 포장도 깔끔해요\",\n",
        "    \"상품 품질이 좋아요 다시 구매할게요\",\n",
        "    \"배송이 빠르고 서비스가 좋아요\"\n",
        "]\n",
        "\n",
        "# 모든 리뷰를 하나로 합치고 단어 분리\n",
        "all_words = []\n",
        "for review in reviews:\n",
        "    words = review.split()\n",
        "    all_words.extend(words)\n",
        "\n",
        "# 단어 빈도 계산\n",
        "word_counts = Counter(all_words)\n",
        "print(\"단어 빈도:\")\n",
        "for word, count in word_counts.most_common(10):\n",
        "    print(f\"  {word}: {count}회\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotly로 상위 키워드 시각화\n",
        "import plotly.express as px\n",
        "\n",
        "# 상위 10개 단어\n",
        "top_words = word_counts.most_common(10)\n",
        "df_words = pd.DataFrame(top_words, columns=['단어', '빈도'])\n",
        "\n",
        "fig = px.bar(\n",
        "    df_words, \n",
        "    x='빈도', \n",
        "    y='단어', \n",
        "    orientation='h',\n",
        "    title='상위 10개 빈출 단어',\n",
        "    color='빈도',\n",
        "    color_continuous_scale='Blues'\n",
        ")\n",
        "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: 불만 키워드 탐지\n",
        "\n",
        "고객센터에 접수된 불만 사항에서 자주 언급되는 키워드를 찾아봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 고객 불만 데이터\n",
        "complaints = [\n",
        "    \"환불 요청했는데 처리가 너무 느려요\",\n",
        "    \"상품이 파손되어 배송됐어요 환불 해주세요\",\n",
        "    \"교환 신청했는데 연락이 없어요\",\n",
        "    \"환불 처리가 안 되고 있어요\",\n",
        "    \"배송 지연이 너무 심해요 취소하고 싶어요\"\n",
        "]\n",
        "\n",
        "# 불만 키워드 추출\n",
        "complaint_words = []\n",
        "for c in complaints:\n",
        "    complaint_words.extend(c.split())\n",
        "\n",
        "complaint_counts = Counter(complaint_words)\n",
        "print(\"불만 키워드 TOP 5:\")\n",
        "for word, count in complaint_counts.most_common(5):\n",
        "    print(f\"  '{word}': {count}회 언급\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.2 n-gram 분석\n",
        "\n",
        "### n-gram이란?\n",
        "\n",
        "- **unigram (1-gram)**: 단일 단어 (\"배송\", \"빠르다\")\n",
        "- **bigram (2-gram)**: 연속된 2개 단어 (\"배송이 빠르다\")\n",
        "- **trigram (3-gram)**: 연속된 3개 단어 (\"정말 배송이 빠르다\")\n",
        "\n",
        "n-gram을 사용하면 단어의 **맥락**을 파악할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n-gram 생성 함수\n",
        "def get_ngrams(text, n):\n",
        "    \"\"\"텍스트에서 n-gram 추출\"\"\"\n",
        "    words = text.split()\n",
        "    ngrams = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = ' '.join(words[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "# 예시 텍스트\n",
        "text = \"배송이 빠르고 상품 품질이 좋아요\"\n",
        "\n",
        "print(\"원본:\", text)\n",
        "print(\"\\n1-gram (unigram):\", get_ngrams(text, 1))\n",
        "print(\"2-gram (bigram):\", get_ngrams(text, 2))\n",
        "print(\"3-gram (trigram):\", get_ngrams(text, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여러 문서에서 bigram 빈도 분석\n",
        "reviews = [\n",
        "    \"배송이 빠르고 포장이 깔끔해요\",\n",
        "    \"배송이 빠르고 상품이 좋아요\",\n",
        "    \"상품이 좋아요 추천합니다\",\n",
        "    \"포장이 깔끔해요 감사합니다\"\n",
        "]\n",
        "\n",
        "# 모든 bigram 수집\n",
        "all_bigrams = []\n",
        "for review in reviews:\n",
        "    all_bigrams.extend(get_ngrams(review, 2))\n",
        "\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "print(\"자주 등장하는 bigram:\")\n",
        "for bigram, count in bigram_counts.most_common(5):\n",
        "    print(f\"  '{bigram}': {count}회\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: 제품 리뷰에서 핵심 표현 찾기\n",
        "\n",
        "bigram 분석으로 고객이 자주 사용하는 표현 패턴을 발견합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전자제품 리뷰 데이터\n",
        "product_reviews = [\n",
        "    \"가성비 좋고 디자인이 예뻐요\",\n",
        "    \"가성비 최고 강력 추천합니다\",\n",
        "    \"배터리 오래가고 가성비 좋아요\",\n",
        "    \"강력 추천 가성비 최고예요\"\n",
        "]\n",
        "\n",
        "# bigram 추출\n",
        "bigrams = []\n",
        "for r in product_reviews:\n",
        "    bigrams.extend(get_ngrams(r, 2))\n",
        "\n",
        "# 빈도 시각화\n",
        "bigram_df = pd.DataFrame(\n",
        "    Counter(bigrams).most_common(8), \n",
        "    columns=['표현', '빈도']\n",
        ")\n",
        "\n",
        "fig = px.bar(\n",
        "    bigram_df,\n",
        "    x='빈도',\n",
        "    y='표현',\n",
        "    orientation='h',\n",
        "    title='자주 사용되는 2단어 표현 (Bigram)',\n",
        "    color='빈도'\n",
        ")\n",
        "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.3 CountVectorizer: 문서-단어 행렬\n",
        "\n",
        "### 문서를 숫자 벡터로 변환하기\n",
        "\n",
        "`CountVectorizer`는 텍스트를 **Bag of Words** 모델로 변환합니다.\n",
        "\n",
        "- 각 문서를 단어 빈도 벡터로 표현\n",
        "- 머신러닝 모델의 입력으로 사용 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 샘플 문서\n",
        "documents = [\n",
        "    \"배송이 빠르고 상품이 좋아요\",\n",
        "    \"상품 품질이 좋아요 추천합니다\",\n",
        "    \"배송이 느리고 상품이 파손됐어요\",\n",
        "    \"빠른 배송 감사합니다 추천합니다\"\n",
        "]\n",
        "\n",
        "# CountVectorizer 생성\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# 문서를 벡터로 변환\n",
        "count_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# 결과 확인\n",
        "print(\"단어 목록:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\n문서-단어 행렬:\")\n",
        "print(count_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame으로 보기 좋게 표시\n",
        "df_counts = pd.DataFrame(\n",
        "    count_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=[f'문서{i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(\"문서-단어 행렬 (Document-Term Matrix):\")\n",
        "df_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CountVectorizer 주요 옵션\n",
        "\n",
        "| 옵션 | 설명 | 예시 |\n",
        "|------|------|------|\n",
        "| `max_features` | 최대 단어 수 제한 | `max_features=100` |\n",
        "| `min_df` | 최소 문서 빈도 | `min_df=2` (2개 이상 문서) |\n",
        "| `max_df` | 최대 문서 빈도 | `max_df=0.8` (80% 이하) |\n",
        "| `ngram_range` | n-gram 범위 | `ngram_range=(1, 2)` |\n",
        "| `stop_words` | 불용어 제거 | `stop_words='english'` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n-gram 포함 CountVectorizer\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))  # unigram + bigram\n",
        "\n",
        "count_matrix_ngram = vectorizer_ngram.fit_transform(documents)\n",
        "\n",
        "print(\"unigram + bigram 단어 목록:\")\n",
        "print(vectorizer_ngram.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.4 TF-IDF 개념 이해\n",
        "\n",
        "### TF-IDF란?\n",
        "\n",
        "**TF-IDF** (Term Frequency - Inverse Document Frequency)는 단어의 **중요도**를 측정합니다.\n",
        "\n",
        "- **TF (Term Frequency)**: 문서 내 단어 빈도 (높을수록 중요)\n",
        "- **IDF (Inverse Document Frequency)**: 전체 문서에서의 희소성 (드물수록 중요)\n",
        "\n",
        "**TF-IDF = TF x IDF**\n",
        "\n",
        "- 문서 내에서 자주 등장하지만, 전체 문서에서는 드문 단어 = **높은 TF-IDF**\n",
        "- \"은\", \"는\", \"이\" 같은 일반적 단어 = **낮은 TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF 직관적 이해\n",
        "print(\"=== TF-IDF 직관적 예시 ===\")\n",
        "print()\n",
        "print(\"문서1: '파이썬 프로그래밍 학습'\")\n",
        "print(\"문서2: '자바 프로그래밍 학습'\")\n",
        "print(\"문서3: '데이터 분석 파이썬'\")\n",
        "print()\n",
        "print(\"'프로그래밍': 모든 문서에 등장 -> IDF 낮음 -> TF-IDF 낮음\")\n",
        "print(\"'파이썬': 일부 문서에만 등장 -> IDF 높음 -> TF-IDF 높음\")\n",
        "print()\n",
        "print(\"=> '파이썬'이 문서를 구별하는 더 중요한 키워드!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.5 TfidfVectorizer로 키워드 추출\n",
        "\n",
        "### 기본 사용법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 뉴스 기사 샘플\n",
        "news_articles = [\n",
        "    \"삼성전자 반도체 수출 증가 반도체 시장 호황\",\n",
        "    \"현대자동차 전기차 판매 증가 전기차 시장 성장\",\n",
        "    \"네이버 AI 기술 투자 AI 서비스 확대\",\n",
        "    \"카카오 AI 플랫폼 출시 AI 신사업 추진\"\n",
        "]\n",
        "\n",
        "# TfidfVectorizer 생성 및 변환\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(news_articles)\n",
        "\n",
        "# 결과를 DataFrame으로\n",
        "df_tfidf = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "    index=['삼성 기사', '현대 기사', '네이버 기사', '카카오 기사']\n",
        ")\n",
        "\n",
        "print(\"TF-IDF 행렬:\")\n",
        "df_tfidf.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 각 문서별 중요 키워드 추출 함수\n",
        "def get_top_keywords(tfidf_matrix, feature_names, doc_index, top_n=5):\n",
        "    \"\"\"문서에서 TF-IDF 상위 키워드 추출\"\"\"\n",
        "    # 해당 문서의 TF-IDF 점수\n",
        "    tfidf_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
        "    \n",
        "    # 점수와 단어를 묶어서 정렬\n",
        "    word_scores = list(zip(feature_names, tfidf_scores))\n",
        "    word_scores_sorted = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    return word_scores_sorted[:top_n]\n",
        "\n",
        "# 각 기사별 핵심 키워드\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, article_name in enumerate(['삼성 기사', '현대 기사', '네이버 기사', '카카오 기사']):\n",
        "    keywords = get_top_keywords(tfidf_matrix, feature_names, i, top_n=3)\n",
        "    print(f\"\\n{article_name} 핵심 키워드:\")\n",
        "    for word, score in keywords:\n",
        "        print(f\"  {word}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: 고객 리뷰 키워드 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 카테고리별 리뷰 데이터\n",
        "category_reviews = {\n",
        "    \"전자제품\": \"가성비 좋고 배터리 오래가요 화면이 선명해요\",\n",
        "    \"의류\": \"재질이 좋고 사이즈가 정확해요 색상이 예뻐요\",\n",
        "    \"식품\": \"맛있고 신선해요 포장이 꼼꼼해요 유통기한 넉넉해요\",\n",
        "    \"가구\": \"조립이 쉽고 튼튼해요 디자인이 깔끔해요\"\n",
        "}\n",
        "\n",
        "# TF-IDF 분석\n",
        "reviews_list = list(category_reviews.values())\n",
        "categories = list(category_reviews.keys())\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_result = tfidf.fit_transform(reviews_list)\n",
        "features = tfidf.get_feature_names_out()\n",
        "\n",
        "# 카테고리별 대표 키워드\n",
        "print(\"=== 카테고리별 핵심 키워드 ===\")\n",
        "for i, category in enumerate(categories):\n",
        "    top_keywords = get_top_keywords(tfidf_result, features, i, top_n=3)\n",
        "    keywords_str = ', '.join([f\"{w}({s:.2f})\" for w, s in top_keywords])\n",
        "    print(f\"{category}: {keywords_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: 심화\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 코사인 유사도 (Cosine Similarity)\n",
        "\n",
        "### 문서 간 유사도 측정\n",
        "\n",
        "코사인 유사도는 두 벡터 간의 **각도**를 측정합니다.\n",
        "\n",
        "- **값 범위**: 0 (완전히 다름) ~ 1 (완전히 같음)\n",
        "- **장점**: 문서 길이에 영향 받지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 비교할 문서들\n",
        "documents = [\n",
        "    \"파이썬 머신러닝 데이터 분석\",  # 문서1\n",
        "    \"파이썬 데이터 분석 시각화\",    # 문서2 (문서1과 유사)\n",
        "    \"자바 웹 개발 스프링\",          # 문서3 (문서1과 다름)\n",
        "    \"데이터 분석 머신러닝 딥러닝\"    # 문서4 (문서1과 유사)\n",
        "]\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# 결과를 DataFrame으로\n",
        "df_similarity = pd.DataFrame(\n",
        "    cosine_sim,\n",
        "    index=[f'문서{i+1}' for i in range(len(documents))],\n",
        "    columns=[f'문서{i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(\"문서 간 코사인 유사도:\")\n",
        "df_similarity.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 히트맵으로 시각화\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# 문서 라벨\n",
        "labels = [f'문서{i+1}' for i in range(len(documents))]\n",
        "\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=cosine_sim.round(2),\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='문서 간 코사인 유사도 히트맵',\n",
        "    xaxis_title='문서',\n",
        "    yaxis_title='문서'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 특정 문서와 가장 유사한 문서 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_similar_documents(query_index, similarity_matrix, documents, top_n=3):\n",
        "    \"\"\"주어진 문서와 가장 유사한 문서 찾기\"\"\"\n",
        "    # 유사도 점수\n",
        "    sim_scores = list(enumerate(similarity_matrix[query_index]))\n",
        "    \n",
        "    # 자기 자신 제외하고 정렬\n",
        "    sim_scores = [(i, score) for i, score in sim_scores if i != query_index]\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # 상위 n개 반환\n",
        "    results = []\n",
        "    for idx, score in sim_scores[:top_n]:\n",
        "        results.append({\n",
        "            '문서': documents[idx],\n",
        "            '유사도': round(score, 3)\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 문서1과 유사한 문서 찾기\n",
        "print(f\"기준 문서: '{documents[0]}'\")\n",
        "print(\"\\n유사한 문서:\")\n",
        "similar_docs = find_similar_documents(0, cosine_sim, documents)\n",
        "for doc in similar_docs:\n",
        "    print(f\"  유사도 {doc['유사도']}: {doc['문서']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: 뉴스 기사 추천 시스템"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 뉴스 기사 데이터\n",
        "news_db = [\n",
        "    {\"id\": 1, \"title\": \"삼성전자 반도체 수출 역대 최고\", \"category\": \"경제\"},\n",
        "    {\"id\": 2, \"title\": \"SK하이닉스 반도체 투자 확대\", \"category\": \"경제\"},\n",
        "    {\"id\": 3, \"title\": \"손흥민 토트넘 결승골 활약\", \"category\": \"스포츠\"},\n",
        "    {\"id\": 4, \"title\": \"TSMC 반도체 생산량 증가\", \"category\": \"경제\"},\n",
        "    {\"id\": 5, \"title\": \"이강인 PSG 리그 우승 기여\", \"category\": \"스포츠\"}\n",
        "]\n",
        "\n",
        "# 기사 제목으로 TF-IDF\n",
        "titles = [n[\"title\"] for n in news_db]\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(titles)\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# 사용자가 읽은 기사 (id=1)\n",
        "user_article_idx = 0\n",
        "print(f\"읽은 기사: {news_db[user_article_idx]['title']}\")\n",
        "print(\"\\n추천 기사:\")\n",
        "\n",
        "similar = find_similar_documents(user_article_idx, cosine_sim, titles)\n",
        "for doc in similar:\n",
        "    print(f\"  - {doc['문서']} (유사도: {doc['유사도']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.2 자카드 유사도 (Jaccard Similarity)\n",
        "\n",
        "### 집합 기반 유사도\n",
        "\n",
        "자카드 유사도는 두 집합의 **교집합/합집합** 비율을 계산합니다.\n",
        "\n",
        "$$\\text{Jaccard}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
        "\n",
        "- **값 범위**: 0 (공통 없음) ~ 1 (완전히 같음)\n",
        "- **장점**: 단어 빈도 무시, 단순 존재 여부만 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"두 집합의 자카드 유사도 계산\"\"\"\n",
        "    intersection = len(set1 & set2)  # 교집합\n",
        "    union = len(set1 | set2)         # 합집합\n",
        "    \n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return intersection / union\n",
        "\n",
        "# 두 문서 비교\n",
        "doc1 = \"파이썬 머신러닝 데이터 분석\"\n",
        "doc2 = \"파이썬 데이터 분석 시각화\"\n",
        "\n",
        "# 단어 집합으로 변환\n",
        "set1 = set(doc1.split())\n",
        "set2 = set(doc2.split())\n",
        "\n",
        "print(f\"문서1 단어: {set1}\")\n",
        "print(f\"문서2 단어: {set2}\")\n",
        "print(f\"\\n교집합: {set1 & set2}\")\n",
        "print(f\"합집합: {set1 | set2}\")\n",
        "print(f\"\\n자카드 유사도: {jaccard_similarity(set1, set2):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여러 문서 비교\n",
        "documents = [\n",
        "    \"파이썬 머신러닝 데이터 분석\",\n",
        "    \"파이썬 데이터 분석 시각화\",\n",
        "    \"자바 웹 개발 스프링\",\n",
        "    \"파이썬 웹 크롤링 스크래핑\"\n",
        "]\n",
        "\n",
        "# 자카드 유사도 행렬 계산\n",
        "n = len(documents)\n",
        "jaccard_matrix = []\n",
        "\n",
        "for i in range(n):\n",
        "    row = []\n",
        "    set_i = set(documents[i].split())\n",
        "    for j in range(n):\n",
        "        set_j = set(documents[j].split())\n",
        "        sim = jaccard_similarity(set_i, set_j)\n",
        "        row.append(sim)\n",
        "    jaccard_matrix.append(row)\n",
        "\n",
        "# DataFrame으로 표시\n",
        "df_jaccard = pd.DataFrame(\n",
        "    jaccard_matrix,\n",
        "    index=[f'문서{i+1}' for i in range(n)],\n",
        "    columns=[f'문서{i+1}' for i in range(n)]\n",
        ")\n",
        "\n",
        "print(\"자카드 유사도 행렬:\")\n",
        "df_jaccard.round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 코사인 vs 자카드 비교\n",
        "\n",
        "| 특성 | 코사인 유사도 | 자카드 유사도 |\n",
        "|------|-------------|-------------|\n",
        "| 기준 | 벡터 각도 | 집합 중복 |\n",
        "| 빈도 고려 | O (TF-IDF) | X (존재 여부만) |\n",
        "| 적합한 경우 | 긴 문서 비교 | 짧은 텍스트, 태그 비교 |\n",
        "| 예시 | 뉴스 기사 유사도 | 해시태그 유사도 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.3 중복 문서 탐지\n",
        "\n",
        "### 유사도 임계값으로 중복 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_duplicates(documents, threshold=0.7):\n",
        "    \"\"\"유사도가 임계값 이상인 문서 쌍 찾기\"\"\"\n",
        "    # TF-IDF 벡터화\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(documents)\n",
        "    \n",
        "    # 코사인 유사도 계산\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    \n",
        "    # 중복 쌍 찾기\n",
        "    duplicates = []\n",
        "    n = len(documents)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):  # 상삼각 행렬만 확인\n",
        "            if cosine_sim[i][j] >= threshold:\n",
        "                duplicates.append({\n",
        "                    '문서1_idx': i,\n",
        "                    '문서2_idx': j,\n",
        "                    '문서1': documents[i],\n",
        "                    '문서2': documents[j],\n",
        "                    '유사도': round(cosine_sim[i][j], 3)\n",
        "                })\n",
        "    \n",
        "    return duplicates\n",
        "\n",
        "# 테스트 데이터 (일부 중복 포함)\n",
        "news_headlines = [\n",
        "    \"삼성전자 반도체 수출 역대 최고 실적 달성\",\n",
        "    \"삼성전자 반도체 수출 사상 최대 기록\",  # 중복 의심\n",
        "    \"현대차 전기차 판매량 증가\",\n",
        "    \"LG전자 가전 사업 호조\",\n",
        "    \"삼성 반도체 수출 역대 최고치 경신\"  # 중복 의심\n",
        "]\n",
        "\n",
        "# 중복 탐지 (임계값 0.5)\n",
        "duplicates = find_duplicates(news_headlines, threshold=0.5)\n",
        "\n",
        "print(\"=== 중복 의심 문서 ===\\n\")\n",
        "for dup in duplicates:\n",
        "    print(f\"유사도: {dup['유사도']}\")\n",
        "    print(f\"  문서1: {dup['문서1']}\")\n",
        "    print(f\"  문서2: {dup['문서2']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: 중복 리뷰 탐지 시스템"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 쇼핑몰 리뷰 데이터 (일부 조작 리뷰 포함)\n",
        "reviews = [\n",
        "    {\"id\": 1, \"user\": \"user1\", \"text\": \"배송 빠르고 상품 좋아요 강력 추천\"},\n",
        "    {\"id\": 2, \"user\": \"user2\", \"text\": \"상품 품질 최고입니다 감사합니다\"},\n",
        "    {\"id\": 3, \"user\": \"user3\", \"text\": \"배송 빠르고 상품 좋아요 추천합니다\"},  # user1과 유사\n",
        "    {\"id\": 4, \"user\": \"user4\", \"text\": \"가격 대비 만족스러워요\"},\n",
        "    {\"id\": 5, \"user\": \"user5\", \"text\": \"배송이 빠르고 상품이 좋아요 추천\"} # user1과 유사\n",
        "]\n",
        "\n",
        "# 중복 탐지\n",
        "review_texts = [r[\"text\"] for r in reviews]\n",
        "duplicates = find_duplicates(review_texts, threshold=0.6)\n",
        "\n",
        "print(\"=== 조작 의심 리뷰 탐지 ===\")\n",
        "print(f\"총 리뷰: {len(reviews)}개\\n\")\n",
        "\n",
        "for dup in duplicates:\n",
        "    idx1, idx2 = dup['문서1_idx'], dup['문서2_idx']\n",
        "    print(f\"유사도 {dup['유사도']} - 조작 의심!\")\n",
        "    print(f\"  리뷰 {reviews[idx1]['id']} ({reviews[idx1]['user']}): {reviews[idx1]['text']}\")\n",
        "    print(f\"  리뷰 {reviews[idx2]['id']} ({reviews[idx2]['user']}): {reviews[idx2]['text']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.4 간단한 문서 검색 시스템\n",
        "\n",
        "### TF-IDF 기반 검색 엔진"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleSearchEngine:\n",
        "    \"\"\"TF-IDF 기반 간단한 검색 엔진\"\"\"\n",
        "    \n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
        "    \n",
        "    def search(self, query, top_n=3):\n",
        "        \"\"\"검색 쿼리로 관련 문서 찾기\"\"\"\n",
        "        # 쿼리를 벡터로 변환\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        \n",
        "        # 모든 문서와 유사도 계산\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "        \n",
        "        # 상위 n개 결과\n",
        "        top_indices = similarities.argsort()[::-1][:top_n]\n",
        "        \n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:  # 유사도 0보다 큰 것만\n",
        "                results.append({\n",
        "                    '문서': self.documents[idx],\n",
        "                    '유사도': round(similarities[idx], 3)\n",
        "                })\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문서 데이터베이스\n",
        "document_db = [\n",
        "    \"파이썬 머신러닝 입문 강좌입니다\",\n",
        "    \"데이터 분석을 위한 판다스 튜토리얼\",\n",
        "    \"딥러닝 텐서플로우 기초 가이드\",\n",
        "    \"파이썬 웹 크롤링 실습 예제\",\n",
        "    \"머신러닝 알고리즘 비교 분석\",\n",
        "    \"자연어 처리 NLP 기초 강좌\",\n",
        "    \"파이썬 데이터 시각화 플롯리\"\n",
        "]\n",
        "\n",
        "# 검색 엔진 생성\n",
        "search_engine = SimpleSearchEngine(document_db)\n",
        "\n",
        "# 검색 테스트\n",
        "queries = [\"머신러닝\", \"데이터 분석\", \"파이썬 기초\"]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\n검색어: '{query}'\")\n",
        "    results = search_engine.search(query, top_n=3)\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"  {i}. [{result['유사도']}] {result['문서']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 💡 실무 예시: FAQ 자동 응답 시스템"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAQ 데이터베이스\n",
        "faq_db = [\n",
        "    {\"q\": \"배송 얼마나 걸리나요\", \"a\": \"일반 배송은 2-3일, 빠른 배송은 1일 소요됩니다.\"},\n",
        "    {\"q\": \"환불 가능한가요\", \"a\": \"구매 후 7일 이내 환불 가능합니다.\"},\n",
        "    {\"q\": \"교환 신청 어떻게 하나요\", \"a\": \"마이페이지 > 주문내역에서 교환 신청하세요.\"},\n",
        "    {\"q\": \"결제 방법 뭐가 있나요\", \"a\": \"카드, 계좌이체, 간편결제 모두 가능합니다.\"},\n",
        "    {\"q\": \"포인트 어떻게 사용하나요\", \"a\": \"결제 시 포인트 사용 선택하시면 됩니다.\"}\n",
        "]\n",
        "\n",
        "# FAQ 검색 엔진\n",
        "faq_questions = [f[\"q\"] for f in faq_db]\n",
        "faq_engine = SimpleSearchEngine(faq_questions)\n",
        "\n",
        "def answer_question(user_question):\n",
        "    \"\"\"사용자 질문에 가장 적합한 FAQ 답변 찾기\"\"\"\n",
        "    results = faq_engine.search(user_question, top_n=1)\n",
        "    \n",
        "    if results and results[0]['유사도'] > 0.2:\n",
        "        # 가장 유사한 FAQ 찾기\n",
        "        matched_q = results[0]['문서']\n",
        "        for faq in faq_db:\n",
        "            if faq['q'] == matched_q:\n",
        "                return faq['a'], results[0]['유사도']\n",
        "    \n",
        "    return \"죄송합니다. 해당 질문에 대한 답변을 찾을 수 없습니다.\", 0.0\n",
        "\n",
        "# 테스트\n",
        "test_questions = [\n",
        "    \"배송 시간이 얼마나 되나요?\",\n",
        "    \"환불하고 싶어요\",\n",
        "    \"결제는 어떻게 해요?\",\n",
        "    \"제품 AS 문의\"\n",
        "]\n",
        "\n",
        "print(\"=== FAQ 자동 응답 시스템 ===\")\n",
        "for q in test_questions:\n",
        "    answer, score = answer_question(q)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"   (신뢰도: {score:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🎯 실습 퀴즈\n",
        "\n",
        "**난이도**: ⭐ (쉬움) ~ ⭐⭐⭐⭐⭐ (어려움)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. 단어 빈도 계산 ⭐\n",
        "\n",
        "**문제**: 다음 리뷰 텍스트에서 가장 많이 등장한 단어와 그 빈도를 출력하세요.\n",
        "\n",
        "```python\n",
        "review = \"배송이 빠르고 상품이 좋아요 배송도 빠르고 서비스도 좋아요\"\n",
        "```\n",
        "\n",
        "**기대 결과**: `('좋아요', 2)` 또는 `('빠르고', 2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "review = \"배송이 빠르고 상품이 좋아요 배송도 빠르고 서비스도 좋아요\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. bigram 추출 ⭐\n",
        "\n",
        "**문제**: 주어진 텍스트에서 모든 bigram(연속된 2단어)을 추출하세요.\n",
        "\n",
        "```python\n",
        "text = \"파이썬 데이터 분석 입문\"\n",
        "```\n",
        "\n",
        "**기대 결과**: `['파이썬 데이터', '데이터 분석', '분석 입문']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"파이썬 데이터 분석 입문\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. CountVectorizer 기초 ⭐⭐\n",
        "\n",
        "**문제**: CountVectorizer를 사용하여 문서-단어 행렬을 만들고, '배송'이라는 단어가 각 문서에 몇 번 등장하는지 출력하세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"배송이 빨라요 배송 최고\",\n",
        "    \"상품이 좋아요\",\n",
        "    \"배송도 빠르고 상품도 좋아요\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"배송이 빨라요 배송 최고\",\n",
        "    \"상품이 좋아요\",\n",
        "    \"배송도 빠르고 상품도 좋아요\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. TF-IDF 키워드 추출 ⭐⭐\n",
        "\n",
        "**문제**: TfidfVectorizer를 사용하여 첫 번째 문서의 TF-IDF 값이 가장 높은 단어를 찾으세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"삼성전자 반도체 수출\",\n",
        "    \"현대자동차 전기차 판매\",\n",
        "    \"LG전자 가전 수출\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"삼성전자 반도체 수출\",\n",
        "    \"현대자동차 전기차 판매\",\n",
        "    \"LG전자 가전 수출\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. 자카드 유사도 계산 ⭐⭐\n",
        "\n",
        "**문제**: 두 문서의 자카드 유사도를 계산하세요.\n",
        "\n",
        "```python\n",
        "doc1 = \"파이썬 데이터 분석\"\n",
        "doc2 = \"파이썬 머신러닝 분석\"\n",
        "```\n",
        "\n",
        "**힌트**: 교집합 / 합집합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc1 = \"파이썬 데이터 분석\"\n",
        "doc2 = \"파이썬 머신러닝 분석\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 심화 퀴즈\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. 코사인 유사도로 유사 문서 찾기 ⭐⭐⭐\n",
        "\n",
        "**문제**: 기준 문서와 가장 유사한 문서를 찾고 유사도를 출력하세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"파이썬 머신러닝 딥러닝\",  # 기준 문서 (인덱스 0)\n",
        "    \"자바 웹 개발\",\n",
        "    \"파이썬 데이터 분석 머신러닝\",\n",
        "    \"자바스크립트 프론트엔드\"\n",
        "]\n",
        "```\n",
        "\n",
        "**기대 결과**: 문서 인덱스와 유사도 값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"파이썬 머신러닝 딥러닝\",\n",
        "    \"자바 웹 개발\",\n",
        "    \"파이썬 데이터 분석 머신러닝\",\n",
        "    \"자바스크립트 프론트엔드\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. n-gram + TF-IDF 조합 ⭐⭐⭐\n",
        "\n",
        "**문제**: unigram과 bigram을 모두 포함하는 TfidfVectorizer를 만들고, 첫 번째 문서에서 TF-IDF 값이 가장 높은 상위 3개 feature를 출력하세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"배송이 빠르고 상품 품질 좋아요\",\n",
        "    \"상품 품질은 좋은데 배송이 느려요\",\n",
        "    \"가격 대비 만족스러워요\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"배송이 빠르고 상품 품질 좋아요\",\n",
        "    \"상품 품질은 좋은데 배송이 느려요\",\n",
        "    \"가격 대비 만족스러워요\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. 중복 문서 탐지 ⭐⭐⭐⭐\n",
        "\n",
        "**문제**: 코사인 유사도가 0.6 이상인 문서 쌍을 모두 찾아 출력하세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"삼성전자 반도체 수출 증가\",\n",
        "    \"현대차 전기차 판매 호조\",\n",
        "    \"삼성 반도체 수출 사상 최대\",  # 0번과 유사\n",
        "    \"LG전자 가전 사업 성장\",\n",
        "    \"삼성전자 반도체 수출 신기록\"   # 0번과 유사\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"삼성전자 반도체 수출 증가\",\n",
        "    \"현대차 전기차 판매 호조\",\n",
        "    \"삼성 반도체 수출 사상 최대\",\n",
        "    \"LG전자 가전 사업 성장\",\n",
        "    \"삼성전자 반도체 수출 신기록\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. 검색 쿼리 매칭 ⭐⭐⭐⭐\n",
        "\n",
        "**문제**: 검색 쿼리 \"데이터 분석\"과 가장 관련 높은 문서 2개를 찾아 유사도와 함께 출력하세요.\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"파이썬 기초 문법 강좌\",\n",
        "    \"데이터 분석을 위한 판다스\",\n",
        "    \"웹 개발 입문 가이드\",\n",
        "    \"머신러닝 데이터 전처리\",\n",
        "    \"SQL 데이터베이스 기초\"\n",
        "]\n",
        "\n",
        "query = \"데이터 분석\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"파이썬 기초 문법 강좌\",\n",
        "    \"데이터 분석을 위한 판다스\",\n",
        "    \"웹 개발 입문 가이드\",\n",
        "    \"머신러닝 데이터 전처리\",\n",
        "    \"SQL 데이터베이스 기초\"\n",
        "]\n",
        "\n",
        "query = \"데이터 분석\"\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. 종합: 문서 분류 및 키워드 추출 시스템 ⭐⭐⭐⭐⭐\n",
        "\n",
        "**문제**: 다음 요구사항을 모두 만족하는 분석 시스템을 구현하세요.\n",
        "\n",
        "1. TF-IDF로 각 문서의 상위 2개 키워드 추출\n",
        "2. 코사인 유사도로 가장 유사한 문서 쌍 찾기\n",
        "3. Plotly로 문서 간 유사도 히트맵 시각화\n",
        "\n",
        "```python\n",
        "documents = [\n",
        "    \"삼성전자 스마트폰 갤럭시 신제품 출시\",\n",
        "    \"애플 아이폰 신제품 발표\",\n",
        "    \"현대차 전기차 아이오닉 판매 증가\",\n",
        "    \"테슬라 전기차 모델Y 인기\",\n",
        "    \"삼성 갤럭시 폴드 스마트폰 혁신\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import plotly.figure_factory as ff\n",
        "import pandas as pd\n",
        "\n",
        "documents = [\n",
        "    \"삼성전자 스마트폰 갤럭시 신제품 출시\",\n",
        "    \"애플 아이폰 신제품 발표\",\n",
        "    \"현대차 전기차 아이오닉 판매 증가\",\n",
        "    \"테슬라 전기차 모델Y 인기\",\n",
        "    \"삼성 갤럭시 폴드 스마트폰 혁신\"\n",
        "]\n",
        "\n",
        "# 여기에 코드를 작성하세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 📊 학습 정리\n",
        "\n",
        "### Part 1: 기초 핵심 요약\n",
        "\n",
        "| 개념 | 핵심 메소드/클래스 | 실무 활용 |\n",
        "|------|------------------|----------|\n",
        "| 단어 빈도 | `Counter`, `most_common()` | 키워드 순위, 불만 분석 |\n",
        "| n-gram | 직접 구현 또는 `ngram_range` | 연속 표현 패턴 탐지 |\n",
        "| CountVectorizer | `fit_transform()`, `get_feature_names_out()` | 문서-단어 행렬 생성 |\n",
        "| TF-IDF | `TfidfVectorizer` | 핵심 키워드 추출, 중요도 측정 |\n",
        "\n",
        "### Part 2: 심화 핵심 요약\n",
        "\n",
        "| 기법 | 사용법 | 언제 쓰나? |\n",
        "|------|--------|----------|\n",
        "| 코사인 유사도 | `cosine_similarity(tfidf_matrix)` | 긴 문서 비교, 추천 시스템 |\n",
        "| 자카드 유사도 | `len(A&B) / len(A|B)` | 짧은 텍스트, 태그 비교 |\n",
        "| 중복 탐지 | 유사도 > 임계값 | 표절 검사, 스팸 필터링 |\n",
        "| 문서 검색 | 쿼리 벡터 + 유사도 | FAQ 봇, 검색 엔진 |\n",
        "\n",
        "### 💡 실무 팁\n",
        "\n",
        "1. **전처리 중요**: TF-IDF 전에 불용어 제거, 형태소 분석 적용\n",
        "2. **n-gram 범위**: bigram까지가 일반적, trigram은 데이터 많을 때만\n",
        "3. **유사도 임계값**: 중복 탐지 시 0.7~0.8, 추천은 0.3~0.5\n",
        "4. **희소 행렬**: 대용량 문서는 `sparse matrix` 유지 (toarray() 자제)\n",
        "5. **한글 분석**: KoNLPy 형태소 분석과 조합하면 정확도 향상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
