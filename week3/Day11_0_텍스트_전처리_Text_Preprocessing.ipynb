{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 11-0: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (Text Preprocessing)\n",
        "\n",
        "**í•™ìŠµ ì‹œê°„**: 3ì‹œê°„\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "**Part 1: ê¸°ì´ˆ ì „ì²˜ë¦¬**\n",
        "1. **í…ìŠ¤íŠ¸ ì •ì œ ê¸°ë³¸**: ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "2. **ì •ê·œí‘œí˜„ì‹ í™œìš©**: `re` ëª¨ë“ˆë¡œ íŒ¨í„´ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
        "3. **í† í°í™” (Tokenization)**: ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
        "4. **ë¶ˆìš©ì–´ ì œê±°**: NLTK stopwordsë¡œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì œê±°\n",
        "5. **ì–´ê°„ ì¶”ì¶œ (Stemming)**: ë‹¨ì–´ì˜ ì–´ê·¼ ì¶”ì¶œ\n",
        "\n",
        "**Part 2: í•œê¸€ ì²˜ë¦¬**\n",
        "1. **KoNLPy ì†Œê°œ**: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "2. **í˜•íƒœì†Œ ë¶„ì„**: Okt, Komoranìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ë¦¬\n",
        "3. **í’ˆì‚¬ íƒœê¹… (POS Tagging)**: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ êµ¬ë¶„\n",
        "4. **ëª…ì‚¬ ì¶”ì¶œ**: í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ë²•\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ ì™œ ì´ê²ƒì„ ë°°ìš°ë‚˜ìš”?\n",
        "\n",
        "| ê°œë… | ì‹¤ë¬´ í™œìš© | ì˜ˆì‹œ |\n",
        "|------|----------|------|\n",
        "| í…ìŠ¤íŠ¸ ì •ì œ | ë°ì´í„° í’ˆì§ˆ í–¥ìƒ | ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ HTML íƒœê·¸ ì œê±° |\n",
        "| í† í°í™” | ë¶„ì„ ë‹¨ìœ„ ìƒì„± | ê³ ê° ë¦¬ë·°ë¥¼ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬ |\n",
        "| ë¶ˆìš©ì–´ ì œê±° | ë…¸ì´ì¦ˆ ê°ì†Œ | \"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\" ì œê±° |\n",
        "| í˜•íƒœì†Œ ë¶„ì„ | í•œê¸€ ì²˜ë¦¬ | \"ë¨¹ì—ˆìŠµë‹ˆë‹¤\" â†’ \"ë¨¹ë‹¤\" ì¶”ì¶œ |\n",
        "| ëª…ì‚¬ ì¶”ì¶œ | í‚¤ì›Œë“œ ë¶„ì„ | ë¦¬ë·°ì—ì„œ \"ë°°ì†¡\", \"í’ˆì§ˆ\" ì¶”ì¶œ |\n",
        "\n",
        "**ë¶„ì„ê°€ ê´€ì **: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ëŠ” ëª¨ë“  NLP íŒŒì´í”„ë¼ì¸ì˜ ì²« ë‹¨ê³„ì…ë‹ˆë‹¤. ê¹¨ë—í•œ ë°ì´í„°ê°€ ì¢‹ì€ ë¶„ì„ ê²°ê³¼ë¥¼ ë§Œë“­ë‹ˆë‹¤!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "\n",
        "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "51f238f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[2mResolved \u001b[1m141 packages\u001b[0m \u001b[2min 801ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`numpy==2.4.0` is yanked (reason: \"Backward compatibility bug\")\u001b[0m\n",
            "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`numpy==2.4.0` is yanked (reason: \"Backward compatibility bug\")\u001b[0m\n",
            "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)                                                   \n",
            "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m     0 B/568.73 KiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 16.00 KiB/568.73 KiB        \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 16.00 KiB/568.73 KiB        \u001b[1A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m\u001b[30m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 16.00 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 16.00 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 32.00 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 48.00 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m-\u001b[30m\u001b[2m-----------------------------\u001b[0m\u001b[0m 14.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 30.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m----\u001b[30m\u001b[2m--------------------------\u001b[0m\u001b[0m 46.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m------\u001b[30m\u001b[2m------------------------\u001b[0m\u001b[0m 62.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m--------\u001b[30m\u001b[2m----------------------\u001b[0m\u001b[0m 78.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m--------\u001b[30m\u001b[2m----------------------\u001b[0m\u001b[0m 78.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m--------\u001b[30m\u001b[2m----------------------\u001b[0m\u001b[0m 78.89 KiB/282.12 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB        \u001b[2A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m--------\u001b[30m\u001b[2m----------------------\u001b[0m\u001b[0m 78.89 KiB/282.12 KiB\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m     0 B/18.53 MiB           \u001b[3A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m----------\u001b[30m\u001b[2m--------------------\u001b[0m\u001b[0m 94.89 KiB/282.12 KiB\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m---\u001b[30m\u001b[2m---------------------------\u001b[0m\u001b[0m 61.18 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m     0 B/18.53 MiB           \u001b[3A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m----------\u001b[30m\u001b[2m--------------------\u001b[0m\u001b[0m 94.89 KiB/282.12 KiB\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m----\u001b[30m\u001b[2m--------------------------\u001b[0m\u001b[0m 77.18 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m     0 B/18.53 MiB           \u001b[3A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m-----------\u001b[30m\u001b[2m-------------------\u001b[0m\u001b[0m 110.89 KiB/282.12 KiB\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m----\u001b[30m\u001b[2m--------------------------\u001b[0m\u001b[0m 77.18 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m     0 B/18.53 MiB           \u001b[3A\n",
            "\u001b[2mregex               \u001b[0m \u001b[32m------------------------------\u001b[30m\u001b[2m\u001b[0m\u001b[0m 282.12 KiB/282.12 KiB\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m--------------------\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 397.08 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[3A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 256.00 KiB/18.53 MiB        \u001b[3A\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m--------------------\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 397.08 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 256.00 KiB/18.53 MiB        \u001b[2A\n",
            "\u001b[2mjpype1              \u001b[0m \u001b[32m---------------------\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 413.18 KiB/568.73 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)-------------------\u001b[0m\u001b[0m 651.00 KiB/18.53 MiB        \u001b[2A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 916.81 KiB/18.53 MiB        \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 2.05 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 2.32 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 2.45 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 2.88 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 3.25 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 3.58 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 3.69 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 4.32 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 5.26 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------------------\u001b[0m\u001b[0m 6.38 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)m------------------\u001b[0m\u001b[0m 7.46 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)2m-----------------\u001b[0m\u001b[0m 8.27 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)2m-----------------\u001b[0m\u001b[0m 8.35 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)[2m----------------\u001b[0m\u001b[0m 8.81 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)\u001b[2m---------------\u001b[0m\u001b[0m 9.85 MiB/18.53 MiB          \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ´\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)0m\u001b[2m-------------\u001b[0m\u001b[0m 10.64 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)0m\u001b[2m-------------\u001b[0m\u001b[0m 10.65 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)30m\u001b[2m------------\u001b[0m\u001b[0m 11.32 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)\u001b[30m\u001b[2m----------\u001b[0m\u001b[0m 12.43 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ¦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-\u001b[30m\u001b[2m---------\u001b[0m\u001b[0m 13.52 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--\u001b[30m\u001b[2m--------\u001b[0m\u001b[0m 14.05 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 15.20 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)----\u001b[30m\u001b[2m------\u001b[0m\u001b[0m 15.39 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ §\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-----\u001b[30m\u001b[2m-----\u001b[0m\u001b[0m 15.48 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)------\u001b[30m\u001b[2m----\u001b[0m\u001b[0m 16.08 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 17.22 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)-------\u001b[30m\u001b[2m---\u001b[0m\u001b[0m 17.23 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[1A\u001b[37mâ ‡\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/3)--------\u001b[30m\u001b[2m--\u001b[0m\u001b[0m 17.73 MiB/18.53 MiB         \u001b[1A\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 1.62s\u001b[0m\u001b[0m                                                      \u001b[1A\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m                                \u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjpype1\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkonlpy\u001b[0m\u001b[2m==0.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2026.1.15\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv add nltk konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/dante/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/dante/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /Users/dante/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /Users/dante/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)\n",
        "# !pip install nltk konlpy\n",
        "\n",
        "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)\n",
        "import nltk\n",
        "nltk.download('punkt')        # í† í°í™”\n",
        "nltk.download('stopwords')    # ë¶ˆìš©ì–´\n",
        "nltk.download('wordnet')      # í‘œì œì–´ ì¶”ì¶œ\n",
        "nltk.download('punkt_tab')    # í† í°í™” ì¶”ê°€ ë°ì´í„°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "385b26c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x@   6 dante  staff   192 Jul 11  2024 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
            "drwxr-x---+ 231 dante  staff  7392 Jan 15 13:27 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
            "-rw-r--r--@   1 dante  staff  6148 Nov 18 16:59 .DS_Store\n",
            "drwxr-xr-x@   5 dante  staff   160 Jan 15 13:48 \u001b[1m\u001b[36mcorpora\u001b[m\u001b[m\n",
            "drwxr-xr-x@   7 dante  staff   224 Nov 18 16:58 \u001b[1m\u001b[36mtaggers\u001b[m\u001b[m\n",
            "drwxr-xr-x@   7 dante  staff   224 Nov 22 12:40 \u001b[1m\u001b[36mtokenizers\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls -al ~/nltk*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK (ì˜ì–´ ì²˜ë¦¬)\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# KoNLPy (í•œê¸€ ì²˜ë¦¬)\n",
        "from konlpy.tag import Okt, Komoran\n",
        "\n",
        "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: ê¸°ì´ˆ ì „ì²˜ë¦¬\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 í…ìŠ¤íŠ¸ ì •ì œ ê¸°ë³¸\n",
        "\n",
        "í…ìŠ¤íŠ¸ ë¶„ì„ì˜ ì²« ë‹¨ê³„ëŠ” **ë°ì´í„° ì •ì œ**ì…ë‹ˆë‹¤. ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ê¸°ë³¸ ì •ì œ ê¸°ë²•\n",
        "1. **ì†Œë¬¸ì ë³€í™˜**: ëŒ€ì†Œë¬¸ì í†µì¼\n",
        "2. **ê³µë°± ì •ë¦¬**: ì¤‘ë³µ ê³µë°± ì œê±°\n",
        "3. **ì•ë’¤ ê³µë°± ì œê±°**: strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì†Œë¬¸ì: '  hello  world!   welcome to   text  processing.  '\n",
            "strip(): 'hello  world!   welcome to   text  processing.'\n",
            "ìµœì¢…: 'hello world! welcome to text processing.'\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 1] ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ\n",
        "raw_text = \"  Hello  WORLD!   Welcome to   TEXT  Processing.  \"\n",
        "\n",
        "# 1. ì†Œë¬¸ì ë³€í™˜\n",
        "lower_text = raw_text.lower()\n",
        "print(f\"ì†Œë¬¸ì: '{lower_text}'\")\n",
        "\n",
        "# 2. ì•ë’¤ ê³µë°± ì œê±°\n",
        "stripped = lower_text.strip()\n",
        "print(f\"strip(): '{stripped}'\")\n",
        "\n",
        "# 3. ì¤‘ë³µ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ (ì •ê·œí‘œí˜„ì‹)\n",
        "cleaned = re.sub(r'\\s+', ' ', stripped)\n",
        "print(f\"ìµœì¢…: '{cleaned}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: '  ë°°ì†¡ì´ ë„ˆë¬´ ë¹¨ë¼ìš”!!!  '\n",
            "ì •ì œ: 'ë°°ì†¡ì´ ë„ˆë¬´ ë¹¨ë¼ìš”!!!'\n",
            "\n",
            "ì›ë³¸: 'í’ˆì§ˆ   ì¢‹ìŠµë‹ˆë‹¤~~~ ì¶”ì²œí•´ìš”  '\n",
            "ì •ì œ: 'í’ˆì§ˆ ì¢‹ìŠµë‹ˆë‹¤~~~ ì¶”ì²œí•´ìš”'\n",
            "\n",
            "ì›ë³¸: '   ê°€ê²©ëŒ€ë¹„ BEST ìƒí’ˆì…ë‹ˆë‹¤.   '\n",
            "ì •ì œ: 'ê°€ê²©ëŒ€ë¹„ best ìƒí’ˆì…ë‹ˆë‹¤.'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ê³ ê° ë¦¬ë·° ì •ì œ\n",
        "customer_reviews = [\n",
        "    \"  ë°°ì†¡ì´ ë„ˆë¬´ ë¹¨ë¼ìš”!!!  \",\n",
        "    \"í’ˆì§ˆ   ì¢‹ìŠµë‹ˆë‹¤~~~ ì¶”ì²œí•´ìš”  \",\n",
        "    \"   ê°€ê²©ëŒ€ë¹„ BEST ìƒí’ˆì…ë‹ˆë‹¤.   \"\n",
        "]\n",
        "\n",
        "def clean_basic(text):\n",
        "    \"\"\"ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\"\"\"\n",
        "    text = text.lower()              # ì†Œë¬¸ì\n",
        "    text = text.strip()              # ì•ë’¤ ê³µë°±\n",
        "    text = re.sub(r'\\s+', ' ', text) # ì¤‘ë³µ ê³µë°±\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_basic(review) for review in customer_reviews]\n",
        "for original, cleaned in zip(customer_reviews, cleaned_reviews):\n",
        "    print(f\"ì›ë³¸: '{original}'\")\n",
        "    print(f\"ì •ì œ: '{cleaned}'\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.2 ì •ê·œí‘œí˜„ì‹ (Regular Expression)\n",
        "\n",
        "**ì •ê·œí‘œí˜„ì‹**ì€ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ì„ ì°¾ê³  ì¹˜í™˜í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” ë©”íƒ€ë¬¸ì\n",
        "| íŒ¨í„´ | ì˜ë¯¸ | ì˜ˆì‹œ |\n",
        "|------|------|------|\n",
        "| `\\d` | ìˆ«ì | `[0-9]` |\n",
        "| `\\w` | ë‹¨ì–´ ë¬¸ì | `[a-zA-Z0-9_]` |\n",
        "| `\\s` | ê³µë°± ë¬¸ì | ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ |\n",
        "| `.` | ëª¨ë“  ë¬¸ì | ì¤„ë°”ê¿ˆ ì œì™¸ |\n",
        "| `+` | 1ê°œ ì´ìƒ | `a+` â†’ \"a\", \"aa\", \"aaa\" |\n",
        "| `*` | 0ê°œ ì´ìƒ | `a*` â†’ \"\", \"a\", \"aa\" |\n",
        "| `[]` | ë¬¸ì í´ë˜ìŠ¤ | `[abc]` â†’ a, b, c ì¤‘ í•˜ë‚˜ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°©ë²• 1: Hello How are you I'm fine, thank you \n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 2] íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "text = \"Hello! How are you? I'm fine, thank you~ :)\"\n",
        "\n",
        "# ë°©ë²• 1: íŠ¹ì • ë¬¸ìë§Œ ì œê±°\n",
        "clean1 = re.sub(r'[!?~:()]', '', text)\n",
        "print(f\"ë°©ë²• 1: {clean1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8aa996b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°©ë²• 2: Hello How are you Im fine thank you \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ë°©ë²• 2: ì•ŒíŒŒë²³ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "clean2 = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(f\"ë°©ë²• 2: {clean2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "acf02532",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë°©ë²• 3: Price 100 Discount 20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ë°©ë²• 3: ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "text_with_num = \"Price: $100, Discount: 20%!\"\n",
        "clean3 = re.sub(r'[^a-zA-Z0-9\\s]', '', text_with_num)\n",
        "print(f\"ë°©ë²• 3: {clean3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: ë°°ì†¡ì´ ë¹ ë¥´ë„¤ìš”!!! ìµœê³ ì—ìš”~~~ ^^ ë³„ì : 5ì \n",
            "ì •ì œ: ë°°ì†¡ì´ ë¹ ë¥´ë„¤ìš” ìµœê³ ì—ìš”  ë³„ì  5ì \n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 3] í•œê¸€ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "korean_text = \"ë°°ì†¡ì´ ë¹ ë¥´ë„¤ìš”!!! ìµœê³ ì—ìš”~~~ ^^ ë³„ì : 5ì \"\n",
        "\n",
        "# í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "clean_korean = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', korean_text)\n",
        "print(f\"ì›ë³¸: {korean_text}\")\n",
        "print(f\"ì •ì œ: {clean_korean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ë©”ì¼: ['support@company.com', 'sales@shop.co.kr']\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ë‹¤ì–‘í•œ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´\n",
        "\n",
        "# 1. ì´ë©”ì¼ ì¶”ì¶œ\n",
        "text_email = \"ë¬¸ì˜: support@company.com ë˜ëŠ” sales@shop.co.kr\"\n",
        "emails = re.findall(r'[\\w.-]+@[\\w.-]+', text_email)\n",
        "print(f\"ì´ë©”ì¼: {emails}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4a4b21e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „í™”ë²ˆí˜¸: ['010-1234-5678', '02-123-4567']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2. ì „í™”ë²ˆí˜¸ ì¶”ì¶œ\n",
        "text_phone = \"ì—°ë½ì²˜: 010-1234-5678, 02-123-4567\"\n",
        "phones = re.findall(r'\\d{2,3}-\\d{3,4}-\\d{4}', text_phone)\n",
        "print(f\"ì „í™”ë²ˆí˜¸: {phones}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b901e4ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í•´ì‹œíƒœê·¸: ['#ì„œìš¸', '#ë§‘ìŒ', '#ì‚°ì±…', '#ì£¼ë§']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. í•´ì‹œíƒœê·¸ ì¶”ì¶œ\n",
        "text_hashtag = \"ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë‹¤ #ì„œìš¸ #ë§‘ìŒ #ì‚°ì±… #ì£¼ë§\"\n",
        "hashtags = re.findall(r'#\\w+', text_hashtag)\n",
        "print(f\"í•´ì‹œíƒœê·¸: {hashtags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6fc9a111",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL ì œê±°: ìì„¸í•œ ë‚´ìš©ì€  ì°¸ê³ í•˜ì„¸ìš”\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 4. URL ì œê±°\n",
        "text_url = \"ìì„¸í•œ ë‚´ìš©ì€ https://example.com/page ì°¸ê³ í•˜ì„¸ìš”\"\n",
        "clean_url = re.sub(r'https?://\\S+', '', text_url)\n",
        "print(f\"URL ì œê±°: {clean_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: <p>ì•ˆë…•í•˜ì„¸ìš”!</p><br><strong>ì¤‘ìš”í•œ ê³µì§€</strong>ì…ë‹ˆë‹¤.\n",
            "ì •ì œ: ì•ˆë…•í•˜ì„¸ìš”!ì¤‘ìš”í•œ ê³µì§€ì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 4] HTML íƒœê·¸ ì œê±°\n",
        "html_text = \"<p>ì•ˆë…•í•˜ì„¸ìš”!</p><br><strong>ì¤‘ìš”í•œ ê³µì§€</strong>ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "# HTML íƒœê·¸ ì œê±°\n",
        "clean_html = re.sub(r'<[^>]+>', '', html_text)\n",
        "print(f\"ì›ë³¸: {html_text}\")\n",
        "print(f\"ì •ì œ: {clean_html}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.3 í† í°í™” (Tokenization)\n",
        "\n",
        "**í† í°í™”**ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ë‹¨ìœ„(í† í°)ë¡œ ë¶„ë¦¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "### í† í°í™” ì¢…ë¥˜\n",
        "- **ë‹¨ì–´ í† í°í™” (Word Tokenization)**: ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
        "- **ë¬¸ì¥ í† í°í™” (Sentence Tokenization)**: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split(): ['I', 'love', 'natural', 'language', 'processing']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 5] ê¸°ë³¸ í† í°í™” (split)\n",
        "sentence = \"I love natural language processing\"\n",
        "\n",
        "# ê³µë°± ê¸°ì¤€ ë¶„ë¦¬\n",
        "tokens = sentence.split()\n",
        "print(f\"split(): {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹¨ì–´ í† í°: ['Hello', '!', 'I', \"'m\", 'learning', 'NLP', '.', 'It', \"'s\", 'amazing', ',', 'is', \"n't\", 'it', '?']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 6] NLTK í† í°í™” (ë” ì •êµí•¨)\n",
        "text = \"Hello! I'm learning NLP. It's amazing, isn't it?\"\n",
        "\n",
        "# ë‹¨ì–´ í† í°í™”\n",
        "word_tokens = word_tokenize(text)\n",
        "print(f\"ë‹¨ì–´ í† í°: {word_tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "074813e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¬¸ì¥ í† í°: ['Hello!', \"I'm learning NLP.\", \"It's amazing, isn't it?\"]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ë¬¸ì¥ í† í°í™”\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(f\"ë¬¸ì¥ í† í°: {sent_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ 3ê°œ ë¬¸ì¥:\n",
            "  1. ì„œìš¸ì‹œëŠ” ì˜¤ëŠ˜ ìƒˆë¡œìš´ êµí†µ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤.\n",
            "  2. ì´ ì •ì±…ì€ ëŒ€ì¤‘êµí†µ ì´ìš©ì„ ì´‰ì§„í•˜ê¸° ìœ„í•œ ê²ƒì´ë‹¤.\n",
            "  3. ì‹œë¯¼ë“¤ì˜ ë°˜ì‘ì€ ëŒ€ì²´ë¡œ ê¸ì •ì ì´ë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ë‰´ìŠ¤ ê¸°ì‚¬ í† í°í™”\n",
        "news_article = \"\"\"\n",
        "ì„œìš¸ì‹œëŠ” ì˜¤ëŠ˜ ìƒˆë¡œìš´ êµí†µ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤. \n",
        "ì´ ì •ì±…ì€ ëŒ€ì¤‘êµí†µ ì´ìš©ì„ ì´‰ì§„í•˜ê¸° ìœ„í•œ ê²ƒì´ë‹¤.\n",
        "ì‹œë¯¼ë“¤ì˜ ë°˜ì‘ì€ ëŒ€ì²´ë¡œ ê¸ì •ì ì´ë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# ë¬¸ì¥ ë¶„ë¦¬\n",
        "sentences = sent_tokenize(news_article.strip())\n",
        "print(f\"ì´ {len(sentences)}ê°œ ë¬¸ì¥:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"  {i}. {sent}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.4 ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)\n",
        "\n",
        "**ë¶ˆìš©ì–´(Stopwords)**ëŠ” ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.\n",
        "- ì˜ì–´: \"the\", \"is\", \"at\", \"which\", \"on\" ë“±\n",
        "- í•œê¸€: \"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\" ë“±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì˜ì–´ ë¶ˆìš©ì–´ ìˆ˜: 198\n",
            "ì˜ˆì‹œ: ['and', \"hadn't\", 'same', 'myself', 'did', 'just', 'the', 'before', 'once', \"they'd\"]\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 7] ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ì˜ì–´ ë¶ˆìš©ì–´ ëª©ë¡\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "print(f\"ì˜ì–´ ë¶ˆìš©ì–´ ìˆ˜: {len(english_stopwords)}\")\n",
        "print(f\"ì˜ˆì‹œ: {list(english_stopwords)[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸ í† í°: ['this', 'is', 'an', 'example', 'showing', 'how', 'to', 'remove', 'stopwords', 'from', 'text']\n",
            "ë¶ˆìš©ì–´ ì œê±°: ['example', 'showing', 'remove', 'stopwords', 'text']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 8] ë¶ˆìš©ì–´ ì œê±° ì ìš©\n",
        "text = \"This is an example showing how to remove stopwords from text\"\n",
        "\n",
        "# í† í°í™”\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(f\"ì›ë³¸ í† í°: {tokens}\")\n",
        "\n",
        "# ë¶ˆìš©ì–´ ì œê±°\n",
        "filtered_tokens = [token for token in tokens if token not in english_stopwords]\n",
        "print(f\"ë¶ˆìš©ì–´ ì œê±°: {filtered_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: ['the', 'product', 'is', 'very', 'good', 'and', 'the', 'delivery', 'was', 'fast']\n",
            "í•„í„°: ['product', 'good', 'delivery', 'fast']\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€\n",
        "text = \"The product is very good and the delivery was fast\"\n",
        "\n",
        "# ê¸°ë³¸ ë¶ˆìš©ì–´ + ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´\n",
        "custom_stopwords = english_stopwords.union({'very', 'really', 'just'})\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [t for t in tokens if t not in custom_stopwords and t.isalpha()]\n",
        "print(f\"ì›ë³¸: {tokens}\")\n",
        "print(f\"í•„í„°: {filtered}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.5 ì–´ê°„ ì¶”ì¶œ (Stemming) vs í‘œì œì–´ ì¶”ì¶œ (Lemmatization)\n",
        "\n",
        "### ì–´ê°„ ì¶”ì¶œ (Stemming)\n",
        "- ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ì˜ **ì–´ë¯¸ë¥¼ ì œê±°**\n",
        "- ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ\n",
        "- ì˜ˆ: \"running\" â†’ \"run\", \"studies\" â†’ \"studi\"\n",
        "\n",
        "### í‘œì œì–´ ì¶”ì¶œ (Lemmatization)\n",
        "- ì‚¬ì „ì„ ì´ìš©í•´ **ê¸°ë³¸í˜•(lemma)** ì°¾ê¸°\n",
        "- ëŠë¦¬ì§€ë§Œ ì •í™•ë„ ë†’ìŒ\n",
        "- ì˜ˆ: \"running\" â†’ \"run\", \"better\" â†’ \"good\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì–´ê°„ ì¶”ì¶œ ê²°ê³¼:\n",
            "  running      â†’ run\n",
            "  runs         â†’ run\n",
            "  ran          â†’ ran\n",
            "  easily       â†’ easili\n",
            "  fairly       â†’ fairli\n",
            "  studies      â†’ studi\n",
            "  studying     â†’ studi\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 9] ì–´ê°„ ì¶”ì¶œ (Stemming)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['running', 'runs', 'ran', 'easily', 'fairly', 'studies', 'studying']\n",
        "\n",
        "print(\"ì–´ê°„ ì¶”ì¶œ ê²°ê³¼:\")\n",
        "for word in words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"  {word:12} â†’ {stemmed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼:\n",
            "  running      â†’ ë™ì‚¬: run        ëª…ì‚¬: running\n",
            "  runs         â†’ ë™ì‚¬: run        ëª…ì‚¬: run\n",
            "  ran          â†’ ë™ì‚¬: run        ëª…ì‚¬: ran\n",
            "  better       â†’ ë™ì‚¬: better     ëª…ì‚¬: better\n",
            "  studies      â†’ ë™ì‚¬: study      ëª…ì‚¬: study\n",
            "  studying     â†’ ë™ì‚¬: study      ëª…ì‚¬: studying\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 10] í‘œì œì–´ ì¶”ì¶œ (Lemmatization)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'runs', 'ran', 'better', 'studies', 'studying']\n",
        "\n",
        "print(\"í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼:\")\n",
        "for word in words:\n",
        "    # ë™ì‚¬(v)ë¡œ ì²˜ë¦¬\n",
        "    lemma_v = lemmatizer.lemmatize(word, pos='v')\n",
        "    # ëª…ì‚¬(n)ë¡œ ì²˜ë¦¬\n",
        "    lemma_n = lemmatizer.lemmatize(word, pos='n')\n",
        "    print(f\"  {word:12} â†’ ë™ì‚¬: {lemma_v:10} ëª…ì‚¬: {lemma_n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸:     ['the', 'studies', 'showed', 'that', 'cats', 'are', 'running', 'faster', 'than', 'dogs']\n",
            "Stemmed:  ['the', 'studi', 'show', 'that', 'cat', 'are', 'run', 'faster', 'than', 'dog']\n",
            "Lemma:    ['the', 'study', 'show', 'that', 'cat', 'be', 'run', 'faster', 'than', 'dog']\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ ë¹„êµ\n",
        "sentence = \"The studies showed that cats are running faster than dogs\"\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [stemmer.stem(t) for t in tokens]\n",
        "lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
        "\n",
        "print(f\"ì›ë³¸:     {tokens}\")\n",
        "print(f\"Stemmed:  {stemmed}\")\n",
        "print(f\"Lemma:    {lemmatized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: í•œê¸€ ì²˜ë¦¬ (KoNLPy)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 KoNLPy ì†Œê°œ\n",
        "\n",
        "**KoNLPy**ëŠ” í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "### í˜•íƒœì†Œ ë¶„ì„ê¸° ì¢…ë¥˜\n",
        "| ë¶„ì„ê¸° | íŠ¹ì§• | ì†ë„ | ì •í™•ë„ |\n",
        "|--------|------|------|--------|\n",
        "| **Okt** | Twitter ê¸°ë°˜, ê°„í¸í•¨ | ë¹ ë¦„ | ë³´í†µ |\n",
        "| **Komoran** | Java ê¸°ë°˜, ì‚¬ìš©ì ì‚¬ì „ | ë³´í†µ | ë†’ìŒ |\n",
        "| **Mecab** | C ê¸°ë°˜, ê°€ì¥ ë¹ ë¦„ | ë§¤ìš° ë¹ ë¦„ | ë†’ìŒ |\n",
        "| **Kkma** | ì •í™•í•˜ì§€ë§Œ ëŠë¦¼ | ëŠë¦¼ | ë§¤ìš° ë†’ìŒ |\n",
        "| **Hannanum** | KAIST ê°œë°œ | ë³´í†µ | ë³´í†µ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í˜•íƒœì†Œ: ['ì•„ë²„ì§€', 'ê°€ë°©', 'ì—', 'ë“¤ì–´ê°€ì‹ ë‹¤']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 11] Okt í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤\"\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„\n",
        "morphs = okt.morphs(text)\n",
        "print(f\"í˜•íƒœì†Œ: {morphs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a5b1a066",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í’ˆì‚¬ íƒœê¹…: [('ì•„ë²„ì§€', 'Noun'), ('ê°€ë°©', 'Noun'), ('ì—', 'Josa'), ('ë“¤ì–´ê°€ì‹ ë‹¤', 'Verb')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# í’ˆì‚¬ íƒœê¹…\n",
        "pos_tags = okt.pos(text)\n",
        "print(f\"í’ˆì‚¬ íƒœê¹…: {pos_tags}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "98b4fffa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª…ì‚¬: ['ì•„ë²„ì§€', 'ê°€ë°©']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ëª…ì‚¬ ì¶”ì¶œ\n",
        "nouns = okt.nouns(text)\n",
        "print(f\"ëª…ì‚¬: {nouns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í˜•íƒœì†Œ: ['ì½”ë¡œë‚˜', '19', 'ë¡œ', 'ì¸í•´', 'ì¬íƒê·¼ë¬´', 'ê°€', 'ëŠ˜ì–´ë‚˜', 'ì•˜', 'ìŠµë‹ˆë‹¤']\n",
            "í’ˆì‚¬ íƒœê¹…: [('ì½”ë¡œë‚˜', 'NNP'), ('19', 'NNP'), ('ë¡œ', 'JKB'), ('ì¸í•´', 'NNP'), ('ì¬íƒê·¼ë¬´', 'NNP'), ('ê°€', 'JKS'), ('ëŠ˜ì–´ë‚˜', 'VV'), ('ì•˜', 'EP'), ('ìŠµë‹ˆë‹¤', 'EC')]\n",
            "ëª…ì‚¬: ['ì½”ë¡œë‚˜', '19', 'ì¸í•´', 'ì¬íƒê·¼ë¬´']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 12] Komoran í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "\n",
        "text = \"ì½”ë¡œë‚˜19ë¡œ ì¸í•´ ì¬íƒê·¼ë¬´ê°€ ëŠ˜ì–´ë‚¬ìŠµë‹ˆë‹¤\"\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„\n",
        "morphs = komoran.morphs(text)\n",
        "print(f\"í˜•íƒœì†Œ: {morphs}\")\n",
        "\n",
        "# í’ˆì‚¬ íƒœê¹…\n",
        "pos_tags = komoran.pos(text)\n",
        "print(f\"í’ˆì‚¬ íƒœê¹…: {pos_tags}\")\n",
        "\n",
        "# ëª…ì‚¬ ì¶”ì¶œ\n",
        "nouns = komoran.nouns(text)\n",
        "print(f\"ëª…ì‚¬: {nouns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.2 í˜•íƒœì†Œ ë¶„ì„ (Morphological Analysis)\n",
        "\n",
        "**í˜•íƒœì†Œ**ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ ê°€ì¥ ì‘ì€ ì–¸ì–´ ë‹¨ìœ„ì…ë‹ˆë‹¤.\n",
        "- \"ë¨¹ì—ˆìŠµë‹ˆë‹¤\" â†’ \"ë¨¹\" + \"ì—ˆ\" + \"ìŠµë‹ˆë‹¤\"\n",
        "- \"ìì—°ì–´ì²˜ë¦¬\" â†’ \"ìì—°ì–´\" + \"ì²˜ë¦¬\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ì›ë¬¸: ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤\n",
            "í˜•íƒœì†Œ: ['ë§›ìˆëŠ”', 'ìŒì‹', 'ì„', 'ë¨¹ì—ˆìŠµë‹ˆë‹¤']\n",
            "í’ˆì‚¬: [('ë§›ìˆëŠ”', 'Adjective'), ('ìŒì‹', 'Noun'), ('ì„', 'Josa'), ('ë¨¹ì—ˆìŠµë‹ˆë‹¤', 'Verb')]\n",
            "\n",
            "ì›ë¬¸: ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ KTXë¡œ 2ì‹œê°„ ê±¸ë¦½ë‹ˆë‹¤\n",
            "í˜•íƒœì†Œ: ['ì„œìš¸', 'ì—ì„œ', 'ë¶€ì‚°', 'ê¹Œì§€', 'KTX', 'ë¡œ', '2ì‹œê°„', 'ê±¸ë¦½ë‹ˆë‹¤']\n",
            "í’ˆì‚¬: [('ì„œìš¸', 'Noun'), ('ì—ì„œ', 'Josa'), ('ë¶€ì‚°', 'Noun'), ('ê¹Œì§€', 'Josa'), ('KTX', 'Alpha'), ('ë¡œ', 'Noun'), ('2ì‹œê°„', 'Number'), ('ê±¸ë¦½ë‹ˆë‹¤', 'Verb')]\n",
            "\n",
            "ì›ë¬¸: ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”\n",
            "í˜•íƒœì†Œ: ['ì˜¤ëŠ˜', 'ë‚ ì”¨', 'ê°€', 'ì •ë§', 'ì¢‹ë„¤ìš”']\n",
            "í’ˆì‚¬: [('ì˜¤ëŠ˜', 'Noun'), ('ë‚ ì”¨', 'Noun'), ('ê°€', 'Josa'), ('ì •ë§', 'Noun'), ('ì¢‹ë„¤ìš”', 'Adjective')]\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 13] í˜•íƒœì†Œ ë¶„ì„ ìƒì„¸\n",
        "okt = Okt()\n",
        "\n",
        "sentences = [\n",
        "    \"ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤\",\n",
        "    \"ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ KTXë¡œ 2ì‹œê°„ ê±¸ë¦½ë‹ˆë‹¤\",\n",
        "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”\"\n",
        "]\n",
        "\n",
        "for sent in sentences:\n",
        "    print(f\"\\nì›ë¬¸: {sent}\")\n",
        "    print(f\"í˜•íƒœì†Œ: {okt.morphs(sent)}\")\n",
        "    print(f\"í’ˆì‚¬: {okt.pos(sent)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë¬¸: ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆì´ ì¢‹ì•„ìš”\n",
            "ë¶„ì„: ['ë°°ì†¡', 'ì´', 'ë¹ ë¥´ë‹¤', 'ìƒí’ˆ', 'í’ˆì§ˆ', 'ì´', 'ì¢‹ë‹¤']\n",
            "\n",
            "ì›ë¬¸: ê°€ê²©ì€ ë¹„ì‹¸ì§€ë§Œ ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤\n",
            "ë¶„ì„: ['ê°€ê²©', 'ì€', 'ë¹„ì‹¸ë‹¤', 'ì„±ëŠ¥', 'ì´', 'ë›°ì–´ë‚˜ë‹¤']\n",
            "\n",
            "ì›ë¬¸: í¬ì¥ì´ ê¼¼ê¼¼í•˜ê²Œ ì˜ ë˜ì–´ìˆì—ˆì–´ìš”\n",
            "ë¶„ì„: ['í¬ì¥', 'ì´', 'ê¼¼ê¼¼í•˜ë‹¤', 'ìë‹¤', 'ë˜ì–´ë‹¤']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ê³ ê° ë¦¬ë·° í˜•íƒœì†Œ ë¶„ì„\n",
        "reviews = [\n",
        "    \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆì´ ì¢‹ì•„ìš”\",\n",
        "    \"ê°€ê²©ì€ ë¹„ì‹¸ì§€ë§Œ ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤\",\n",
        "    \"í¬ì¥ì´ ê¼¼ê¼¼í•˜ê²Œ ì˜ ë˜ì–´ìˆì—ˆì–´ìš”\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "for review in reviews:\n",
        "    # í˜•íƒœì†Œ ë¶„ì„ (ì •ê·œí™” + ì–´ê°„ ì¶”ì¶œ)\n",
        "    morphs = okt.morphs(review, norm=True, stem=True)\n",
        "    print(f\"ì›ë¬¸: {review}\")\n",
        "    print(f\"ë¶„ì„: {morphs}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.3 í’ˆì‚¬ íƒœê¹… (POS Tagging)\n",
        "\n",
        "**í’ˆì‚¬ íƒœê¹…**ì€ ê° í˜•íƒœì†Œì— í’ˆì‚¬ ì •ë³´ë¥¼ ë¶€ì—¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "### Okt ì£¼ìš” í’ˆì‚¬ íƒœê·¸\n",
        "| íƒœê·¸ | í’ˆì‚¬ | ì˜ˆì‹œ |\n",
        "|------|------|------|\n",
        "| Noun | ëª…ì‚¬ | ì‚¬ëŒ, ì»´í“¨í„° |\n",
        "| Verb | ë™ì‚¬ | ê°€ë‹¤, ë¨¹ë‹¤ |\n",
        "| Adjective | í˜•ìš©ì‚¬ | ì¢‹ë‹¤, í¬ë‹¤ |\n",
        "| Adverb | ë¶€ì‚¬ | ë§¤ìš°, ë¹¨ë¦¬ |\n",
        "| Josa | ì¡°ì‚¬ | ì€, ëŠ”, ì´, ê°€ |\n",
        "| Eomi | ì–´ë¯¸ | ~ë‹¤, ~ìš” |\n",
        "| Punctuation | êµ¬ë‘ì  | ., !, ? |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „ì²´: [('ì„œìš¸', 'Noun'), ('ì€', 'Josa'), ('ëŒ€í•œë¯¼êµ­', 'Noun'), ('ì˜', 'Josa'), ('ìˆ˜ë„', 'Noun'), ('ì´ë©°', 'Josa'), ('ë§¤ìš°', 'Noun'), ('ì•„ë¦„ë‹¤ìš´', 'Adjective'), ('ë„ì‹œ', 'Noun'), ('ì…ë‹ˆë‹¤', 'Adjective')]\n",
            "\n",
            "ëª…ì‚¬: ['ì„œìš¸', 'ëŒ€í•œë¯¼êµ­', 'ìˆ˜ë„', 'ë§¤ìš°', 'ë„ì‹œ']\n",
            "ë™ì‚¬: []\n",
            "í˜•ìš©ì‚¬: ['ì•„ë¦„ë‹¤ìš´', 'ì…ë‹ˆë‹¤']\n",
            "ë¶€ì‚¬: []\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 14] í’ˆì‚¬ë³„ ë‹¨ì–´ ì¶”ì¶œ\n",
        "okt = Okt()\n",
        "\n",
        "text = \"ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë©° ë§¤ìš° ì•„ë¦„ë‹¤ìš´ ë„ì‹œì…ë‹ˆë‹¤\"\n",
        "\n",
        "# ì „ì²´ í’ˆì‚¬ íƒœê¹…\n",
        "pos_result = okt.pos(text)\n",
        "print(f\"ì „ì²´: {pos_result}\")\n",
        "\n",
        "# í’ˆì‚¬ë³„ ë¶„ë¥˜\n",
        "nouns = [word for word, pos in pos_result if pos == 'Noun']\n",
        "verbs = [word for word, pos in pos_result if pos == 'Verb']\n",
        "adjectives = [word for word, pos in pos_result if pos == 'Adjective']\n",
        "adverbs = [word for word, pos in pos_result if pos == 'Adverb']\n",
        "\n",
        "print(f\"\\nëª…ì‚¬: {nouns}\")\n",
        "print(f\"ë™ì‚¬: {verbs}\")\n",
        "print(f\"í˜•ìš©ì‚¬: {adjectives}\")\n",
        "print(f\"ë¶€ì‚¬: {adverbs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¦¬ë·°: ì´ ì œí’ˆì€ ì •ë§ ì¢‹ê³  í¸ë¦¬í•´ìš”\n",
            "í˜•ìš©ì‚¬: ['ì¢‹ë‹¤', 'í¸ë¦¬í•˜ë‹¤']\n",
            "\n",
            "ë¦¬ë·°: ë°°ì†¡ì€ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì´ ë³„ë¡œì˜ˆìš”\n",
            "í˜•ìš©ì‚¬: ['ë¹ ë¥´ë‹¤']\n",
            "\n",
            "ë¦¬ë·°: ê°€ê²©ì´ ì €ë ´í•˜ê³  ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤\n",
            "í˜•ìš©ì‚¬: ['ì €ë ´í•˜ë‹¤', 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: ê°ì„± ë¶„ì„ìš© í˜•ìš©ì‚¬ ì¶”ì¶œ\n",
        "reviews = [\n",
        "    \"ì´ ì œí’ˆì€ ì •ë§ ì¢‹ê³  í¸ë¦¬í•´ìš”\",\n",
        "    \"ë°°ì†¡ì€ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì´ ë³„ë¡œì˜ˆìš”\",\n",
        "    \"ê°€ê²©ì´ ì €ë ´í•˜ê³  ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "for review in reviews:\n",
        "    pos_result = okt.pos(review, stem=True)  # ì–´ê°„ ì¶”ì¶œ ì ìš©\n",
        "    \n",
        "    adjectives = [word for word, pos in pos_result if pos == 'Adjective']\n",
        "    \n",
        "    print(f\"ë¦¬ë·°: {review}\")\n",
        "    print(f\"í˜•ìš©ì‚¬: {adjectives}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.4 ëª…ì‚¬ ì¶”ì¶œ\n",
        "\n",
        "**ëª…ì‚¬ ì¶”ì¶œ**ì€ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì°¾ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì¶”ì¶œëœ ëª…ì‚¬: ['ì „ì', 'ìŠ¤ë§ˆíŠ¸í°', 'ì¶œì‹œ', 'ì´ë²ˆ', 'ìŠ¤ë§ˆíŠ¸í°', 'ì¹´ë©”ë¼', 'ì„±ëŠ¥', 'í¬ê²Œ', 'í–¥ìƒ', 'ì‚¼ì„±', 'ì „ì', 'ê´€ê³„ì', 'ì´ë²ˆ', 'ì œí’ˆ', 'ì‹œì¥', 'ë°˜ì‘', 'ê²ƒ', 'ì• í”Œ', 'ê²½ìŸ', 'ìš°ìœ„', 'ì ', 'ê²ƒ', 'ì˜ˆìƒ']\n",
            "\n",
            "ìƒìœ„ 5ê°œ ëª…ì‚¬:\n",
            "  ì „ì: 2íšŒ\n",
            "  ìŠ¤ë§ˆíŠ¸í°: 2íšŒ\n",
            "  ì´ë²ˆ: 2íšŒ\n",
            "  ê²ƒ: 2íšŒ\n",
            "  ì¶œì‹œ: 1íšŒ\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 15] ëª…ì‚¬ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„\n",
        "from collections import Counter\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "news = \"\"\"\n",
        "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸í°ì„ ì¶œì‹œí–ˆë‹¤. \n",
        "ì´ë²ˆ ìŠ¤ë§ˆíŠ¸í°ì€ ì¹´ë©”ë¼ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆë‹¤.\n",
        "ì‚¼ì„±ì „ì ê´€ê³„ìëŠ” ì´ë²ˆ ì œí’ˆì´ ì‹œì¥ì—ì„œ í° ë°˜ì‘ì„ ì–»ì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤ê³  ë°í˜”ë‹¤.\n",
        "ì• í”Œê³¼ì˜ ê²½ìŸì—ì„œë„ ìš°ìœ„ë¥¼ ì í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# ëª…ì‚¬ ì¶”ì¶œ\n",
        "nouns = okt.nouns(news)\n",
        "print(f\"ì¶”ì¶œëœ ëª…ì‚¬: {nouns}\")\n",
        "\n",
        "# ë¹ˆë„ ë¶„ì„\n",
        "noun_counts = Counter(nouns)\n",
        "print(f\"\\nìƒìœ„ 5ê°œ ëª…ì‚¬:\")\n",
        "for noun, count in noun_counts.most_common(5):\n",
        "    print(f\"  {noun}: {count}íšŒ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì£¼ìš” í‚¤ì›Œë“œ:\n",
            "  ì˜í™”: 3íšŒ\n",
            "  ë°°ìš°: 2íšŒ\n",
            "  ìŠ¤í† ë¦¬: 2íšŒ\n",
            "  ê°ë™: 2íšŒ\n",
            "  ì§„ì§œ: 1íšŒ\n",
            "  ì—°ê¸°: 1íšŒ\n",
            "  ìµœê³ : 1íšŒ\n",
            "  ì˜ìƒ: 1íšŒ\n",
            "  ë¯¸ê°€: 1íšŒ\n",
            "  ì—°ê¸°ë ¥: 1íšŒ\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¡ ì‹¤ë¬´ ì˜ˆì‹œ: SNS ëŒ“ê¸€ í‚¤ì›Œë“œ ë¶„ì„\n",
        "comments = [\n",
        "    \"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì–´ìš” ë°°ìš° ì—°ê¸° ìµœê³ \",\n",
        "    \"ìŠ¤í† ë¦¬ê°€ íƒ„íƒ„í•˜ê³  ì˜ìƒë¯¸ê°€ ë›°ì–´ë‚¨\",\n",
        "    \"ë°°ìš°ë“¤ ì—°ê¸°ë ¥ì´ ëŒ€ë‹¨í•´ìš” ê°ë™ ë°›ìŒ\",\n",
        "    \"ì˜í™” OSTë„ ì¢‹ê³  ìŠ¤í† ë¦¬ë„ ê°ë™ì \",\n",
        "    \"ì´ë²ˆ ì£¼ë§ì— ê°€ì¡±ì´ë‘ ì˜í™” ë³´ëŸ¬ ê°€ì•¼ê² ì–´ìš”\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# ëª¨ë“  ëŒ“ê¸€ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ\n",
        "all_nouns = []\n",
        "for comment in comments:\n",
        "    nouns = okt.nouns(comment)\n",
        "    all_nouns.extend(nouns)\n",
        "\n",
        "# 1ê¸€ì ëª…ì‚¬ ì œì™¸ (ë…¸ì´ì¦ˆ ì œê±°)\n",
        "filtered_nouns = [n for n in all_nouns if len(n) > 1]\n",
        "\n",
        "# í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„\n",
        "keyword_counts = Counter(filtered_nouns)\n",
        "print(\"ì£¼ìš” í‚¤ì›Œë“œ:\")\n",
        "for keyword, count in keyword_counts.most_common(10):\n",
        "    print(f\"  {keyword}: {count}íšŒ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2.5 ì¢…í•©: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "ì‹¤ë¬´ì—ì„œëŠ” ì—¬ëŸ¬ ì „ì²˜ë¦¬ ê¸°ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: ì˜¤ëŠ˜ ë°°ì†¡ë°›ì€ ìƒí’ˆì´ ì •ë§ ì¢‹ì•„ìš”!!! í’ˆì§ˆë„ ìµœê³ ì…ë‹ˆë‹¤~~ ê°•ë ¥ ì¶”ì²œí•´ìš” ^^;;;\n",
            "ê²°ê³¼: ['ì˜¤ëŠ˜', 'ë°°ì†¡', 'ë°›ë‹¤', 'ìƒí’ˆ', 'ì •ë§', 'ì¢‹ë‹¤', 'í’ˆì§ˆ', 'ìµœê³ ', 'ì´ë‹¤', 'ê°•ë ¥', 'ì¶”ì²œ']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 16] í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "def preprocess_korean(text, min_length=2):\n",
        "    \"\"\"\n",
        "    í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "    \n",
        "    Args:\n",
        "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "        min_length (int): ìµœì†Œ ë‹¨ì–´ ê¸¸ì´\n",
        "    \n",
        "    Returns:\n",
        "        list: ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    okt = Okt()\n",
        "    \n",
        "    # 1. íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)\n",
        "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # 2. ì¤‘ë³µ ê³µë°± ì œê±°\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 3. í˜•íƒœì†Œ ë¶„ì„ (ì •ê·œí™” + ì–´ê°„ ì¶”ì¶œ)\n",
        "    tokens = okt.morphs(text, norm=True, stem=True)\n",
        "    \n",
        "    # 4. ë¶ˆìš©ì–´ ì œê±° (í•œê¸€ ì¡°ì‚¬, ì–´ë¯¸ ë“±)\n",
        "    korean_stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', \n",
        "                        'ë¡œ', 'ìœ¼ë¡œ', 'ë„', 'ë§Œ', 'ê³¼', 'ì™€', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤'}\n",
        "    tokens = [t for t in tokens if t not in korean_stopwords]\n",
        "    \n",
        "    # 5. ìµœì†Œ ê¸¸ì´ í•„í„°ë§\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "sample_text = \"ì˜¤ëŠ˜ ë°°ì†¡ë°›ì€ ìƒí’ˆì´ ì •ë§ ì¢‹ì•„ìš”!!! í’ˆì§ˆë„ ìµœê³ ì…ë‹ˆë‹¤~~ ê°•ë ¥ ì¶”ì²œí•´ìš” ^^;;;\"\n",
        "result = preprocess_korean(sample_text)\n",
        "\n",
        "print(f\"ì›ë³¸: {sample_text}\")\n",
        "print(f\"ê²°ê³¼: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì›ë³¸: I was running in the park yesterday! The weather was amazing.\n",
            "ê²°ê³¼: ['run', 'park', 'yesterday', 'weather', 'amaze']\n"
          ]
        }
      ],
      "source": [
        "# [ì˜ˆì œ 17] ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "def preprocess_english(text, min_length=2):\n",
        "    \"\"\"\n",
        "    ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "    \n",
        "    Args:\n",
        "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "        min_length (int): ìµœì†Œ ë‹¨ì–´ ê¸¸ì´\n",
        "    \n",
        "    Returns:\n",
        "        list: ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    # 1. ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    \n",
        "    # 3. ì¤‘ë³µ ê³µë°± ì œê±°\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 4. í† í°í™”\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # 5. ë¶ˆìš©ì–´ ì œê±°\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    \n",
        "    # 6. í‘œì œì–´ ì¶”ì¶œ\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
        "    \n",
        "    # 7. ìµœì†Œ ê¸¸ì´ í•„í„°ë§\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "sample_text = \"I was running in the park yesterday! The weather was amazing.\"\n",
        "result = preprocess_english(sample_text)\n",
        "\n",
        "print(f\"ì›ë³¸: {sample_text}\")\n",
        "print(f\"ê²°ê³¼: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ ì‹¤ìŠµ í€´ì¦ˆ\n",
        "\n",
        "**ë‚œì´ë„**: â­ (ì‰¬ì›€) ~ â­â­â­â­â­ (ì–´ë ¤ì›€)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ â­\n",
        "\n",
        "**ë¬¸ì œ**: ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- ì†Œë¬¸ì ë³€í™˜\n",
        "- ì•ë’¤ ê³µë°± ì œê±°\n",
        "- ì¤‘ë³µ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "```python\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `\"hello world! welcome to nlp.\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. ì •ê·œí‘œí˜„ì‹ - íŠ¹ìˆ˜ë¬¸ì ì œê±° â­\n",
        "\n",
        "**ë¬¸ì œ**: ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "- ë‚˜ë¨¸ì§€ ë¬¸ìëŠ” ëª¨ë‘ ì œê±°\n",
        "\n",
        "```python\n",
        "text = \"ë°°ì†¡ ë¹ ë¥´ë„¤ìš”!!! ê°€ê²© 10,000ì› @#$% ì¶”ì²œí•´ìš”~~\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `\"ë°°ì†¡ ë¹ ë¥´ë„¤ìš” ê°€ê²© 10000ì›  ì¶”ì²œí•´ìš”\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "text = \"ë°°ì†¡ ë¹ ë¥´ë„¤ìš”!!! ê°€ê²© 10,000ì› @#$% ì¶”ì²œí•´ìš”~~\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. ì •ê·œí‘œí˜„ì‹ - í•´ì‹œíƒœê·¸ ì¶”ì¶œ â­â­\n",
        "\n",
        "**ë¬¸ì œ**: SNS ê²Œì‹œë¬¼ì—ì„œ ëª¨ë“  í•´ì‹œíƒœê·¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- `re.findall()` ì‚¬ìš©\n",
        "- í•´ì‹œíƒœê·¸ í˜•ì‹: `#ë‹¨ì–´`\n",
        "\n",
        "```python\n",
        "post = \"ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë‹¤ #ì„œìš¸ #ë§‘ìŒ #ì£¼ë§ë‚˜ë“¤ì´ ì ì‹¬ì€ #ë§›ì§‘ ê°€ì•¼ì§€\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `['#ì„œìš¸', '#ë§‘ìŒ', '#ì£¼ë§ë‚˜ë“¤ì´', '#ë§›ì§‘']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "post = \"ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë‹¤ #ì„œìš¸ #ë§‘ìŒ #ì£¼ë§ë‚˜ë“¤ì´ ì ì‹¬ì€ #ë§›ì§‘ ê°€ì•¼ì§€\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. NLTK í† í°í™” â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ì˜ì–´ ë¬¸ì¥ì„ ë‹¨ì–´ì™€ ë¬¸ì¥ìœ¼ë¡œ í† í°í™”í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- `word_tokenize()`: ë‹¨ì–´ í† í°í™”\n",
        "- `sent_tokenize()`: ë¬¸ì¥ í† í°í™”\n",
        "\n",
        "```python\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**:\n",
        "- ë‹¨ì–´: `['Hello', '!', 'How', 'are', 'you', '?', ...]`\n",
        "- ë¬¸ì¥: `['Hello!', 'How are you?', ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. ë¶ˆìš©ì–´ ì œê±° â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ì˜ì–´ ë¬¸ì¥ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- ì†Œë¬¸ì ë³€í™˜ í›„ í† í°í™”\n",
        "- NLTK ë¶ˆìš©ì–´ ì œê±°\n",
        "- ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ë§Œ ìœ ì§€\n",
        "\n",
        "```python\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `['movie', 'really', 'amazing', 'loved', 'much']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ì–´ê°„ ì¶”ì¶œê³¼ í‘œì œì–´ ì¶”ì¶œì„ ê°ê° ì ìš©í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- `PorterStemmer` ì‚¬ìš©\n",
        "- `WordNetLemmatizer` ì‚¬ìš© (ë™ì‚¬ í’ˆì‚¬)\n",
        "- ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜: `{ì›ë³¸: (stemmed, lemmatized)}`\n",
        "\n",
        "```python\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: \n",
        "```\n",
        "running: stem=run, lemma=run\n",
        "studies: stem=studi, lemma=study\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. KoNLPy ëª…ì‚¬ ì¶”ì¶œ â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ê³ ê° ë¦¬ë·°ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- `Okt.nouns()` ì‚¬ìš©\n",
        "- 1ê¸€ì ëª…ì‚¬ ì œì™¸\n",
        "- ìƒìœ„ 5ê°œ ëª…ì‚¬ ì¶œë ¥\n",
        "\n",
        "```python\n",
        "reviews = [\n",
        "    \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆì´ ì¢‹ì•„ìš”\",\n",
        "    \"ê°€ê²©ëŒ€ë¹„ í’ˆì§ˆì´ í›Œë¥­í•©ë‹ˆë‹¤\",\n",
        "    \"ë°°ì†¡ì€ ë¹ ë¥¸ë° í¬ì¥ì´ ì•„ì‰¬ì›Œìš”\",\n",
        "    \"ìƒí’ˆ í€„ë¦¬í‹° ìµœê³  ë°°ì†¡ë„ ë¹ ë¦„\"\n",
        "]\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**:\n",
        "```\n",
        "ë°°ì†¡: 3íšŒ\n",
        "í’ˆì§ˆ: 2íšŒ\n",
        "ìƒí’ˆ: 2íšŒ\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "reviews = [\n",
        "    \"ë°°ì†¡ì´ ë¹ ë¥´ê³  ìƒí’ˆ í’ˆì§ˆì´ ì¢‹ì•„ìš”\",\n",
        "    \"ê°€ê²©ëŒ€ë¹„ í’ˆì§ˆì´ í›Œë¥­í•©ë‹ˆë‹¤\",\n",
        "    \"ë°°ì†¡ì€ ë¹ ë¥¸ë° í¬ì¥ì´ ì•„ì‰¬ì›Œìš”\",\n",
        "    \"ìƒí’ˆ í€„ë¦¬í‹° ìµœê³  ë°°ì†¡ë„ ë¹ ë¦„\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. í’ˆì‚¬ë³„ ë‹¨ì–´ ë¶„ë¥˜ â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ë¬¸ì¥ì—ì„œ í’ˆì‚¬ë³„ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- `Okt.pos()` ì‚¬ìš©\n",
        "- ëª…ì‚¬(Noun), ë™ì‚¬(Verb), í˜•ìš©ì‚¬(Adjective) ë¶„ë¥˜\n",
        "- ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜\n",
        "\n",
        "```python\n",
        "text = \"ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ê³  í–‰ë³µí•œ í•˜ë£¨ë¥¼ ë³´ëƒˆë‹¤\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**:\n",
        "```python\n",
        "{\n",
        "    'Noun': ['ìŒì‹', 'í•˜ë£¨'],\n",
        "    'Verb': ['ë¨¹ë‹¤', 'ë³´ë‚´ë‹¤'],\n",
        "    'Adjective': ['ë§›ìˆë‹¤', 'í–‰ë³µí•˜ë‹¤']\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "text = \"ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ê³  í–‰ë³µí•œ í•˜ë£¨ë¥¼ ë³´ëƒˆë‹¤\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. í•œê¸€ ì „ì²˜ë¦¬ í•¨ìˆ˜ â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- í•¨ìˆ˜ëª…: `clean_korean_text`\n",
        "- ê¸°ëŠ¥:\n",
        "  1. íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)\n",
        "  2. ì¤‘ë³µ ê³µë°± ì œê±°\n",
        "  3. í˜•íƒœì†Œ ë¶„ì„ (ì •ê·œí™”, ì–´ê°„ ì¶”ì¶œ)\n",
        "  4. ë¶ˆìš©ì–´ ì œê±° (ì¡°ì‚¬, ì–´ë¯¸)\n",
        "  5. 2ê¸€ì ì´ìƒ í† í°ë§Œ ìœ ì§€\n",
        "- ë°˜í™˜: í† í° ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "```python\n",
        "text = \"ì˜¤ëŠ˜ ë°°ì†¡ë°›ì€ ìƒí’ˆì´ ì •ë§ ì¢‹ì•„ìš”!!! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤~~\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `['ì˜¤ëŠ˜', 'ë°°ì†¡', 'ë°›ë‹¤', 'ìƒí’ˆ', 'ì •ë§', 'ì¢‹ë‹¤', 'ê°•ë ¥', 'ì¶”ì²œ']` (ìœ ì‚¬í•œ ê²°ê³¼)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "text = \"ì˜¤ëŠ˜ ë°°ì†¡ë°›ì€ ìƒí’ˆì´ ì •ë§ ì¢‹ì•„ìš”!!! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤~~\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. ì¢…í•©: ë‰´ìŠ¤ ê¸°ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ â­â­â­â­â­\n",
        "\n",
        "**ë¬¸ì œ**: ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•˜ì„¸ìš”.\n",
        "\n",
        "**ìš”êµ¬ì‚¬í•­**:\n",
        "- í•¨ìˆ˜ëª…: `extract_keywords`\n",
        "- ì…ë ¥: ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸\n",
        "- ì²˜ë¦¬ ê³¼ì •:\n",
        "  1. íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "  2. í˜•íƒœì†Œ ë¶„ì„ (Okt)\n",
        "  3. ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
        "  4. 1ê¸€ì ëª…ì‚¬ ì œì™¸\n",
        "  5. ë¹ˆë„ ë¶„ì„\n",
        "- ë°˜í™˜: ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ `[(í‚¤ì›Œë“œ, ë¹ˆë„), ...]`\n",
        "\n",
        "```python\n",
        "news = \"\"\"\n",
        "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°ì„ ì¶œì‹œí–ˆë‹¤.\n",
        "ì´ë²ˆ ê°¤ëŸ­ì‹œëŠ” ì¹´ë©”ë¼ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìœ¼ë©°,\n",
        "ë°°í„°ë¦¬ ìš©ëŸ‰ë„ ëŠ˜ì–´ë‚¬ë‹¤. ì‚¼ì„±ì „ì ê´€ê³„ìëŠ”\n",
        "\"ì´ë²ˆ ê°¤ëŸ­ì‹œê°€ ì‹œì¥ì—ì„œ ì¢‹ì€ ë°˜ì‘ì„ ì–»ì„ ê²ƒ\"ì´ë¼ê³  ë°í˜”ë‹¤.\n",
        "ê²½ìŸì‚¬ì¸ ì• í”Œì˜ ì•„ì´í°ê³¼ì˜ ê²½ìŸë„ ì¹˜ì—´í•´ì§ˆ ì „ë§ì´ë‹¤.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**ê¸°ëŒ€ ê²°ê³¼**: `[('ê°¤ëŸ­ì‹œ', 3), ('ì‚¼ì„±ì „ì', 2), ('ìŠ¤ë§ˆíŠ¸í°', 1), ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
        "news = \"\"\"\n",
        "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°ì„ ì¶œì‹œí–ˆë‹¤.\n",
        "ì´ë²ˆ ê°¤ëŸ­ì‹œëŠ” ì¹´ë©”ë¼ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìœ¼ë©°,\n",
        "ë°°í„°ë¦¬ ìš©ëŸ‰ë„ ëŠ˜ì–´ë‚¬ë‹¤. ì‚¼ì„±ì „ì ê´€ê³„ìëŠ”\n",
        "\"ì´ë²ˆ ê°¤ëŸ­ì‹œê°€ ì‹œì¥ì—ì„œ ì¢‹ì€ ë°˜ì‘ì„ ì–»ì„ ê²ƒ\"ì´ë¼ê³  ë°í˜”ë‹¤.\n",
        "ê²½ìŸì‚¬ì¸ ì• í”Œì˜ ì•„ì´í°ê³¼ì˜ ê²½ìŸë„ ì¹˜ì—´í•´ì§ˆ ì „ë§ì´ë‹¤.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š í•™ìŠµ ì •ë¦¬\n",
        "\n",
        "### Part 1: ê¸°ì´ˆ ì „ì²˜ë¦¬ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê¸°ë²• | ëª©ì  | ì£¼ìš” í•¨ìˆ˜/íŒ¨í„´ |\n",
        "|------|------|---------------|\n",
        "| ì†Œë¬¸ì ë³€í™˜ | ëŒ€ì†Œë¬¸ì í†µì¼ | `text.lower()` |\n",
        "| ê³µë°± ì •ë¦¬ | ë…¸ì´ì¦ˆ ì œê±° | `strip()`, `re.sub(r'\\s+', ' ')` |\n",
        "| íŠ¹ìˆ˜ë¬¸ì ì œê±° | íŒ¨í„´ ê¸°ë°˜ ì •ì œ | `re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '')` |\n",
        "| í† í°í™” | ë¶„ì„ ë‹¨ìœ„ ë¶„ë¦¬ | `word_tokenize()`, `sent_tokenize()` |\n",
        "| ë¶ˆìš©ì–´ ì œê±° | ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì œê±° | `stopwords.words('english')` |\n",
        "| ì–´ê°„ ì¶”ì¶œ | ë‹¨ì–´ ì •ê·œí™” | `PorterStemmer()` |\n",
        "| í‘œì œì–´ ì¶”ì¶œ | ì‚¬ì „ ê¸°ë°˜ ì •ê·œí™” | `WordNetLemmatizer()` |\n",
        "\n",
        "### Part 2: í•œê¸€ ì²˜ë¦¬ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "| ê¸°ë²• | ëª©ì  | ì£¼ìš” í•¨ìˆ˜ |\n",
        "|------|------|----------|\n",
        "| í˜•íƒœì†Œ ë¶„ì„ | í˜•íƒœì†Œ ë¶„ë¦¬ | `okt.morphs()` |\n",
        "| í’ˆì‚¬ íƒœê¹… | í’ˆì‚¬ ë¶€ì—¬ | `okt.pos()` |\n",
        "| ëª…ì‚¬ ì¶”ì¶œ | í‚¤ì›Œë“œ ì¶”ì¶œ | `okt.nouns()` |\n",
        "| ì •ê·œí™” | ë¹„í‘œì¤€ í‘œí˜„ ì •ì œ | `okt.morphs(norm=True)` |\n",
        "| ì–´ê°„ ì¶”ì¶œ | ì›í˜• ë³µì› | `okt.morphs(stem=True)` |\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ íŒ\n",
        "\n",
        "1. **ì „ì²˜ë¦¬ ìˆœì„œê°€ ì¤‘ìš”**: íŠ¹ìˆ˜ë¬¸ì ì œê±° â†’ í† í°í™” â†’ ë¶ˆìš©ì–´ ì œê±° â†’ ì •ê·œí™”\n",
        "2. **í•œê¸€ì€ í˜•íƒœì†Œ ë¶„ì„ì´ í•„ìˆ˜**: ë„ì–´ì“°ê¸°ë§Œìœ¼ë¡œëŠ” ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ ë¶ˆê°€\n",
        "3. **ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ëª©ë¡ ê´€ë¦¬**: ë„ë©”ì¸ë³„ë¡œ ì¶”ê°€/ì œê±°í•  ë‹¨ì–´ê°€ ë‹¤ë¦„\n",
        "4. **ì •ê·œí™” vs ì–´ê°„ ì¶”ì¶œ**: ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤\n",
        "5. **ë¶„ì„ ëª©ì ì— ë”°ë¥¸ í’ˆì‚¬ ì„ íƒ**: ê°ì„± ë¶„ì„ì€ í˜•ìš©ì‚¬, í‚¤ì›Œë“œ ë¶„ì„ì€ ëª…ì‚¬\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒì€ **í…ìŠ¤íŠ¸ ë¶„ì„ ê¸°ë²•**ì…ë‹ˆë‹¤.\n",
        "\n",
        "**ë‹¤ìŒ ë…¸íŠ¸ë¶**: `Day11_1_í…ìŠ¤íŠ¸_ë¶„ì„_ê¸°ë²•_Text_Analysis.ipynb`\n",
        "- ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (Word Frequency)\n",
        "- TF-IDF ë²¡í„°í™”\n",
        "- ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™”\n",
        "- ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
