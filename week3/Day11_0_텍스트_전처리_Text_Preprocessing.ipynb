{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 11-0: 텍스트 전처리 (Text Preprocessing)\n\n",
        "**학습 시간**: 3시간\n\n",
        "## 📚 학습 목표\n\n",
        "**Part 1: 기초 전처리**\n",
        "1. **텍스트 정제 기본**: 소문자 변환, 공백 정리, 특수문자 제거\n",
        "2. **정규표현식 활용**: `re` 모듈로 패턴 기반 텍스트 처리\n",
        "3. **토큰화 (Tokenization)**: 문장을 단어 단위로 분리\n",
        "4. **불용어 제거**: NLTK stopwords로 의미 없는 단어 제거\n",
        "5. **어간 추출 (Stemming)**: 단어의 어근 추출\n\n",
        "**Part 2: 한글 처리**\n",
        "1. **KoNLPy 소개**: 한국어 자연어 처리 라이브러리\n",
        "2. **형태소 분석**: Okt, Komoran으로 형태소 분리\n",
        "3. **품사 태깅 (POS Tagging)**: 명사, 동사, 형용사 구분\n",
        "4. **명사 추출**: 핵심 키워드 추출 기법\n\n",
        "---\n\n",
        "## 🎯 왜 이것을 배우나요?\n\n",
        "| 개념 | 실무 활용 | 예시 |\n",
        "|------|----------|------|\n",
        "| 텍스트 정제 | 데이터 품질 향상 | 뉴스 기사에서 HTML 태그 제거 |\n",
        "| 토큰화 | 분석 단위 생성 | 고객 리뷰를 단어별로 분리 |\n",
        "| 불용어 제거 | 노이즈 감소 | \"은\", \"는\", \"이\", \"가\" 제거 |\n",
        "| 형태소 분석 | 한글 처리 | \"먹었습니다\" → \"먹다\" 추출 |\n",
        "| 명사 추출 | 키워드 분석 | 리뷰에서 \"배송\", \"품질\" 추출 |\n\n",
        "**분석가 관점**: 텍스트 전처리는 모든 NLP 파이프라인의 첫 단계입니다. 깨끗한 데이터가 좋은 분석 결과를 만듭니다!\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 라이브러리 설치 및 임포트\n\n",
        "텍스트 전처리에 필요한 라이브러리들을 설치하고 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치 (최초 1회)\n",
        "# !pip install nltk konlpy\n",
        "\n",
        "# NLTK 데이터 다운로드 (최초 1회)\n",
        "import nltk\n",
        "nltk.download('punkt')        # 토큰화\n",
        "nltk.download('stopwords')    # 불용어\n",
        "nltk.download('wordnet')      # 표제어 추출\n",
        "nltk.download('punkt_tab')    # 토큰화 추가 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 라이브러리\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK (영어 처리)\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# KoNLPy (한글 처리)\n",
        "from konlpy.tag import Okt, Komoran\n",
        "\n",
        "print(\"라이브러리 임포트 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "# Part 1: 기초 전처리\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 텍스트 정제 기본\n\n",
        "텍스트 분석의 첫 단계는 **데이터 정제**입니다. 노이즈를 제거하고 일관된 형식으로 변환합니다.\n\n",
        "### 기본 정제 기법\n",
        "1. **소문자 변환**: 대소문자 통일\n",
        "2. **공백 정리**: 중복 공백 제거\n",
        "3. **앞뒤 공백 제거**: strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 1] 기본 텍스트 정제\n",
        "raw_text = \"  Hello  WORLD!   Welcome to   TEXT  Processing.  \"\n",
        "\n",
        "# 1. 소문자 변환\n",
        "lower_text = raw_text.lower()\n",
        "print(f\"소문자: '{lower_text}'\")\n",
        "\n",
        "# 2. 앞뒤 공백 제거\n",
        "stripped = lower_text.strip()\n",
        "print(f\"strip(): '{stripped}'\")\n",
        "\n",
        "# 3. 중복 공백을 단일 공백으로 (정규표현식)\n",
        "cleaned = re.sub(r'\\s+', ' ', stripped)\n",
        "print(f\"최종: '{cleaned}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 고객 리뷰 정제\n",
        "customer_reviews = [\n",
        "    \"  배송이 너무 빨라요!!!  \",\n",
        "    \"품질   좋습니다~~~ 추천해요  \",\n",
        "    \"   가격대비 BEST 상품입니다.   \"\n",
        "]\n",
        "\n",
        "def clean_basic(text):\n",
        "    \"\"\"기본 텍스트 정제 함수\"\"\"\n",
        "    text = text.lower()              # 소문자\n",
        "    text = text.strip()              # 앞뒤 공백\n",
        "    text = re.sub(r'\\s+', ' ', text) # 중복 공백\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_basic(review) for review in customer_reviews]\n",
        "for original, cleaned in zip(customer_reviews, cleaned_reviews):\n",
        "    print(f\"원본: '{original}'\")\n",
        "    print(f\"정제: '{cleaned}'\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 1.2 정규표현식 (Regular Expression)\n\n",
        "**정규표현식**은 텍스트에서 패턴을 찾고 치환하는 강력한 도구입니다.\n\n",
        "### 주요 메타문자\n",
        "| 패턴 | 의미 | 예시 |\n",
        "|------|------|------|\n",
        "| `\\d` | 숫자 | `[0-9]` |\n",
        "| `\\w` | 단어 문자 | `[a-zA-Z0-9_]` |\n",
        "| `\\s` | 공백 문자 | 스페이스, 탭, 줄바꿈 |\n",
        "| `.` | 모든 문자 | 줄바꿈 제외 |\n",
        "| `+` | 1개 이상 | `a+` → \"a\", \"aa\", \"aaa\" |\n",
        "| `*` | 0개 이상 | `a*` → \"\", \"a\", \"aa\" |\n",
        "| `[]` | 문자 클래스 | `[abc]` → a, b, c 중 하나 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 2] 특수문자 제거\n",
        "text = \"Hello! How are you? I'm fine, thank you~ :)\"\n",
        "\n",
        "# 방법 1: 특정 문자만 제거\n",
        "clean1 = re.sub(r'[!?~:()]', '', text)\n",
        "print(f\"방법 1: {clean1}\")\n",
        "\n",
        "# 방법 2: 알파벳과 공백만 남기기\n",
        "clean2 = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(f\"방법 2: {clean2}\")\n",
        "\n",
        "# 방법 3: 알파벳, 숫자, 공백만 남기기\n",
        "text_with_num = \"Price: $100, Discount: 20%!\"\n",
        "clean3 = re.sub(r'[^a-zA-Z0-9\\s]', '', text_with_num)\n",
        "print(f\"방법 3: {clean3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 3] 한글 텍스트에서 특수문자 제거\n",
        "korean_text = \"배송이 빠르네요!!! 최고에요~~~ ^^ 별점: 5점\"\n",
        "\n",
        "# 한글, 영어, 숫자, 공백만 남기기\n",
        "clean_korean = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', korean_text)\n",
        "print(f\"원본: {korean_text}\")\n",
        "print(f\"정제: {clean_korean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 다양한 정규표현식 패턴\n",
        "\n",
        "# 1. 이메일 추출\n",
        "text_email = \"문의: support@company.com 또는 sales@shop.co.kr\"\n",
        "emails = re.findall(r'[\\w.-]+@[\\w.-]+', text_email)\n",
        "print(f\"이메일: {emails}\")\n",
        "\n",
        "# 2. 전화번호 추출\n",
        "text_phone = \"연락처: 010-1234-5678, 02-123-4567\"\n",
        "phones = re.findall(r'\\d{2,3}-\\d{3,4}-\\d{4}', text_phone)\n",
        "print(f\"전화번호: {phones}\")\n",
        "\n",
        "# 3. 해시태그 추출\n",
        "text_hashtag = \"오늘 날씨 좋다 #서울 #맑음 #산책 #주말\"\n",
        "hashtags = re.findall(r'#\\w+', text_hashtag)\n",
        "print(f\"해시태그: {hashtags}\")\n",
        "\n",
        "# 4. URL 제거\n",
        "text_url = \"자세한 내용은 https://example.com/page 참고하세요\"\n",
        "clean_url = re.sub(r'https?://\\S+', '', text_url)\n",
        "print(f\"URL 제거: {clean_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 4] HTML 태그 제거\n",
        "html_text = \"<p>안녕하세요!</p><br><strong>중요한 공지</strong>입니다.\"\n",
        "\n",
        "# HTML 태그 제거\n",
        "clean_html = re.sub(r'<[^>]+>', '', html_text)\n",
        "print(f\"원본: {html_text}\")\n",
        "print(f\"정제: {clean_html}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 1.3 토큰화 (Tokenization)\n\n",
        "**토큰화**는 텍스트를 분석 단위(토큰)로 분리하는 과정입니다.\n\n",
        "### 토큰화 종류\n",
        "- **단어 토큰화 (Word Tokenization)**: 단어 단위로 분리\n",
        "- **문장 토큰화 (Sentence Tokenization)**: 문장 단위로 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 5] 기본 토큰화 (split)\n",
        "sentence = \"I love natural language processing\"\n",
        "\n",
        "# 공백 기준 분리\n",
        "tokens = sentence.split()\n",
        "print(f\"split(): {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 6] NLTK 토큰화 (더 정교함)\n",
        "text = \"Hello! I'm learning NLP. It's amazing, isn't it?\"\n",
        "\n",
        "# 단어 토큰화\n",
        "word_tokens = word_tokenize(text)\n",
        "print(f\"단어 토큰: {word_tokens}\")\n",
        "\n",
        "# 문장 토큰화\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(f\"문장 토큰: {sent_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 뉴스 기사 토큰화\n",
        "news_article = \"\"\"\n",
        "서울시는 오늘 새로운 교통 정책을 발표했다. \n",
        "이 정책은 대중교통 이용을 촉진하기 위한 것이다.\n",
        "시민들의 반응은 대체로 긍정적이다.\n",
        "\"\"\"\n",
        "\n",
        "# 문장 분리\n",
        "sentences = sent_tokenize(news_article.strip())\n",
        "print(f\"총 {len(sentences)}개 문장:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"  {i}. {sent}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 1.4 불용어 제거 (Stopword Removal)\n\n",
        "**불용어(Stopwords)**는 분석에 큰 의미가 없는 단어들입니다.\n",
        "- 영어: \"the\", \"is\", \"at\", \"which\", \"on\" 등\n",
        "- 한글: \"은\", \"는\", \"이\", \"가\", \"을\", \"를\" 등"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 7] 영어 불용어 제거\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 영어 불용어 목록\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "print(f\"영어 불용어 수: {len(english_stopwords)}\")\n",
        "print(f\"예시: {list(english_stopwords)[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 8] 불용어 제거 적용\n",
        "text = \"This is an example showing how to remove stopwords from text\"\n",
        "\n",
        "# 토큰화\n",
        "tokens = word_tokenize(text.lower())\n",
        "print(f\"원본 토큰: {tokens}\")\n",
        "\n",
        "# 불용어 제거\n",
        "filtered_tokens = [token for token in tokens if token not in english_stopwords]\n",
        "print(f\"불용어 제거: {filtered_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 커스텀 불용어 추가\n",
        "text = \"The product is very good and the delivery was fast\"\n",
        "\n",
        "# 기본 불용어 + 커스텀 불용어\n",
        "custom_stopwords = english_stopwords.union({'very', 'really', 'just'})\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [t for t in tokens if t not in custom_stopwords and t.isalpha()]\n",
        "print(f\"원본: {tokens}\")\n",
        "print(f\"필터: {filtered}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 1.5 어간 추출 (Stemming) vs 표제어 추출 (Lemmatization)\n\n",
        "### 어간 추출 (Stemming)\n",
        "- 규칙 기반으로 단어의 **어미를 제거**\n",
        "- 빠르지만 정확도 낮음\n",
        "- 예: \"running\" → \"run\", \"studies\" → \"studi\"\n\n",
        "### 표제어 추출 (Lemmatization)\n",
        "- 사전을 이용해 **기본형(lemma)** 찾기\n",
        "- 느리지만 정확도 높음\n",
        "- 예: \"running\" → \"run\", \"better\" → \"good\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 9] 어간 추출 (Stemming)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['running', 'runs', 'ran', 'easily', 'fairly', 'studies', 'studying']\n",
        "\n",
        "print(\"어간 추출 결과:\")\n",
        "for word in words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"  {word:12} → {stemmed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 10] 표제어 추출 (Lemmatization)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'runs', 'ran', 'better', 'studies', 'studying']\n",
        "\n",
        "print(\"표제어 추출 결과:\")\n",
        "for word in words:\n",
        "    # 동사(v)로 처리\n",
        "    lemma_v = lemmatizer.lemmatize(word, pos='v')\n",
        "    # 명사(n)로 처리\n",
        "    lemma_n = lemmatizer.lemmatize(word, pos='n')\n",
        "    print(f\"  {word:12} → 동사: {lemma_v:10} 명사: {lemma_n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 어간 추출 vs 표제어 추출 비교\n",
        "sentence = \"The studies showed that cats are running faster than dogs\"\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [stemmer.stem(t) for t in tokens]\n",
        "lemmatized = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
        "\n",
        "print(f\"원본:     {tokens}\")\n",
        "print(f\"Stemmed:  {stemmed}\")\n",
        "print(f\"Lemma:    {lemmatized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "# Part 2: 한글 처리 (KoNLPy)\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 KoNLPy 소개\n\n",
        "**KoNLPy**는 한국어 자연어 처리를 위한 파이썬 라이브러리입니다.\n\n",
        "### 형태소 분석기 종류\n",
        "| 분석기 | 특징 | 속도 | 정확도 |\n",
        "|--------|------|------|--------|\n",
        "| **Okt** | Twitter 기반, 간편함 | 빠름 | 보통 |\n",
        "| **Komoran** | Java 기반, 사용자 사전 | 보통 | 높음 |\n",
        "| **Mecab** | C 기반, 가장 빠름 | 매우 빠름 | 높음 |\n",
        "| **Kkma** | 정확하지만 느림 | 느림 | 매우 높음 |\n",
        "| **Hannanum** | KAIST 개발 | 보통 | 보통 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 11] Okt 형태소 분석기\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"아버지가방에들어가신다\"\n",
        "\n",
        "# 형태소 분석\n",
        "morphs = okt.morphs(text)\n",
        "print(f\"형태소: {morphs}\")\n",
        "\n",
        "# 품사 태깅\n",
        "pos_tags = okt.pos(text)\n",
        "print(f\"품사 태깅: {pos_tags}\")\n",
        "\n",
        "# 명사 추출\n",
        "nouns = okt.nouns(text)\n",
        "print(f\"명사: {nouns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 12] Komoran 형태소 분석기\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "\n",
        "text = \"코로나19로 인해 재택근무가 늘어났습니다\"\n",
        "\n",
        "# 형태소 분석\n",
        "morphs = komoran.morphs(text)\n",
        "print(f\"형태소: {morphs}\")\n",
        "\n",
        "# 품사 태깅\n",
        "pos_tags = komoran.pos(text)\n",
        "print(f\"품사 태깅: {pos_tags}\")\n",
        "\n",
        "# 명사 추출\n",
        "nouns = komoran.nouns(text)\n",
        "print(f\"명사: {nouns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 2.2 형태소 분석 (Morphological Analysis)\n\n",
        "**형태소**는 의미를 가진 가장 작은 언어 단위입니다.\n",
        "- \"먹었습니다\" → \"먹\" + \"었\" + \"습니다\"\n",
        "- \"자연어처리\" → \"자연어\" + \"처리\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 13] 형태소 분석 상세\n",
        "okt = Okt()\n",
        "\n",
        "sentences = [\n",
        "    \"맛있는 음식을 먹었습니다\",\n",
        "    \"서울에서 부산까지 KTX로 2시간 걸립니다\",\n",
        "    \"오늘 날씨가 정말 좋네요\"\n",
        "]\n",
        "\n",
        "for sent in sentences:\n",
        "    print(f\"\\n원문: {sent}\")\n",
        "    print(f\"형태소: {okt.morphs(sent)}\")\n",
        "    print(f\"품사: {okt.pos(sent)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 고객 리뷰 형태소 분석\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"가격은 비싸지만 성능이 뛰어납니다\",\n",
        "    \"포장이 꼼꼼하게 잘 되어있었어요\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "for review in reviews:\n",
        "    # 형태소 분석 (정규화 + 어간 추출)\n",
        "    morphs = okt.morphs(review, norm=True, stem=True)\n",
        "    print(f\"원문: {review}\")\n",
        "    print(f\"분석: {morphs}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 2.3 품사 태깅 (POS Tagging)\n\n",
        "**품사 태깅**은 각 형태소에 품사 정보를 부여하는 과정입니다.\n\n",
        "### Okt 주요 품사 태그\n",
        "| 태그 | 품사 | 예시 |\n",
        "|------|------|------|\n",
        "| Noun | 명사 | 사람, 컴퓨터 |\n",
        "| Verb | 동사 | 가다, 먹다 |\n",
        "| Adjective | 형용사 | 좋다, 크다 |\n",
        "| Adverb | 부사 | 매우, 빨리 |\n",
        "| Josa | 조사 | 은, 는, 이, 가 |\n",
        "| Eomi | 어미 | ~다, ~요 |\n",
        "| Punctuation | 구두점 | ., !, ? |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 14] 품사별 단어 추출\n",
        "okt = Okt()\n",
        "\n",
        "text = \"서울은 대한민국의 수도이며 매우 아름다운 도시입니다\"\n",
        "\n",
        "# 전체 품사 태깅\n",
        "pos_result = okt.pos(text)\n",
        "print(f\"전체: {pos_result}\")\n",
        "\n",
        "# 품사별 분류\n",
        "nouns = [word for word, pos in pos_result if pos == 'Noun']\n",
        "verbs = [word for word, pos in pos_result if pos == 'Verb']\n",
        "adjectives = [word for word, pos in pos_result if pos == 'Adjective']\n",
        "adverbs = [word for word, pos in pos_result if pos == 'Adverb']\n",
        "\n",
        "print(f\"\\n명사: {nouns}\")\n",
        "print(f\"동사: {verbs}\")\n",
        "print(f\"형용사: {adjectives}\")\n",
        "print(f\"부사: {adverbs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: 감성 분석용 형용사 추출\n",
        "reviews = [\n",
        "    \"이 제품은 정말 좋고 편리해요\",\n",
        "    \"배송은 빠르지만 품질이 별로예요\",\n",
        "    \"가격이 저렴하고 만족스럽습니다\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "for review in reviews:\n",
        "    pos_result = okt.pos(review, stem=True)  # 어간 추출 적용\n",
        "    \n",
        "    adjectives = [word for word, pos in pos_result if pos == 'Adjective']\n",
        "    \n",
        "    print(f\"리뷰: {review}\")\n",
        "    print(f\"형용사: {adjectives}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 2.4 명사 추출\n\n",
        "**명사 추출**은 텍스트에서 핵심 키워드를 찾는 가장 기본적인 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 15] 명사 추출 및 빈도 분석\n",
        "from collections import Counter\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "news = \"\"\"\n",
        "삼성전자가 새로운 스마트폰을 출시했다. \n",
        "이번 스마트폰은 카메라 성능이 크게 향상되었다.\n",
        "삼성전자 관계자는 이번 제품이 시장에서 큰 반응을 얻을 것으로 기대한다고 밝혔다.\n",
        "애플과의 경쟁에서도 우위를 점할 것으로 예상된다.\n",
        "\"\"\"\n",
        "\n",
        "# 명사 추출\n",
        "nouns = okt.nouns(news)\n",
        "print(f\"추출된 명사: {nouns}\")\n",
        "\n",
        "# 빈도 분석\n",
        "noun_counts = Counter(nouns)\n",
        "print(f\"\\n상위 5개 명사:\")\n",
        "for noun, count in noun_counts.most_common(5):\n",
        "    print(f\"  {noun}: {count}회\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 💡 실무 예시: SNS 댓글 키워드 분석\n",
        "comments = [\n",
        "    \"이 영화 진짜 재밌어요 배우 연기 최고\",\n",
        "    \"스토리가 탄탄하고 영상미가 뛰어남\",\n",
        "    \"배우들 연기력이 대단해요 감동 받음\",\n",
        "    \"영화 OST도 좋고 스토리도 감동적\",\n",
        "    \"이번 주말에 가족이랑 영화 보러 가야겠어요\"\n",
        "]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# 모든 댓글에서 명사 추출\n",
        "all_nouns = []\n",
        "for comment in comments:\n",
        "    nouns = okt.nouns(comment)\n",
        "    all_nouns.extend(nouns)\n",
        "\n",
        "# 1글자 명사 제외 (노이즈 제거)\n",
        "filtered_nouns = [n for n in all_nouns if len(n) > 1]\n",
        "\n",
        "# 키워드 빈도 분석\n",
        "keyword_counts = Counter(filtered_nouns)\n",
        "print(\"주요 키워드:\")\n",
        "for keyword, count in keyword_counts.most_common(10):\n",
        "    print(f\"  {keyword}: {count}회\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 2.5 종합: 텍스트 전처리 파이프라인\n\n",
        "실무에서는 여러 전처리 기법을 순차적으로 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 16] 한글 텍스트 전처리 파이프라인\n",
        "def preprocess_korean(text, min_length=2):\n",
        "    \"\"\"\n",
        "    한글 텍스트 전처리 파이프라인\n",
        "    \n",
        "    Args:\n",
        "        text (str): 원본 텍스트\n",
        "        min_length (int): 최소 단어 길이\n",
        "    \n",
        "    Returns:\n",
        "        list: 전처리된 토큰 리스트\n",
        "    \"\"\"\n",
        "    okt = Okt()\n",
        "    \n",
        "    # 1. 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # 2. 중복 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 3. 형태소 분석 (정규화 + 어간 추출)\n",
        "    tokens = okt.morphs(text, norm=True, stem=True)\n",
        "    \n",
        "    # 4. 불용어 제거 (한글 조사, 어미 등)\n",
        "    korean_stopwords = {'은', '는', '이', '가', '을', '를', '의', '에', '에서', \n",
        "                        '로', '으로', '도', '만', '과', '와', '하다', '있다', '되다'}\n",
        "    tokens = [t for t in tokens if t not in korean_stopwords]\n",
        "    \n",
        "    # 5. 최소 길이 필터링\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# 테스트\n",
        "sample_text = \"오늘 배송받은 상품이 정말 좋아요!!! 품질도 최고입니다~~ 강력 추천해요 ^^;;;\"\n",
        "result = preprocess_korean(sample_text)\n",
        "\n",
        "print(f\"원본: {sample_text}\")\n",
        "print(f\"결과: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [예제 17] 영어 텍스트 전처리 파이프라인\n",
        "def preprocess_english(text, min_length=2):\n",
        "    \"\"\"\n",
        "    영어 텍스트 전처리 파이프라인\n",
        "    \n",
        "    Args:\n",
        "        text (str): 원본 텍스트\n",
        "        min_length (int): 최소 단어 길이\n",
        "    \n",
        "    Returns:\n",
        "        list: 전처리된 토큰 리스트\n",
        "    \"\"\"\n",
        "    # 1. 소문자 변환\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. 특수문자 제거\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    \n",
        "    # 3. 중복 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 4. 토큰화\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # 5. 불용어 제거\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    \n",
        "    # 6. 표제어 추출\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
        "    \n",
        "    # 7. 최소 길이 필터링\n",
        "    tokens = [t for t in tokens if len(t) >= min_length]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# 테스트\n",
        "sample_text = \"I was running in the park yesterday! The weather was amazing.\"\n",
        "result = preprocess_english(sample_text)\n",
        "\n",
        "print(f\"원본: {sample_text}\")\n",
        "print(f\"결과: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 🎯 실습 퀴즈\n\n",
        "**난이도**: ⭐ (쉬움) ~ ⭐⭐⭐⭐⭐ (어려움)\n\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1. 기본 텍스트 정제 ⭐\n\n",
        "**문제**: 아래 텍스트를 정제하는 함수를 작성하세요.\n\n",
        "**요구사항**:\n",
        "- 소문자 변환\n",
        "- 앞뒤 공백 제거\n",
        "- 중복 공백을 단일 공백으로 변환\n\n",
        "```python\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n",
        "```\n\n",
        "**기대 결과**: `\"hello world! welcome to nlp.\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "raw_text = \"  Hello   WORLD!  Welcome to   NLP.  \"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. 정규표현식 - 특수문자 제거 ⭐\n\n",
        "**문제**: 정규표현식을 사용하여 특수문자를 제거하세요.\n\n",
        "**요구사항**:\n",
        "- 한글, 영어, 숫자, 공백만 남기기\n",
        "- 나머지 문자는 모두 제거\n\n",
        "```python\n",
        "text = \"배송 빠르네요!!! 가격 10,000원 @#$% 추천해요~~\"\n",
        "```\n\n",
        "**기대 결과**: `\"배송 빠르네요 가격 10000원  추천해요\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "text = \"배송 빠르네요!!! 가격 10,000원 @#$% 추천해요~~\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. 정규표현식 - 해시태그 추출 ⭐⭐\n\n",
        "**문제**: SNS 게시물에서 모든 해시태그를 추출하세요.\n\n",
        "**요구사항**:\n",
        "- `re.findall()` 사용\n",
        "- 해시태그 형식: `#단어`\n\n",
        "```python\n",
        "post = \"오늘 날씨 좋다 #서울 #맑음 #주말나들이 점심은 #맛집 가야지\"\n",
        "```\n\n",
        "**기대 결과**: `['#서울', '#맑음', '#주말나들이', '#맛집']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "post = \"오늘 날씨 좋다 #서울 #맑음 #주말나들이 점심은 #맛집 가야지\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4. NLTK 토큰화 ⭐⭐\n\n",
        "**문제**: 영어 문장을 단어와 문장으로 토큰화하세요.\n\n",
        "**요구사항**:\n",
        "- `word_tokenize()`: 단어 토큰화\n",
        "- `sent_tokenize()`: 문장 토큰화\n\n",
        "```python\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n",
        "```\n\n",
        "**기대 결과**:\n",
        "- 단어: `['Hello', '!', 'How', 'are', 'you', '?', ...]`\n",
        "- 문장: `['Hello!', 'How are you?', ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "text = \"Hello! How are you? I'm learning NLP. It's really fun.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5. 불용어 제거 ⭐⭐⭐\n\n",
        "**문제**: 영어 문장에서 불용어를 제거하세요.\n\n",
        "**요구사항**:\n",
        "- 소문자 변환 후 토큰화\n",
        "- NLTK 불용어 제거\n",
        "- 알파벳으로만 이루어진 단어만 유지\n\n",
        "```python\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n",
        "```\n\n",
        "**기대 결과**: `['movie', 'really', 'amazing', 'loved', 'much']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "sentence = \"The movie was really amazing and I loved it very much!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6. 어간 추출 vs 표제어 추출 ⭐⭐⭐\n\n",
        "**문제**: 단어 리스트에 어간 추출과 표제어 추출을 각각 적용하세요.\n\n",
        "**요구사항**:\n",
        "- `PorterStemmer` 사용\n",
        "- `WordNetLemmatizer` 사용 (동사 품사)\n",
        "- 결과를 딕셔너리로 반환: `{원본: (stemmed, lemmatized)}`\n\n",
        "```python\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n",
        "```\n\n",
        "**기대 결과**: \n",
        "```\n",
        "running: stem=run, lemma=run\n",
        "studies: stem=studi, lemma=study\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "words = ['running', 'studies', 'played', 'better', 'happily']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7. KoNLPy 명사 추출 ⭐⭐⭐\n\n",
        "**문제**: 고객 리뷰에서 명사를 추출하고 빈도를 분석하세요.\n\n",
        "**요구사항**:\n",
        "- `Okt.nouns()` 사용\n",
        "- 1글자 명사 제외\n",
        "- 상위 5개 명사 출력\n\n",
        "```python\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"가격대비 품질이 훌륭합니다\",\n",
        "    \"배송은 빠른데 포장이 아쉬워요\",\n",
        "    \"상품 퀄리티 최고 배송도 빠름\"\n",
        "]\n",
        "```\n\n",
        "**기대 결과**:\n",
        "```\n",
        "배송: 3회\n",
        "품질: 2회\n",
        "상품: 2회\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "reviews = [\n",
        "    \"배송이 빠르고 상품 품질이 좋아요\",\n",
        "    \"가격대비 품질이 훌륭합니다\",\n",
        "    \"배송은 빠른데 포장이 아쉬워요\",\n",
        "    \"상품 퀄리티 최고 배송도 빠름\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8. 품사별 단어 분류 ⭐⭐⭐⭐\n\n",
        "**문제**: 문장에서 품사별로 단어를 분류하세요.\n\n",
        "**요구사항**:\n",
        "- `Okt.pos()` 사용\n",
        "- 명사(Noun), 동사(Verb), 형용사(Adjective) 분류\n",
        "- 결과를 딕셔너리로 반환\n\n",
        "```python\n",
        "text = \"맛있는 음식을 먹고 행복한 하루를 보냈다\"\n",
        "```\n\n",
        "**기대 결과**:\n",
        "```python\n",
        "{\n",
        "    'Noun': ['음식', '하루'],\n",
        "    'Verb': ['먹다', '보내다'],\n",
        "    'Adjective': ['맛있다', '행복하다']\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "text = \"맛있는 음식을 먹고 행복한 하루를 보냈다\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9. 한글 전처리 함수 ⭐⭐⭐⭐\n\n",
        "**문제**: 한글 텍스트 전처리 함수를 작성하세요.\n\n",
        "**요구사항**:\n",
        "- 함수명: `clean_korean_text`\n",
        "- 기능:\n",
        "  1. 특수문자 제거 (한글, 영어, 숫자, 공백만 유지)\n",
        "  2. 중복 공백 제거\n",
        "  3. 형태소 분석 (정규화, 어간 추출)\n",
        "  4. 불용어 제거 (조사, 어미)\n",
        "  5. 2글자 이상 토큰만 유지\n",
        "- 반환: 토큰 리스트\n\n",
        "```python\n",
        "text = \"오늘 배송받은 상품이 정말 좋아요!!! 강력 추천합니다~~\"\n",
        "```\n\n",
        "**기대 결과**: `['오늘', '배송', '받다', '상품', '정말', '좋다', '강력', '추천']` (유사한 결과)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "text = \"오늘 배송받은 상품이 정말 좋아요!!! 강력 추천합니다~~\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10. 종합: 뉴스 기사 키워드 추출 ⭐⭐⭐⭐⭐\n\n",
        "**문제**: 뉴스 기사에서 핵심 키워드를 추출하는 파이프라인을 완성하세요.\n\n",
        "**요구사항**:\n",
        "- 함수명: `extract_keywords`\n",
        "- 입력: 뉴스 기사 텍스트\n",
        "- 처리 과정:\n",
        "  1. 특수문자 제거\n",
        "  2. 형태소 분석 (Okt)\n",
        "  3. 명사만 추출\n",
        "  4. 1글자 명사 제외\n",
        "  5. 빈도 분석\n",
        "- 반환: 상위 N개 키워드 리스트 `[(키워드, 빈도), ...]`\n\n",
        "```python\n",
        "news = \"\"\"\n",
        "삼성전자가 새로운 갤럭시 스마트폰을 출시했다.\n",
        "이번 갤럭시는 카메라 성능이 크게 향상되었으며,\n",
        "배터리 용량도 늘어났다. 삼성전자 관계자는\n",
        "\"이번 갤럭시가 시장에서 좋은 반응을 얻을 것\"이라고 밝혔다.\n",
        "경쟁사인 애플의 아이폰과의 경쟁도 치열해질 전망이다.\n",
        "\"\"\"\n",
        "```\n\n",
        "**기대 결과**: `[('갤럭시', 3), ('삼성전자', 2), ('스마트폰', 1), ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여기에 코드를 작성하세요\n",
        "news = \"\"\"\n",
        "삼성전자가 새로운 갤럭시 스마트폰을 출시했다.\n",
        "이번 갤럭시는 카메라 성능이 크게 향상되었으며,\n",
        "배터리 용량도 늘어났다. 삼성전자 관계자는\n",
        "\"이번 갤럭시가 시장에서 좋은 반응을 얻을 것\"이라고 밝혔다.\n",
        "경쟁사인 애플의 아이폰과의 경쟁도 치열해질 전망이다.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n",
        "## 📊 학습 정리\n\n",
        "### Part 1: 기초 전처리 핵심 요약\n\n",
        "| 기법 | 목적 | 주요 함수/패턴 |\n",
        "|------|------|---------------|\n",
        "| 소문자 변환 | 대소문자 통일 | `text.lower()` |\n",
        "| 공백 정리 | 노이즈 제거 | `strip()`, `re.sub(r'\\s+', ' ')` |\n",
        "| 특수문자 제거 | 패턴 기반 정제 | `re.sub(r'[^가-힣a-zA-Z0-9\\s]', '')` |\n",
        "| 토큰화 | 분석 단위 분리 | `word_tokenize()`, `sent_tokenize()` |\n",
        "| 불용어 제거 | 의미 없는 단어 제거 | `stopwords.words('english')` |\n",
        "| 어간 추출 | 단어 정규화 | `PorterStemmer()` |\n",
        "| 표제어 추출 | 사전 기반 정규화 | `WordNetLemmatizer()` |\n\n",
        "### Part 2: 한글 처리 핵심 요약\n\n",
        "| 기법 | 목적 | 주요 함수 |\n",
        "|------|------|----------|\n",
        "| 형태소 분석 | 형태소 분리 | `okt.morphs()` |\n",
        "| 품사 태깅 | 품사 부여 | `okt.pos()` |\n",
        "| 명사 추출 | 키워드 추출 | `okt.nouns()` |\n",
        "| 정규화 | 비표준 표현 정제 | `okt.morphs(norm=True)` |\n",
        "| 어간 추출 | 원형 복원 | `okt.morphs(stem=True)` |\n\n",
        "### 💡 실무 팁\n\n",
        "1. **전처리 순서가 중요**: 특수문자 제거 → 토큰화 → 불용어 제거 → 정규화\n",
        "2. **한글은 형태소 분석이 필수**: 띄어쓰기만으로는 의미 단위 분리 불가\n",
        "3. **커스텀 불용어 목록 관리**: 도메인별로 추가/제거할 단어가 다름\n",
        "4. **정규화 vs 어간 추출**: 정확도 vs 속도 트레이드오프 고려\n",
        "5. **분석 목적에 따른 품사 선택**: 감성 분석은 형용사, 키워드 분석은 명사\n\n",
        "---\n\n",
        "## 🚀 다음 단계\n\n",
        "텍스트 전처리를 마스터했습니다! 다음은 **텍스트 분석 기법**입니다.\n\n",
        "**다음 노트북**: `Day11_1_텍스트_분석_기법_Text_Analysis.ipynb`\n",
        "- 단어 빈도 분석 (Word Frequency)\n",
        "- TF-IDF 벡터화\n",
        "- 워드 클라우드 시각화\n",
        "- 문서 유사도 분석"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
